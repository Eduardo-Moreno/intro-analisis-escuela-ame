# Análisis predictivo


```{r setup6, message = FALSE, echo = FALSE}
library(tidyverse)
library(lubridate)
library(ggthemes)
library(glmnet)
theme_set(theme_minimal(base_size = 14))
```

En análisis predictivo nos interesa principalmente utilizar información conocida para
entender qué puede pasar con valores de alguna variable que no conocemos usando reglas
computacionales. 

Por ejemplo:

- Predecir los niveles de ingresos que puede tener un hogar dadas sus características
físicas y características de sus ocupantes.
- Predecir cuáles van a ser los niveles de Ozono en la próxima hora, o el próximo día
- Dadas características de una casa, predecir cuánto tardará en venderse si sale al mercado en cierto precio.
- Dado el comportamiento de clientes de una compañía, clasificar aquellos clientes
con alta probabilidad de cancelar su contrato, o de caer en impago de un crédito.
- En una colección grande de imágenes, clasificar aquellas con alta probabilidad de 
tener a una persona o una cara en alguna parte de la imagen.
- En una colección grande de grabaciones de audio en un bosque, extraer las grabaciones que tienen alta probabilidad de tener un sonido de murciélagos.

Las razones por las que queremos hacer análisis predictivo son diferentes en cada caso:

- En algunos casos, se debe a que es costoso medir directamente la variable (el ejemplo de los ingresos de hogares, extraer segmentos de grabaciones de días de grabaciones).
- El valor ocurre en el futuro o es hipotético (tiempo de venta de una casa, si una persona va caer en impago o no)
- Puede ser más aceptable y apropiado que un algoritmo indique qué imagenes contienen caras a que personas revisen imágenes buscando caras (aún cuando el costo fuera razonable)

En esta sección nuestra referencia principal es @ESL.


## ¿Qué significa tener buenos resultados? {-}

En la tarea general del análisis predictivo, hablamos de lograr **predicciones precisas** como
el criterio más importante. Este punto de vista limitado y poco vago también es parte de su fortaleza en problemas aplicados, pues el criterio de éxito es más suceptible de ser definido con precisión.


Sin embargo, la tarea predictiva generalmente es parte de un proceso o ingrediente
en una decisión. En este contexto más amplio, tener éxito significa
usualmente que la inclusión de predicciones en el proceso de toma decisiones muestra
mejoras claras en métricas importantes *con respecto a lo que se hacía antes* (uso de reglas simples, completar tareas con intervención discrecional de personas). Esto significa que generalmente estamos trabajando contra un **estándar dado**:

- Dar créditos según juicios de un gerente de banco en lugar de mediante un modelo de calificación crediticia.
- Asignar programas sociales según predicciones de vulnerabilidades y capacidades futuras de los hogares,  en lugar de usar reglas simples como localización geográfica o estimaciones simples de ingreso.
- Acercarnos a un *benchmark* humano de clasificación de textos o imágenes de manera comparable a una persona, traducir textos a niveles comparables a un humano (según páneles de jueces), etc.


## ¿Qué se necesita para lograr buenos resultados? {-}

Más adelante hablaremos de la construcción y evaluación de modelos. Consideramos primero los ingredientes
básicos para tener buenos resultados con el enfoque predictivo:

- Tenemos la información correcta acerca de la variable que queremos predecir. Esto implica generalmente que 
nuestro esfuerzo se hace en un contexto donde la recolección y el análisis de datos
ya se hace de manera sistemática y adaptada al problema.

- El enfoque predictivo se enriquece con un ciclo de aplicación de modelo, medición de métricas, y mejora constante. Este ciclo
produce más datos, a veces diferentes, y el objetivo es dirigir esfuerzos en recolectar más datos importantes y adaptarnos
a cambios en el sistema.

- Finalmente, y quizá más centrado en la tarea fundamental del análisis predictivo, es necesario experimentar con distintos
enfoque de procesamiento de la información disponible y el uso de distintos modelos y enfoques predictivos.

En resumen, el enfoque predictivo tiende a producir resultados en áreas donde 
ya existe una *cultura analítica* sólida. 


## Cómo evaluar predicciones: error de prueba {-}

Supongamos que buscamos hacer predicciones  de una variable $Y$  en términos
de un conjunto de variables $X$, usando una función disponible $f$.

Asi que dados datos $X$ construimos nuestra predicción como
$\hat{Y} = f(X)$. Por ejemplo, para predecir el precio de venta de una
casa  usamos la función

$$ f(X) = f(m2, calidad) = (1000 + 50 * calidad ) * m^2  $$

donde *m^2* es el área de las casas, y *calidad* es una calificación de los
terminados de la casa. ¿Cómo mediríamos su desempeño?

La respuesta más directa es: tomamos una muestra de casas que se han vendido
recientemente y registramos su precio de venta (ventas que ocurrieron típicamente
después de que construimos la función $f$. Comparamos el precio de venta
observado con las predicciones del modelo.Si los datos observados son

$$(\mathbf{x}^{(1)}, \mathbf{y}^{(1)}), (\mathbf{x}^{(1)}, \mathbf{y}^{(1)}), \ldots, (\mathbf{x}^{(m)}, \mathbf{y}^{(m)}),$$
Calcularíamos el **error de predicción** para cada caso:

$$\mathbf{y}^{(i)} - f(\mathbf{x}^{(i)})$$
y observaríamos cómo se comportan estos errores (¿son muy grandes? ¿subestimamos o sobrestimamos mucho?, etc ). 

Podemos usar un resumen concreto para medir el error
sobre estos datos, por ejemplo calculando un error promedio. Una opción común es
usar el error cuadrático:

$$ \hat{Err} = \frac{1}{m} \sum_{j=1}^m (\mathbf{y}^{(i)} - f(\mathbf{x}^{(i)}))^2$$

Aunque podríamos usar también error absoluto u otra medida más interpretable o apropiada para nuestro problema. Esta cantidad estima el **error predictivo**, y le llamamos
**error de prueba**. A los datos que usamos le llamamos el **conjunto de datos de prueba**.

 Nótese que esta medida resumen está sujeta a variación muestral, por ejemplo, así que podríamos aplicar bootstrap u otra cosa para producir una estimación de intervalo.


## ¿Cómo construimos nuestro predictor $f$? {-}

Hay muchas maneras de construir $f$. Por ejemplo, quizá para el ejemplo de las casas
se usaba experiencia de valuadores de casas y reglas simples o tabuladores.

En análisis predictivo, la idea es aprender de los datos *etiquetados* (donde conocemos la variable respuesta $y$) para construir reglas de predicción computacionales, con el objetivo específico de que el error predictivo sea bajo.

Así es que emepezamos tomando unos **datos de entrenamiento**, donde observamos
datos de entrada (covariables) y la respuesta que queremos predecir

$$({x}^{(1)}, {y}^{(1)}), ({x}^{(2)}, {y}^{(2)}), \ldots, ({x}^{(m)}, {y}^{(m)}),$$
Y queremos tener un método para construír $f$ a partir de los datos

$$({x}^{(1)}, {y}^{(1)}), ({x}^{(2)}, {y}^{(2)}), \ldots, ({x}^{(m)}, {y}^{(m)}) \to f$$

## La tarea del análisis predictivo {-}

En resumen, la tarea del análisis predictivo es la siguiente: dados 

- Datos de entrenamiento $\mathcal{L}  = \{({x}^{(1)}, y^{1)}), ({x}^{(2)}, y^{2)}), \ldots, ({x}^{(n)}, y^{n)})\}$
- Datos de prueba $\mathcal{T} =  \{(\mathbf{x}^{(1)}, \mathbf{y}^{1)}), (\mathbf{x}^{(2)}, \mathbf{y}^{2)}), \ldots, (\mathbf{x}^{(m)}, \mathbf{y}^{m)})\}$

Queremos construir una función $\mathcal{L} \to f$ (sólo depende de datos de entrenamiento) tal que el promedio
de error sobre los datos de prueba:

$$\hat{Err} = \frac{1}{m} \sum_{i = 1}^m ( \mathbf{y}^{(j)}- {f}(\mathbf{x}^{(j)}))^2$$

sea lo más chico posible. 

Es crucial que los datos de entrenamiento sean distintos de los datos de prueba: esta es la
esencia de *aprender a predecir*, y no memorizar, por ejemplo. Memorizar es como
sobreinterpretar patrones que no generalizan en los datos de entrenamiento, como vimos en
la sesión anterior.

Otra manera de decir esto es que buscamos buen desempeño **fuera de la muestra** con la que construimos el modelo. 

Esta formulación de la tarea del análisis predictivo ha sido muy productiva (@donoho50), y ha permitido
avances grandes en muchos problemas interesantes desde hace unas cuantas décadas. En parte
es porque 
 esta formulación es relativamente fácil de implementar
para agregar contribuciones de muchas personas. Sitios como [Kaggle](https://www.kaggle.com/) son implementaciones donde:

- Hay un conjunto de datos disponibles, con los que cualquiera puede construir modelos
- Existen competidores que se inscriben y producen reglas de predicción
- Hay un referee que evalúa las reglas de los concursantes usando datos a los que sólo el referee
tiene acceso.
- (En algunos casos) Al final del concurso, los competidores muestran su metodología.

**Ejemplo**: el concurso de Netflix (2009) siguió este patrón. De este concurso se desarrolló 
parcialmente el área ahora floreciente de sistemas de recomendación.

## Error de entrenamiento y prueba {-}

La estimación del error de predicción que mostramos arriba, que usamos para
evaluar los modelos, es el **error de prueba**:

$$\hat{Err} = \frac{1}{m} \sum_{i = 1}^m ( \mathbf{y}^{(j)}- {f}(\mathbf{x}^{(j)}))^2.$$

Y podemos también calcular una medida de diagnóstico que es el **error de entrenamiento**:

$$\overline{err} = \frac{1}{m} \sum_{i = 1}^m ( {y}^{(j)}- {f}({x}^{(j)}))^2.$$

Nótese que esta última medida usa **los datos de entrenamiento**. En general,
no es una evaluación del desempeño predictivo, sino una medida de qué tan bien
nuestra $f$ "memorizó" los datos de entrenamiento. 

Esta cantidad nos ayudará a diagnosticar problemas de desempeño predictivo, pero
es una cantidad de importancia secundaria en el análisis predictivo. La cantidad
importante es el error de prueba.


## Evaluación de predictores {-}

Comenzamos mostrando algunas formas de construir predicciones en un ejemplo simple,
y cómo los evaluaríamos. Nos interesa comenzar contestar las preguntas:

- ¿Por qué son diferentes error de entrenamiento y error de prueba?
- ¿Por qué unos métodos son mejores que otros para un problema dado?
- ¿Por qué los errores de predicción pueden ser altos?
- ¿Qué hacer cuando las predicciones del modelo son malas?

Comenzamos usando datos simulados. En todos los casos, usaremos, según la tarea
fundamental del análisis predictivo, una muestra de entrenamiento para construir las
funciones para predecir, y otra muestra independiente de prueba para evaluarlos.

## Tres ejemplos de predictores {-}

Para este ejemplo simulado de una sola covariable, los puntos muestran los datos de entrenamiento.

```{r, fig.width=5, fig.asp=0.7}
set.seed(280572)
x <- c(1,7,10,0,0,5,9,13,2,4,17,18,1,2)
f <- function(x){
  ifelse(x < 10, 1000*sqrt(x), 1000*sqrt(10))
}
error <- rnorm(length(x), 0, 500)
y <- round(f(x) + error)
datos_entrena <- data.frame(x = x, y = y)
curva_1 <- geom_smooth(data = datos_entrena,
  method = "loess", se = FALSE, color = "gray", span = 1.2, size = 1.1)
curva_2 <- geom_smooth(data = datos_entrena,
  method = "loess", se = FALSE, color = "red", span = 0.5, size = 1.1)
curva_3 <- geom_smooth(data = datos_entrena,
  method = "lm", se = FALSE, color = "blue", size = 1.1)
head(datos_entrena)
```


Usamos tres métodos distintos para construir un predictor, que dan las 3 curvas que vemos abajo.
¿Cuál preferirías usar para hacer predicciones?

```{r, fig.width=5, fig.asp=0.7, warning = FALSE}
graf_ajustes <- ggplot(datos_entrena, aes(x=x, y=y)) + geom_point() + 
    curva_1 + curva_2 + curva_3
graf_ajustes
```

Calculamos los errores de entrenamiento de cada curva:

```{r, warning = FALSE}
mod_rojo <- loess(y ~ x, data = datos_entrena, span=0.3)
mod_gris <- loess(y ~ x, data = datos_entrena, span=1)
mod_recta <- lm(y ~ x, data = datos_entrena)
df_mods <- tibble(nombre = c('recta', 'rojo', 'gris'))
df_mods$modelo <- list(mod_recta, mod_rojo, mod_gris)
```

```{r, echo = FALSE}
error_f <- function(df, mod){
  function(mod){
    preds <- predict(mod, newdata = df)
    round(sqrt(mean((preds - df$y) ^ 2)))
  }
}
error_ent <- error_f(datos_entrena)
df_mods <- df_mods %>% 
  mutate(error_entrena = map_dbl(modelo, error_ent))
df_mods %>% select(-modelo) %>% knitr::kable(type = "html")
```

y vemos que el modelo  con menor error de entrenamiento es la curva roja que parece ser demasiado
sensible a variación en los datos. Eso no es sorprendente: se trata de un método flexible
que puede casi "interpolar" los datos.

Ahora simulamos una nueva muestra grande y calculemos el *error de prueba*, que es
lo realmente importante.

```{r, fig.width=5, fig.asp=0.7}
set.seed(218052272)
x_0 <- sample(0:13, 2000, replace = T)
error <- rnorm(length(x_0), 0, 500)
y_0 <- f(x_0) + error
datos_prueba <- tibble(x = x_0, y = y_0)
datos_prueba %>% head
```

```{r}
error_p <- error_f(datos_prueba)
df_mods <- df_mods %>% 
  mutate(error_prueba = map_dbl(modelo, error_p))
resultados_tbl <- df_mods %>% select(-modelo) 
resultados_tbl %>% knitr::kable(type = "html")
```

y vemos que el modelo con mejor desempeño está dado por la curva gris, lo cual coincide con nuestra
intuición. 

## Interpretación del experimento de simulación {-}


¿Cómo explicamos los resultados que obtuvimos en el ejercicio anterior? Aquí están
los tres ajustes:

<!---
código de  https://gist.github.com/jennybc/e9e9aba6ba18c72cec26#file-2015-03-02_plot-next-to-table-rmd
-->

<div class="twoC">
```{r results = 'asis', echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(resultados_tbl)
```

```{r echo = FALSE, message=FALSE, warning=FALSE}
graf_ajustes
```
</div>
<div class="clearer"></div>

**Curva roja: el modelo muy flexible**

- Su error de prueba es no fue el mejor, sin embargo, su error de entrenamiento fue el más bajo. La **brecha** prueba - entrenamiento es grande.
- Esto quiere decir los patrones de variación que muestra no se generalizaron a la muestra de prueba. Se **sobreajustó** a los datos de entrenamiento.
- También podemos decir que este modelo memorizó, en lugar de aprender
- Intuitivamente, es un método que depende muy fuertemente de los datos. Si variamos un poco los datos de entrenamiento, las predicciones pueden cambiar mucho.

**Recta: el modelo más rígido**

- Su error de prueba no fue el mejor, sin embargo, su error de entrenamiento es similar al de prueba. No hay **brecha** prueba - entrenamiento
- Esto quiere decir que los patrones que encontró generalizan a la muestra de prueba.
- Sin embargo, comparado con el modelo gris, no encontró patrones de variación que **sí** generalizan a la muestra de prueba. **Subajustó** los datos de entrenamiento.
- También podemos decir que este modelo no pudo aprender patrones estables.
- Intuitimvamente, es un método que depende menos de los datos. Si variamos los datos de entrenamiento, las predicciones probablemente no cambien mucho.

**Curva gris**

- Su error de prueba fue el mejor. Su **brecha** entrenamiento - prueba fue intermedia entre el modelo lineal y la curva roja.
- Esto quiere decir que algunos patrones de variación que ajustó no generalizan, así que tiene cierto grado de sobreajuste - pero menos que la curva más flexible.
- Este modelo no pudo memorizar los datos tan bien como la curva roja, pero aprendió mejor.
- Intuitivamente, depende de los datos de entrenamiento de manera más razonable: ni ignora señal en los datos, pero tampoco es insensible al ruido.


En resumen, el mejor desempeño lo encontramos buscando un un balance entre:

- Flexibilidad y rigidez
- Sobreajuste y subajuste
- Capturar ruido vs capturar señal
- Sesgo y Varianza

Podemos tener predicciones malas porque no tenemos la flexibilidad suficiente para capturar
en nuestro predictor señales que generalizan, y también porque tenemos tanta flexibilidad que capturamos ruido o patrones que nogeneralizan en nuestro predictor

## Errores por flexibilidad y por rigidez {-}

Para entender la "dependencia" de los datos de entrenamiento (demasiado fuerte o demasiada baja), tendremos que ver varios conjuntos de entrenamiento y qué pasa con nuestras
curvas gris, roja y la recta (recordemos la distribución de muestreo de un estimador, por ejemplo).

Para ver más concretamente cómo operan estos dos tipos de error intentamos
ver qué sucede con las predicciones en un punto dado:

```{r}
x_0 <- 7
```

Simulamos
varias muestras de entrenamiento y vemos cómo cambia la predicción:

```{r, fig.width=5, fig.asp=0.7, warning = FALSE, echo = FALSE}
x <- c(1,7,10,0,0,5,9,13,2,4,17,18,1,2)
x_g <- seq(0,15,0.5)
y_g <- f(x_g)
dat_g <- tibble(x = x_g, y = y_g)
f <- function(x){
  ifelse(x < 10, 1000*sqrt(x), 1000*sqrt(10))
}
simular_curvas <- function(rep, x_g){
    error <- rnorm(length(x), 0, 500)
    y <- round(f(x) + error)
    datos_entrena <- data.frame(x = x, y = y)
    curva_1 <- loess(y ~ x, data = datos_entrena, degree = 2, span = 1.2)
    curva_2 <- loess(y ~ x, data = datos_entrena, degree = 2, span = 0.6)
    curva_3 <- lm(y ~ x, data = datos_entrena)
    modelos <- list(curva_1, curva_2, curva_3)
    x_in <- tibble(x = x_g)
    preds <- map(modelos, ~ tibble(pred = predict(.x, newdata = x_in))) %>% 
             map(~ bind_cols(x_in, .x))
    tibble(rep = rep, metodo = c("gris","rojo", "recta"), preds = preds)
}
set.seed(8812)
sims_curvas <- map(1:30, ~ simular_curvas(.x, x_g)) %>% bind_rows %>% unnest
```

En negro está la verdadera $f$: esta es la curva que queremos estimar. Cada línea de color
representa un modelo ajustado con una muestra de entrenamiento distinta:

```{r, fig.width = 10, fig.height=4}
ggplot(sims_curvas, aes(x = x)) +
    geom_vline(xintercept = 7, colour = "gray") +
    geom_line(aes(y = pred, group = interaction(metodo, rep), colour = metodo)) +
     geom_line(data = dat_g, aes(y = y), size = 2) + facet_wrap(~metodo) +
    scale_colour_manual(values = c("gray40", "blue", "red")) 
```

Y vemos que la recta tiene relativamente poca variabilidad, pero consistemente falla en aprender la forma de los
datos. Por el contrario, en algunos puntos las predicciones del modelo rojo son muy malas, pues el suavizador
varía mucho.

Ahora vemos las predicciones comparadas con el óptimo en el punto que nos interesa, que es $f_0(7)$:

```{r, message = FALSE, warning = FALSE}
sims_curvas <- map(1:200, ~ simular_curvas(.x, x_g)) %>% bind_rows %>% unnest
sims <- filter(sims_curvas, x == 7)
ggplot(sims, aes(x= 7, y = pred)) + 
    geom_line(data = dat_g %>% filter(x > 4 & x < 11), aes(x = x, y = y)) +
    geom_boxplot(aes(colour = metodo)) 
```

Y confirmamos que:

- El error predictivo del modelo rojo es frecuentemente alto porque su varianza
de la predicción es alta (depende muy fuertemente de los datos)
- El error predictivo de la recta es malo porque está consistemente lejos de la curva verdadera, aunque varía menos.
- El modelo gris parece tener ligero sesgo, pero menos que el modelo lineal, y varía menos que
la curva roja.
- El error nunca podrá ser muy bajo, porque en nuestras simulaciones agregamos variación que no está asociada a la variable de entrada $x$.

## Descomposición varianza-sesgo {-}

Consideremos un modelo particular para ayudarnos a entender
de dónde provienen los errores de predicción:

$$y = f_0(x) + \epsilon$$
con $E(\epsilon) = 0$. Esta variable aleatoria $\epsilon$ representa efectos
de otras variables que no tenemos disponibles, y que afectan el resultado $y$ que observamos. 
La función $f_0$ modela la dependencia "verdadera" de $y$ de los datos de entrada $x$.

Sea $f$ nuestro predictor. Escribimos el error como:

$$y-{f}(x) = (f_0(x)- {f}(x)) + \epsilon$$

Y vemos dos razones para errores grandes:

1. La $\epsilon$ puede tomar valores grandes (pues no tenemos variables importantes que influyen en $y$)
2. $f(x) - f_0(x)$ puede ser grande, es decir, nuestra estimación de $f_0$ es mala. A su vez,
hay dos razones por las que puede ocurrir esto:
  - $f(x)$ tiene poca variabilidad y está consistemente lejos de la verdadera $f_0$, independientemente de la muestra
que usemos para entrenar al modelo.
  - La variabilidad de $f(x)$ es grande, de forma que es probabable que el valor obtenido en una
muestra particular esté lejos del valor $f_0(x)$
  - Una combinación de los dos incisos anteriores.


Una manera de cuantificar esto es mediante la descomposición varianza-sesgo. Con algunos cálculos simples,
podemos demostrar que el error de predicción (con error cuadrático) se descompone en (con $X$ fija):

$$E((Y-f(x))^2) = (E(f(x)) - f_0(x))^2 + Var(f(x)) + Var(\epsilon)$$

donde el valor esperado es sobre el conjunto de entrenamiento ${\mathcal L}$ y $\epsilon$ (es decir, promediando sobre muestras de entrenamiento posibles).

- El primer término le llamamos *sesgo* del predictor (si está consistentemente lejos de $f(x)$)
- El segundo término es la *varianza* del predictor (qué tan sensible es a la muestra de entrenamiento)
- Al tercer término le llamamos *error irreducible*.

Este resultado explica lo que observamos en nuestro ejemplo de simulación. En análisis
predictivo, buscamos encontrar el mejor balance de estos dos tipos de errores (sesgo y varianza).


## Dos métodos: regresión lineal y k-vecinos más cercanos {-}

Consideraremos 
dos métodos relativamente simples (@ESL) para hacer construir predicciones:

- $k$-vecinos más cercanos
- mínimos cuadrados regularizados para regresión lineal

Ambos casos, veremos cómo podemos balancear sesgo y varianza para obtener los
mejores resultados predictivos. Hablaremos también de modelos más estructurados
y menos estructurados.


## k  vecinos más cercanos {-}

Podemos empezar ahora con una manera de predecir muy simple, pero 
que nos servirá para ilustrar algunos principios. 

Una primera idea de un predictor simple es tomar $f$ constante
igual a la media de los datos de entrenamiento:

$$f(\mathbf{x}) =\frac{1}{n}\sum_{i=1}^n y_i$$

Si queremos usar las variables de entrada que tenemos disponibles, podríamos en lugar de
esto calcular promedios pero sólo de casos que sean similares al que queremos predecir. 

Podemos definir un número de casos a utilizar $k$ (*número de vecinos cercanos*). Para
las entradas $\mathbf{x}$ encontraríamos los $k$ casos de entrenamiento cuyas $x^{(i)}$ están
más cerca de $\mathbf{x}$. Entonces haríamos nuestras predicciones como:

$${f}(\mathbf{x}) = \frac{1}{k} \sum_{i \in N_k(\mathbf{x})} y^{(i)}$$

Nótese que lo que estamos intentando
estimar es *la media condicional de la respuesta dadas los datos de entrada*, que tiene
sentido pues estamos usando la pérdida cuadrática. 

Este método se llama $k$-vecinos más cercanos. El valor $k$ es un parámetro de complejidad: 

- Cuanto más grande sea $k$, reducimos la varianza de los promedios, pero aumentamos el riesgo sesgo al promediar datos más lejanos que pueden ser irrelevantes
- Cuanto más chico sea $k$, disminuimos el riesgo de sesgo, pero aumentamos potencialmente la varianza de los promedios (promediamos menos casos).



## Ejemplo k-vmc {-}

En nuestro ejemplo del SAT, podríamos querer predecir la calificación promedio de los alumnos que toman
el examen ($Y$) en función de $X$= fracción de alumnos que toman el examen. Trabajamos con una muestra de los datos.

```{r, fig.width = 4, fig.height = 3, message = FALSE}
set.seed(923)
sat_completos <- read_csv("datos/sat.csv") 
sat <- sat_completos %>% sample_frac(0.6)
ggplot(sat, aes(x = frac, y = sat)) + geom_point()
```

Para $k$ vecinos más cercanos, supongamos que queremos predecir en frac = 30, usando 5 vecinos más cercanos. Buscamos los
3 vecinos más cercanos en *frac* y promediamos para obtener una predicción:

```{r}
frac_x <- 30
pred <- sat %>% mutate(dist = abs(frac - frac_x)) %>% filter(rank(dist) <= 5) %>% 
    summarise(sat_pred = mean(sat)) %>% mutate(frac = frac_x)
pred
```

Ponemos en rojo nuestra predición, y luego repetimos para el rango de *frac* para  mostrar
cómo serían las predicciones de este método para distintos valores de *frac*. 

```{r, fig.width = 8, fig.height = 3}
g <-  ggplot(sat, aes(x = frac)) + geom_point(aes(y=sat))
g_una <- g + geom_point(data = pred, aes(y = sat_pred), colour = "red", size = 4)

# k vecinos más cercanos
mod_knn_sat <- caret::knnreg(sat ~ frac, data = sat, k = 5)
preds <- tibble(frac = seq(3, 80,1)) %>% mutate(sat_pred = predict(mod_knn_sat, newdata = .))
g_rango <- g_una + geom_line(data = preds, aes(y = sat_pred), colour = "red", size = 1)
gridExtra::grid.arrange(g_una, g_rango, ncol = 2)
```


- ¿Qué defectos o ventajas piensas que tiene este predictor? 
- ¿Piensas que este modelo ayuda a comparar la diferencia que existe en desempeño por estado
según el porcentaje de alumnos que toma el examen?


## Ejemplo: Ozono {-}

Considéremos los siguientes datos clásicos de Radiación Solar, Temperatura, Velocidad del Viento y 
Ozono para distintos días en Nueva York (@chambers83):

```{r}
library(tidyverse)
air_data <- airquality %>% 
    mutate(Wind_cat = cut(Wind, quantile(Wind, c(0, 0.33, 0.66, 1)), include.lowest = T)) %>% 
    filter(!is.na(Ozone) & !is.na(Solar.R))
air <- air_data
```

```{r, fig.width = 6, fig.height = 3}
ggplot(air, aes(x = Solar.R, y = Ozone,  colour = Temp)) + geom_point() +
    facet_wrap(~Wind_cat, ncol = 3) + 
    scale_colour_gradientn(colours = rainbow(2, rev = TRUE))
```

Esta una gráfica exploratoria útil,
qué nos permite entender estructura en los datos y hacer comparaciones útiles,
pero también sugiere qué podríamos intentar para predecir niveles de Ozono dependiendo de la resto
de las variables (Radiación Solar, Velocidad del viento y Temperatura).

 Podríamos estimar la media condicional
usando vecinos más cercanos. Conviene estandarizar las variables (convertirlas a la misma
escala) para que tenga más sentido calcular distancias. 

En este caso, usamos 27-vecinos más cercanos, y producimos 
predicciones para varios valores de velocidad del viento, temperatura
y radiación solar:

```{r, message = FALSE, echo = FALSE}
pred_grid <- expand.grid(Wind = c(5,10,15), Temp = seq(60, 90, 10), Solar.R = seq(20, 300, by = 10)) %>% 
    mutate(Wind_cat = cut(Wind, quantile(airquality$Wind, c(0, 0.33, 0.66, 1)), include.lowest = T))
air <- air_data

ajustar_modelo <- function(air, pred_grid, k = 25){
    vars <- c("Wind", "Temp", "Solar.R")
    air_s <- scale(air %>% select_if(is.numeric))
    air_medias <- attributes(air_s)$`scaled:center`
    air_scale  <- attributes(air_s)$`scaled:scale`
    air_s[, "Ozone"] <- air$Ozone
    mod_knn <- caret::knnreg(Ozone ~ Wind + Temp + Solar.R, data = air_s, k = k)
    new_data <- scale(pred_grid %>% select_if(is.numeric), 
                      center = air_medias[vars], scale = air_scale[vars]) %>% as.data.frame
    new_data <- bind_cols(new_data, pred_grid %>% select_if(negate(is.numeric)))
    pred_grid$Ozone <- predict(mod_knn, newdata = new_data)
    pred_grid
}
pred_grid_mod <- ajustar_modelo(air, pred_grid, k = 27)
```



```{r, echo =FALSE, fig.width = 6, fig.height = 3}
g_kvmc_1 <- ggplot(air, aes(x = Solar.R, y = Ozone,  colour = Temp)) + geom_point() +
    facet_wrap(~Wind_cat) + 
    scale_colour_gradientn(colours = rainbow(2, rev = TRUE)) +
    geom_line(data = pred_grid_mod, aes(y = Ozone, group = interaction(Temp, Wind_cat)), size = 1) +
    labs(subtitle = "Curvas de k-vmc, para viento = 5, 10, 15")
g_kvmc_1
```


## Ejemplo: Ozono con sobre/subajuste {-}

Si tomamos $k$ demasiado grande, nuestro modelo presenta poca variabilidad pero sesgos claros:

```{r, fig.width = 6, fig.height = 3}
pred_grid_mod <- ajustar_modelo(air, pred_grid, k = 80)
g_kvmc <- ggplot(air, aes(x = Solar.R, y = Ozone,  colour = Temp)) + geom_point() +
    facet_wrap(~Wind_cat) + 
    scale_colour_gradientn(colours = rainbow(2, rev = TRUE)) +
    geom_line(data = pred_grid_mod, aes(y = Ozone, group = interaction(Temp, Wind_cat)), size = 1) +
    labs(subtitle = "Curvas de k-vmc, para viento = 5, 10, 15")
g_kvmc
```

Si tomamos $k$ demasiado chico, nuestro modelo presenta mucha variabilidad:

```{r, fig.width = 6, fig.height = 3}
pred_grid_mod <- ajustar_modelo(air, pred_grid, k = 2)
g_kvmc <- ggplot(air, aes(x = Solar.R, y = Ozone,  colour = Temp)) + geom_point() +
    facet_wrap(~Wind_cat) + 
    scale_colour_gradientn(colours = rainbow(2, rev = TRUE)) +
    geom_line(data = pred_grid_mod, aes(y = Ozone, group = interaction(Temp, Wind_cat)), size = 1) +
    labs(subtitle = "Curvas de k-vmc, para viento = 5, 10, 15")
g_kvmc
```

Estos dos últimos modelos probablemente se desempeñarán
mal con respecto al que construimos con $k=27$ (podemos
usar una muestra de prueba para seleccionar el mejor de los modelos).


## Mínimos cuadrados regularizados para regresión lineal {-}

Inducir estructura adecuada en nuestros predictores puede ser buena idea. Comenzamos
con probar hacer predicciones mediante modelos lineales.

Si tenemos entradas $x_1,\ldots, x_p$, buscamos
predictores que son promedios ponderados de las variables de entrada:

$$f_b(x) = f_b(x_1,\ldots, x_p) = b_0 + b_1 x_1 + \cdots + b_p x_p$$

Con un conjunto de datos de entrenamiento $\mathcal{L}  = \{({x}^{(1)}, y^{1)}), ({x}^{(2)}, y^{2)}), \ldots, ({x}^{(n)}, y^{n)})\}$, intentamos encontrar los coeficientes $\beta$ minimizando
el error de entrenamiento, o sea resolviendo le problema de **mínimos cuadrados**:

$$\min_{b_0,b_1,\ldots, b_p} \frac{1}{n}\sum_{i=1}^n (y^{(i)} - f_b(x^{(i)}))^2$$
Sin embargo, usar este modelo tiene el riesgo, como vimos arriba, de sobreajustar, pues
estamos **directamente intentando minimizar el error de entrenamiento**, que no es lo
que queremos. Esto se expresa en posible varianza alta de los coeficientes y como consecuencia, varianza en las predicciones.

Una solución es regularizar. Dado un parámetro $\lambda > 0$ fijo, podemos
minimizar, en lugar de la fórmula de arriba, su versión *penalizada*

$$\min_{b_0,b_1,\ldots, b_p} \frac{1}{n}\sum_{i=1}^n (y^{(i)} - f_b(x^{(i)}))^2 + \lambda\sum_{i=1}^p b_i^2$$

Las b's (coeficientes) que resuelven este problema (que es convexo y tiene un solo mínimo) se pueden calcular analíticamente
derivando e igualando a cero. Entonces el predictor que usamos es 

$${f}(x ) = \hat{b}_0 +\ {\hat{b}_1} x_1 + \cdots + \hat{b}_p x_p$$

-  Queremos que $\lambda$ nos impida "sobreajustar" los datos, al poner una
restriccion en cuanto a qué tan grandes pueden ser los coeficientes. 
- Reduce la varianza de los coeficientes, pues la penalización impide que tomen
valores muy positivos o negativos. Si $\lambda$ es muy grande, por ejemplo, los coeficientes tendrán que ser muy cercanos a cero.
- Este tipo de regularización se le llama **regularización ridge**
- Las variables de entrada deben tener la misma escala para que esta regularización tenga sentido (se pueden normalizar de alguna forma si no es el caso).
- Usualmente no penalizamos $b_0$, aunque es posible hacerlo. De esta forma el modelo con $\lambda$ muy grande da la media de las $y$'s como predicción.

Nótese que $\lambda$ es un parámetro que controla flexibilidad: $\lambda$ más chica implica mayor varianza y menos sesgo potencial, mientras que $\lambda$ muy grande implica menos varianza pero sesgo potencial alto.


## Ejemplo: Mínimos cuadrados {-}

Ajustamos primero un modelo lineal sin regularización:

```{r, message = FALSE}
modelo_formula <- as.formula(Ozone  ~  Temp + I(Temp^2) + Wind + I(Wind^2)+
              Solar.R + I(Solar.R^2) + Temp:Wind + Solar.R:Temp +
              Wind:Solar.R)
mod_sin_reg <- lm(modelo_formula, data = air)
```

Y examinamos las predicciones del modelo (líneas):

```{r, echo = FALSE}
pred_grid <- expand.grid(Wind = c(5,10,15), Temp = seq(60, 90, 10), Solar.R = seq(20, 300, by = 10)) %>% 
    mutate(Wind_cat = cut(Wind, quantile(airquality$Wind, c(0, 0.33, 0.66, 1)), include.lowest = T))
pred_grid$Ozone <- predict(mod_sin_reg, pred_grid)
g_lineal <- ggplot(air, aes(x = Solar.R, y = Ozone,  colour = Temp)) + geom_point() +
    facet_wrap(~Wind_cat) + 
    scale_colour_gradientn(colours = rainbow(2, rev = TRUE)) +
    geom_line(data = pred_grid, aes(y = Ozone, group = interaction(Temp, Wind_cat)), size = 1) +
    labs(subtitle = "Curvas de modelo lineal, para viento = 5, 10, 15") 
g_lineal
```


Nótese que algunos aspectos de este modelo parecen ser ruidosos: por ejemplo,
el comportamiento de las curvas para el primer pánel (donde hay pocos datos
de temperatura baja), el hecho de que en algunos casos parece haber 
curvaturas decrecientes e incluso predicciones negativas. No deberíamos poner mucho crédito en las predicciones
de este modelo, y tiene peligro de producir predicciones desastrosas.


## Regularización para ejemplo de Ozono {-}

Ahora usemos dos parametros de regularización y hacemos mínimos
cuadrados regularizados. Tomamos $\lambda = 1$ y  $\lambda = 13000$. Usaremos
el paquete @glmnet para hacer regresión ridge.


```{r, message = FALSE}
library(glmnet)
mat_ozono <- model.matrix(modelo_formula, data = air)
mod_reg <- glmnet(x = mat_ozono, y = air$Ozone, alpha = 0, lambda = exp(seq(-20,20, 10)))
```


```{r, message = FALSE, echo = FALSE, fig.width = 7, fig.height = 6}
pred_grid <- expand.grid(Wind = c(5,10,15), Temp = seq(60, 90, 10), Solar.R = seq(20, 300, by = 10)) %>% mutate(Ozone = 0)
pred_mat <- model.matrix(modelo_formula, pred_grid)
preds_ozono <- predict(mod_reg, newx = pred_mat, s = 1) %>% as.numeric
pred_grid$Ozone <- preds_ozono
pred_grid <- pred_grid %>% 
    mutate(Wind_cat = cut(Wind, quantile(airquality$Wind, c(0, 0.33, 0.66, 1)), include.lowest = T))
g_lineal_reg_1 <- ggplot(air, aes(x = Solar.R, y = Ozone,  colour = Temp)) + geom_point() +
    facet_wrap(~Wind_cat) + 
    scale_colour_gradientn(colours = rainbow(2, rev = TRUE)) +
    geom_line(data = pred_grid, aes(y = Ozone, group = interaction(Temp, Wind_cat)), size = 1) +
    labs(subtitle = "Lineal regularizado, lambda = 1") 
```


```{r, echo = FALSE}
preds_ozono <- predict(mod_reg, newx = pred_mat, s = 13000) %>% as.numeric
pred_grid$Ozone <- preds_ozono
pred_grid <- pred_grid %>% 
    mutate(Wind_cat = cut(Wind, quantile(airquality$Wind, c(0, 0.33, 0.66, 1)), include.lowest = T))
g_lineal_reg_5 <- ggplot(air, aes(x = Solar.R, y = Ozone,  colour = Temp)) + geom_point() +
    facet_wrap(~Wind_cat) + 
    scale_colour_gradientn(colours = rainbow(2, rev = TRUE)) +
    geom_line(data = pred_grid, aes(y = Ozone, group = interaction(Temp, Wind_cat)), size = 1) +
    labs(subtitle = "Lineal regularizado, lambda = 13000") 
```


Y comparamos las predicciones de los tres modelos:

```{r, fig.height= 10, fig.width = 7}
gridExtra::grid.arrange(g_lineal, g_lineal_reg_1, g_lineal_reg_5, ncol = 1)
```

¿Qué modelo crees que se desempeñe mejor? ¿Cuál va a producir mejores comparaciones predictivas?

## ¿Cómo seleccionar los hiperparámetros? {-}

Los hiperparámetros en este caso son valores que establecemos para hacer
el ajuste del modelo particular después. En el ejemplo de $k$ vecinos más
cercanos, $k$ es el único hiperparámetro, y en regresión regularizada
$\lambda$ es el único hiperparámetro. 

La manera más simple es la siguiente: si tenemos un conjunto de datos grande,
los dividimos en tres partes al azar,

- Entrenamiento (50\%)
- Validación (25\%)
- Prueba (25\%)

1. Ajustamos varios modelos con distintos grados de regularización (por ejemplo $\lambda$)
2. Seleccionamos la $\lambda$ que da mejor el error en la muestra de validación
3. Repetimos 1-2 con otras familias de modelos (por ejemplo k-vecinos más cercanos)
4. Probamos un número chico de modelos con la muestra de prueba para obtener estimaciones
finales de su error de predicción.

Esta división en 3 partes nos protege se "sobreajustar" cuando hacemos iteraciones
entrenamiento-validación. Al final siempre tenemos una estimación "honesta" del 
desempeño predictivo.

**Observación**: en algunos casos, tenemos que ser cuidadosos con cómo separamos
las muestras de entrenamiento y validación/prueba. Idealmente, queremos 
que **esta separación refleje en lo posible la manera en que las predicciones van
a ser producidas**. Por ejemplo: 

- Si los datos de entrenamiento son caras repetidas
de varias personas (por ejempo, 20 caras para cada una de 500 personas), y queremos crear un clasificador de caras, entonces tendremos cuidado de seleccionar
dividir al azar las *personas*, no las *caras* individuales. 
- Si queremos hacer pronósticos para cantidad de series de tiempo, tendremos cuidado en que las observaciones de validación y prueba están son posteriores a los datos de entrenamiento.


## Ejemplo: selección del parámetro $\lambda$ de regularización {-}

En nuestro caso de mediciones de Ozono, si tuviéramos una muestra de validacion tomaríamos
varias $lambda$ y ajustaríamos un modelo para cada una de ellas. Escogemos la $lambda$
que nos da el menor error de validación:

```{r, message = FALSE, fig.width =5, fig.height = 3}
set.seed(11)
seleccion_reg <- cv.glmnet(x = mat_ozono, y = air$Ozone, alpha = 0, lambda = exp(seq(2,-4, -0.1)), nfolds = 10)
datos_val <- tibble(lambda = seleccion_reg$lambda, error_val = seleccion_reg$cvm)
ggplot(datos_val, aes(x = lambda, y = error_val)) + geom_point() +
  scale_x_log10()
```

**Observaciones**: 

- En este ejemplo particular, usamos validación cruzada, que funciona
como un sustituto de la muestra de validación (@ESL, Sección 7.10). 
- Estas estimaciones están sujetas a error. Si la muestra de validación es relativamente chica conviene calcular intervalos (por ejemplo con bootstrap).


## Métodos de predicción y la maldición de la dimensionalidad {-}

Notamos en primer lugar que en el problema de Ozono, los modelos seleccionados para k-vmc y regresión regularizada tienen predicciones simillares:

```{r, echo=FALSE, fig.width = 6, fig.height = 5}
gridExtra::grid.arrange(g_lineal_reg_1, g_kvmc_1, ncol = 1)
```

En segundo lugar, observamos que k-vecinos más cercanos es un método intuitivo y simple
(aunque quizá no tan computacionalmente conveniente), y que intenta estimar
directamente la cantidad de interés: la media condicional de las $y$ dada las $x$.

Entonces Ingenuamente podríamos preguntarnos ¿por qué tantos métodos? ¿por qué construir predicciones de alguna otra
manera que $k$-vecinos más cercanos, que estima directamente la cantidad de interés?
Y más aún: si tengo grandes cantidades de datos, este método debe funcionar muy bien, pues puedo usar promedios
a lo largo de varias variables que incluyan muchas observaciones. 

Y en parte esto es cierto para problemas de dimensión baja (digamos, 2,3 o 4 variables). El problema
es que generalmente nuestros problemas son de dimensionalidad más alta.

- Cuando la dimensión es no muy chica, todos los datos de entrenamiento tienden a ser *ralos*,
lo que quiere decir que todos los puntos están lejanos unos de otros. 
- Si la dimensión es alta, entonces no hay manera de evitar uno de dos problemas: o tomamos muchos vecinos,
y entonces las predicciones son extrapolaciones graves, o tomamos muy pocos, y nuestro modelo es poco confiable
y tiene alta variabilidad. Peor aún, aunque tomemos un vecino más cercano seguimos extrapolando gravemente.

Esto es contraintuitivo en un principio.

#### Ejemplo {-}

Este ejemplo es tomado de @ESL. Consideremos que la salida Y es determinística $Y = e^{-8\sum_{j=1}^p x_j^2}$.
Vamos a usar 1-vecino más cercano para hacer predicciones, con 
una muestra de entrenamiento de 1000 casos.
Generamos $x^{i}$‘s uniformes en $[ 1,1]$, para $p = 2$, y calculamos la respuesta 
$Y$ para cada caso:

```{r}
fun_exp <- function(x) exp(-8 * sum(x ^ 2))
x <- map(1:1000, ~ runif(2, -1, 1))
dat <- tibble(x = x) %>% 
        mutate(y = map_dbl(x, fun_exp))
ggplot(dat %>% mutate(x_1 = map_dbl(x, 1), x_2 = map_dbl(x, 2)), 
       aes(x = x_1, y = x_2, colour = y)) + geom_point()
```

La mejor predicción en $x_0 = (0,0)$ es $f((0,0)) = 1$. Eñ vecino más
cercano al origen es
```{r vmcbajadim}
dat <- dat %>% mutate(dist_origen = map_dbl(x, ~ sqrt(sum(.x^2)))) %>% 
  arrange(dist_origen)
mas_cercano <- dat[1, ]
mas_cercano; mas_cercano$x[[1]]
```
Nuestra predicción es entonces $\hat{f}(0)=$ `r mas_cercano$y`, que
es bastante cercano al valor verdadero (1).

Ahora intentamos hacer lo mismo para dimensión $p=8$.

```{r vmcalta}
x <- map(1:1000, ~ runif(8, -1, 1))
dat <- tibble(x = x) %>% 
       mutate(y = map_dbl(x, fun_exp))
dat <- dat %>% mutate(dist_origen = map_dbl(x, ~ sqrt(sum(.x^2)))) %>% 
  arrange(dist_origen)
mas_cercano <- dat[1, ]
mas_cercano; mas_cercano$x[[1]]
```

Y el resultado es catastrófico. Nuestra predicción puntual es
```{r}
mas_cercano$y
```

Necesitariamos una muestra de alrededor de  un millón de casos
para obtener resultados no tan malos.

¿Qué es lo que está pasando? La razón es que en dimensiones altas, 
los puntos de la muestra de entrenamiento están muy lejos 
unos de otros, y están cerca de la frontera, 
incluso para tamaños de muestra relativamente grandes como n = 1000.
Cuando la dimensión crece, la situación empeora exponencialmente.

En dimensiones altas, todos los conjuntos de entrenamiento factibles
se distribuyen de manera rala en el espacio de entradas.


## Estructura en análisis predictivo{-}

Estas observaciones apuntan a que  métodos locales simples tenerán a funcionar mal
en cuanto tenemos más de unas cuantas variables. ¿Qué salida puede tener este problema?

- Si el problema tiene alguna estructura explotable (que se puede descubrir mediante sugerencias
de teoría y expertos, exploración y conocimiento de los datos, y experimentación), es posible superar el
la maldición de la dimensionalidad.
- En primer lugar, porque tenemos mejores guías para escoger que variables usamos para predecir, y
cuáles probablemente es mala idea incluír
- En segundo lugar, porque podemos construir modelos más parsimoniosos que exploten la estructura del problema.

### Ejemplo {-}
Este segundo ejemplo también es de @ESL. Supongamos que estamos otra vez en un problema de dimensión alta

Ahora intentamos algo similar con una función que es razonable aproximar
con una función lineal. Esta función *solo depende de la primera entrada, y las demás
componentes de $x$ son ruido en términos del problema de predicción:

```{r}
fun_cuad <- function(x)  0.5 * (1 + x[1])^2
```

Y queremos predecir para $x=(0,0,\ldots,0)$, cuyo valor exacto es

```{r}
fun_cuad(0)
```

Los datos se generan de la siguiente forma:

```{r}
simular_datos <- function(p = 40){
    x <- map(1:1000,  ~ runif(p, -1, 1))
    dat <- tibble(x = x) %>% mutate(y = map_dbl(x, fun_cuad)) 
    dat
}
```

Por ejemplo, para dimensión $p=1$ (nótese que una aproximación
lineal no es tan mala):

```{r, fig.width= 4, fig.height = 3}
ejemplo <- simular_datos(p = 1) %>% mutate(x = unlist(x))
ggplot(ejemplo, aes(x = x, y = y)) + geom_point() +
    geom_smooth(method = "lm")
```


Ahora simulamos el proceso en dimensión $p=40$: simulamos las entradas, y aplicamos un vecino más cercano

```{r}
vmc_1 <- function(dat){
    dat <- dat %>% 
        mutate(dist_origen = map_dbl(x, ~ sqrt(sum(.x^2)))) %>% 
        arrange(dist_origen)
        mas_cercano <- dat[1, ]
        mas_cercano$y
}
set.seed(834)
dat <- simular_datos(p = 40)
vmc_1(dat)
```

Este no es un resultado muy bueno (muy lejos de 0.5). Sin embargo, regresión se
desempeña considerablemente mejor:

```{r}
regresion_pred <- function(dat){
    p <- length(dat$x[[1]])
    dat_reg <- cbind(
        y = dat$y, 
        x = matrix(unlist(dat$x), ncol = p, byrow=T)) %>% 
        as.data.frame()
    mod_lineal <- lm(y ~ ., dat = dat_reg)
    origen <- data.frame(matrix(rep(0, p), 1, p))
    names(origen) <- names(dat_reg)[2:(p+1)]
    predict(mod_lineal, newdata = origen)
}
regresion_pred(dat)
```

Donde podemos ver que típicamente la predicción de regresión
es mucho mejor que la de 1 vecino más cercano. **Ejercicio**: prueba con otras semillas


## ¿Cuál es la solución al problema de dimensión alta? {-}

Igual que en análisis exploratorio,
este problema se mitiga cuando descubrimos estructura apropiada para el problema y 
construimos esta estructura dentro de los modelos que queremos utilizar. Ejemplos son:

- Modelos lineales cuando la estructura lineal es apropiada (muchos ejemplos de éxito): mediante
*feature engineering* o *expansión de entradas* es posible construir una estructura lineal creible. Los modelos lineales
son mucho más poderosos de los que en una mirada rápida puede sugerir.
- Redes neuronales para procesamiento de señales (imágenes, sonido): los éxitos en este campo
se deben a una combinación virtuosa entre conjuntos de datos grandes y estructuras apropiadas,
como son unidades convolucionales que toman en cuenta la estructura espacial del problema
- Reducción de dimensionalidad: distintos métodos que buscan reducir la dimensión sin perder mucha información. Existen muchos ejemplos (por ejemplo, cuando hay muchas variables correlacionadas), por
ejemplo: sistemas de recomendación
- Reducción de dimensionalidad supervisada, como en problemas de procesamiento de texto, donde
recientemente hay muchos éxitos. Los métodos de reducción de dimensionalidad inspirados en 
modelos estadísticos del lenguaje (probabilidades de ocurrencia de palabras dadas el contexto, *embeddings*) son usados de manera general, así como redes neuronales que buscan capturar y utilizar
información secuencialmente cercana.
- Métodos basados en árboles, que buscan explícitamente explotar interacciones, 
pueden capturar relaciones no lineas y son robustos a datos atípicos en las covariables, y promedian sobre muestras bootstrap para controlar varianza.

## Resumen {-}

Las tres componentes que discutimos entonces son:

- Tener la información adecuada, y extraerla de manera apropiada para usar como covariables en nuestros modelos
- Utilizar modelos con estructura apropiada para nuestro problema (incluye crear nuevas variables a partir de otras para que esto tenga sentido)
- Diagnosticar correctamente problemas de sesgo y varianza, y seleccionar hiperparámetros apropiadamente. La siguiente gráfica resume estos puntos:


```{r, fig.height = 3, fig.width = 8, echo = FALSE}
varianza <- function(x) {
    x^2.5 + 1
}
sesgo <- function(x) {
    1/(x^3)
}
sesgo_var <- tibble(x = seq(0.5, 2, by = 0.01)) %>% 
    mutate(sesgo = sesgo(x), varianza = varianza(x), `error de prediccion` = sesgo + varianza + 1) %>% 
    gather(fuente, error, sesgo:`error de prediccion`) %>% mutate(metodo = "1. Método genérico")

sesgo_var <-tibble(x = seq(0.5, 2, by = 0.01)) %>% 
    mutate(sesgo = 0.5*sesgo(x), varianza = varianza(x^0.7) - 1, `error de prediccion` = sesgo + varianza + 2) %>% 
    gather(fuente, error, sesgo:`error de prediccion`) %>% mutate(metodo = "2. Método estructurado") %>% 
    bind_rows(sesgo_var)
sesgo_var <-tibble(x = seq(0.5, 2, by = 0.01)) %>% 
    mutate(sesgo = 0.5*sesgo(x), varianza = varianza(x^0.3) - 1, `error de prediccion` = sesgo + varianza + 0.5) %>% 
    gather(fuente, error, sesgo:`error de prediccion`) %>% mutate(metodo = "3. Método estructurado \n Mejores variables") %>% 
    bind_rows(sesgo_var)

ggplot(sesgo_var, 
       aes(x = x, y = error, colour = fuente)) + geom_line(size = 1) +
    scale_color_colorblind() + xlab("Complejidad del modelo") + facet_wrap(~metodo) + ylab("Error de predicción")
```




## ¿Cómo mejorar un modelo predictivo? {-}

En primer lugar, hay consideraciones que producen mejoras en general (independientemente de métodos particulares
y casi de analistas particulares!)

- Pensar con detalle in la información que hay en las covariables, y cuánta información útil contienen
para hacer predicciones.¿ Existen variables importantes que no tenemos disponibles? ¿Es posible construir o conseguir variables 
relacionadas con esas característias importantes?
- Considerar cómo usar estructura en nuestros modelos: Por ejemplo: ¿estamos usando correctamente información 
espacial o temporal? ¿Un modelo lineal es apropiado? 

Otras acciones tienen efectos sobre distintas componentes el error, y buscan corregir distintos aspectos. 
Por ejemplo, si queremos reducir varianza:

- Podemos intentar modelos más simples
- Reducir o eliminar variables
- Penalizar la complejidad de los modelos para obtener ajustes más simples
- Conseguir más datos

Si nuestro problema es sesgo, sin embargo, estas acciones pueden no ser efectivas. En este caso:

- Usar modelos más complejos
- Penalizar menos por complejidad
- Crear variables derivadas (por ejemplo, interacciones)

La pregunta es: ¿Cómo diagnosticamos si nuestro problema es sesgo o varianza (o ambos)? Algunos tips son:

**Signos de que la varianza es alta (sobreajuste)**:

- El error de prueba tiende a ser considerablemente más alto que el error de entrenamiento. El modelo se "pega" mucho
a los datos de entrenamiento, y el error parece bajo. Sin embargo, cuando lo probamos con otra muestra el error se degrada.
- El modelo cambia mucho con pequeñas perturbaciones de los datos de entrenamiento (por ejemplo, si hacemos bootstrap las 
predicciones varían mucho).

**Signos de que el sesgo es alta (subajuste)**:

- El error de prueba tiende a ser similar al de entrenamiento. 
- El modelo es muy estable, por ejemplo bajo replicaciones bootstrap de los datos. Sin embargo, hay partes en los datos
donde el modelo consistemente sobrepredice y subpredice. 
- Residuales (errores individuales) de entrenamiento tienen una distribución con colas que tienden a ser largas.





## Intervalos predictivos {-}

En algunos problemas de predicción, especialmente cuando el error promedio tiende a ser alto, es importante
producir resúmenes acerca de la incertidumbre que tenemos al hacer las predicciones. En algunos casos esto es 
indispensable. 

Una manera de expresar la incertidumbre en la predicción es mediante la construcción de intervalos predictivos:
Por ejemplo:

Nuestra predicción puntual del precio de una casa es de 250 mil dólares. Un intervalo predictivo de 90\% de probabilidad
podría ir de 210 mil dólares a 270 mil: esto quiere decir, la menos *nominalmente*, que hay un 90\% de probabilidad
de que el intervalo cubra a la observación verdadera. Esto puede ser útil para tomar decisiones o incluir nuestras
predicciones en procesos posteriores. 

Hay varias maneras de construir estos intervalos. Lo más importante es:

- Siempre checamos que la cobertura *nominal* de los intervalos construidos sea cercana a la *real*. Usamos
la muestra de prueba para este propósito.


En el caso de modelos de regresión, una buena alternativa es utilizar **regresión bayesiana** 
(ver por ejemplo @GelmanHill para un curso interesante, y @Bishop para un enfoque más de *machine learning*). Los
métodos de regularización como ridge o lasso caben naturalmente en este contexto (en una forma más poderosa), y 
producir intervalos predictivos es muy natural en este contexto.

En otros casos (por ejemplo, redes neuronales o métodos basados en árboles), existe avance pero no hay
un enfoque establecido. En el siguiente ejemplo, mostramos un ejemplo de construcción de intervalos
de predicción para un modelo de bosques aleatorios (ver por ejemlo @rfintervals, @rfintervalpkg).


## Ejemplo: precios de casas {-}

Consideramos un modelo para predecir precios de casas basadas in un bosque aleatorio:

```{r, message = FALSE, echo = FALSE}
source("R/casas_preprocesamiento.R")
set.seed(21)
casas_completo <- casas
```

Separamos muestra de entrenamiento y prueba:

```{r, message = FALSE, warning = FALSE}
casas_t <- casas_completo %>% sample_frac(0.9)
casas_p <- casas_completo %>% anti_join(casas_t)
```

Ajustamos un bosque aleatorio y construimos intervalos predictivos usando el paquete 
[rfinterval](https://github.com/haozhestat/rfinterval), @rfintervalpkg :

```{r}
library(rfinterval)
intervals_sc <- rfinterval(precio_miles ~ area_habitable_sup_m2 + calidad_gral +
                               nombre_zona + condicion_gral + aire_acondicionado +
                               area_lote_m2 + baños_completos + calidad_calefaccion, 
                   train_data = casas_t, test_data = casas_p,
                   params_ranger = list(mtry = 5),
                   method = "quantreg")
sc_i <- intervals_sc$quantreg_interval
preds <- intervals_sc$testPred
sc_i$precio_miles <- casas_p$precio_miles
sc_i$area <- casas_p$area_habitable_sup_m2
sc_i$pred <- preds
mean(abs(sc_i$pred - sc_i$precio_miles)) / mean(sc_i$precio_miles)
ggplot(sc_i, aes(x = pred, ymin = lower, ymax = upper, y = precio_miles)) +
    geom_point(colour = "red") +
    geom_linerange(colour = "gray") 
sc_i %>% summarise(cobertura = mean(precio_miles < upper & precio_miles > lower))
```

En este caso, los intervalos predictivos cumplen razonablemente bien su cobertura *nominal*, y desde
este punto de vista el modelo es apropiado.





