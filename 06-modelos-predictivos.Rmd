# Modelación predictiva

```{r setup6, message = FALSE}
library(tidyverse)
library(lubridate)
library(ggthemes)
theme_set(theme_minimal(base_size = 14))
```

En esta parte discutiremos las ideas básicas de los modelación predictiva. La tarea básica
del análisis predictivo es la siguiente: 

Tenemos un conjunto de datos observados

 $$\mathcal L = \{(\mathbf{x}_i, y_i)\}_{i=1}^n,$$
donde cada $\mathbf{x} = (x_1, x_2, \ldots, x_p)$ es un conjunto de variables observadas
(que llamamos covariables). 

Cuando observamos un nuevo conjunto de covariables $\mathbf{x}^0$,
cómo podemos utilizar la información de $\mathcal L$ para:

- ¿Predecir su correspondiente valor $y$, o más generalmente, 
- ¿Qué podemos decir acerca de los posibles valores que podría tomar $y$ dado que conocemos sus covariables asociadas?


** Comparaciones predictivas **

En análisis exploratorio vimos varios ejemplos donde buscábamos hacer **comparaciones**
del comportamiento de alguna variable dependiendo de distintos niveles de otras variables
(de forma **multivariada**).
En ese caso usamos métodos relativamente simples para hacer esas comparaciones que nos
ayudan a descubrir y entender **estructura** los datos. 

Podemos ver cómo el análisis predictivo es una extensión natural de este procedimiento. La
diferencia principal es que en este tipo de análisis nos concentramos principalmente
en **desempeño predictivo** y cómo evaluar correctamente este desempeño. En segundo plano
está evaluar la calidad de esas comparaciones y cómo podemos interpretarlas, aunque discutiremos
esto más adelante.

Ejemplos: ingresos de hogares, niveles de contaminantes, comportamiento de clientes (cancelaciones,
valor presente).

### Ejemplo {-}

Considéremos los siguientes dátos clásicos de Radiación Solar, Temperatura, Velocidad del Viento y 
Ozono para distintos días en Nueva York (@chambers83):

```{r}
library(tidyverse)
air_data <- airquality %>% 
    mutate(Wind_cat = cut(Wind, quantile(Wind, c(0, 0.33, 0.66, 1)), include.lowest = T)) %>% 
    filter(!is.na(Ozone) & !is.na(Solar.R))
air <- air_data
```

```{r, fig.width = 6, fig.height = 3}
ggplot(air, aes(x = Solar.R, y = Ozone,  colour = Temp)) + geom_point() +
    facet_wrap(~Wind_cat, ncol = 3) + 
    scale_colour_gradientn(colours = heat.colors(10, rev = TRUE))
```

Esta una gráfica exploratoria útil,
qué nos permite entender estructura en los datos y hacer comparaciones útiles,
pero también sugiere qué podríamos intentar para predecir niveles de Ozono dependiendo de la resto
de las variables (Radiación Solar, Velocidad del viento y Temperatura).

## Predicciones puntuales

Supongamos que buscamos hacer predicciones puntuales de una variable $Y$ mediante una función
$$f(X_1, \ldots, X_p)$$

Si consideramos a (X_1,X_2,\ldots, X_p, Y)$ como variables aleatorias, entonces buscamos
que $Y$ esté lo más cercana posible a $f(X_1, \ldots, X_p)$. La definición de *cercanía* importa,
pero por el momento tomemos el punto de vista más común en este tipo de problemas, y supongamos
que queremos minimizar los errores que observamos:

$$(Y - f(X_1, \ldots, X_p))^2 = (Y - f(X))^2$$
Esta es una variable aleatoria y depende de los valores que observamos. Tiene sentido entonces
buscar una $f$ que minimice, por ejemplo, el valor promedio de este error:

$$EPE = E((Y-f(X))^2)$$
La solución teórica a este problema se puede encontrar condicionando a $X$, y es

$$f(X_1, X_2, \ldots, X_p) = E[Y | X_1, X_2, \ldots, X_p]$$

Es decir: resolvemos el problema de predicción si encontramos esta función, que devuelve la media de la
respuesta $Y$ dados los valores de entrada $X$. Si escogiéramos la pérdida absoluta, por ejemplo,
obtendríamos $f(X_1,\ldots, X_p) = mediana(Y | X_1,\ldots, X_p)$.


## Media condicional y vecinos más cercanos

Ahora consideremos que tenemos un conjunto de datos $\mathcal{L} = \{(\mathbf{x}_i, y_i)\}_{i=1}^n$. 
Bajo los supuestos que hicimos, buscamos estimar la media condicional. En general no es factible
hacer una estimación directa, pues tpicamente encontraremos que para ciertos valores
$(X_1,ldots, X_p)$ no tenemos ninguna observación.

Una idea simple para hacer predicciones y comparaciones predictivas es entonces la siguiente:

Si queremos hacer una predicción en las entradas $x^0 = (x_1^0,x_2^0, \ldots, x_p^0)$, podemos
buscar los puntos de nuestros datos que tengan entradas similares, y producir promedios,
medianas, o el resumen que nos interese hacer:

$$\hat{f}(x_1^0,x_2^0, \ldots, x_p^0) = \frac{1}{k} \sum_{i \in N_k(x_0)} y_i$$

Si los puntos cercanos no están lejos dee $x^0$, y la función $E(Y|X_1,\ldots, X_p)$ es continua,
este es un método que puede ser razonable.

## Ejemplo 

Regresamos a nuestro ejemplo de mediciones de Ozono. Podríamos estimar la media condicional
usando vecinos más cercanos:

```{r}
pred_grid <- expand.grid(Wind = c(5,10,15), Temp = seq(60, 90, 10), Solar.R = seq(20, 300, by = 10)) %>% 
    mutate(Wind_cat = cut(Wind, quantile(airquality$Wind, c(0, 0.33, 0.66, 1)), include.lowest = T))
air <- air_data

ajustar_modelo <- function(air, pred_grid, k = 25){
    vars <- c("Wind", "Temp", "Solar.R")
    air_s <- scale(air %>% select_if(is.numeric))
    air_medias <- attributes(air_s)$`scaled:center`
    air_scale  <- attributes(air_s)$`scaled:scale`
    air_s[, "Ozone"] <- air$Ozone
    mod_knn <- caret::knnreg(Ozone ~ Wind + Temp + Solar.R, data = air_s, k = k)
    new_data <- scale(pred_grid %>% select_if(is.numeric), 
                      center = air_medias[vars], scale = air_scale[vars]) %>% as.data.frame
    new_data <- bind_cols(new_data, pred_grid %>% select_if(negate(is.numeric)))
    pred_grid$Ozone <- predict(mod_knn, newdata = new_data)
    pred_grid
}
pred_grid_mod <- ajustar_modelo(air, pred_grid, k = 27)
```

Y podemos producir predicciones para varios valores de velocidad del viento, temperatura
y radiación solar:

```{r}
ggplot(air, aes(x = Solar.R, y = Ozone,  colour = Temp)) + geom_point() +
    facet_wrap(~Wind_cat) + 
    scale_colour_gradientn(colours = heat.colors(10, rev = TRUE)) +
    geom_line(data = pred_grid_mod, aes(y = Ozone, group = interaction(Temp, Wind_cat)), size = 1) +
    labs(subtitle = "Curvas de k-vmc, para viento = 5, 10, 15")
```


---

**Observación**: nótese que los modelos predictivos nos dan la oportunidad de hacer mejores comparaciones
con los datos, donde controlamos por varias variables para entender cómo cambia la respuesta. En este caso particular, de cómo cambia la media condicional de Ozono con las
variables de temperatura, radiación y velocidad del viento. A este tipo de 
comparaciones les llamamos **comparaciones predictivas**. Para que estas sean efectivas, sin
embargo, necesitamos hacer más trabajo de validación que discutimos más adelante. 

- Más en la sección de inferencia de modelos predictivos (cobertura de intervalos).
- Más acerca de inclusión de variables relevantes y teoría acerca del proceso
- Aleatorización?

## Evaluación de modelos predictivos {-}

En nuestros contexto, la modelación predictiva se separa en dos pasos claramente separados:

- Construir con algunos datos $\mathcal{L} = \{(\mathbf{x}_i, y_i)\}_{i=1}^n$ una función $\hat{f}$.
- Evaluar el desempeño de $\hat{f}$ como predictor.

En el ejemplo anterior usamos $k$-vecinos más cercanos. Una aproximación ingenua para medir
la calidad de la función $\hat{f}$ que construimos sería calcular el **error de entrenamiento**,
que está dado por

$$\overline{err} = \frac{1}{n} \sum_i (y_i - \hat{f}(x_i))^2,$$

**y esto es una muy mala idea**. La razón es que cuando decimos **predicción** nos referimos a que
vamos a aplicar el modelo para casos nuevos, que no hemos visto en la muestra de entrenamiento.
Como veremos, que este error sea bajo no es necesariamente indicación que el modelo tienen capacidad
de **generalizar** para casos nuevos

## Ejemplo {-}

Para este ejemplo simulado de una sola covariable, los puntos muestran los datos de entrenamiento.

```{r, fig.width=5, fig.asp=0.7}
set.seed(280572)
x <- c(1,7,10,0,0,5,9,13,2,4,17,18,1,2)
f <- function(x){
  ifelse(x < 10, 1000*sqrt(x), 1000*sqrt(10))
}
error <- rnorm(length(x), 0, 500)
y <- round(f(x) + error)
datos_entrena <- data.frame(x = x, y = y)
curva_1 <- geom_smooth(data = datos_entrena,
  method = "loess", se = FALSE, color = "gray", span = 1.2, size = 1.1)
curva_2 <- geom_smooth(data = datos_entrena,
  method = "loess", se = FALSE, color = "red", span = 0.5, size = 1.1)
curva_3 <- geom_smooth(data = datos_entrena,
  method = "lm", se = FALSE, color = "blue", size = 1.1)
head(datos_entrena)
```


Usamos tres métodos distintos para construir un predictor, que dan las 3 curvas que vemos abajo.
¿Cuál preferirías usar para hacer predicciones?

```{r, fig.width=5, fig.asp=0.7, warning = FALSE}
ggplot(datos_entrena, aes(x=x, y=y)) + geom_point() + 
    curva_1 + curva_2 + curva_3
```

Calculamos primero los errores de entrenamiento de cada curva:


Calculamos los errores de entrenamiento de cada curva:

```{r, warning = FALSE}
mod_rojo <- loess(y ~ x, data = datos_entrena, span=0.3)
mod_gris <- loess(y ~ x, data = datos_entrena, span=1)
mod_recta <- lm(y ~ x, data = datos_entrena)
df_mods <- tibble(nombre = c('recta', 'rojo', 'gris'))
df_mods$modelo <- list(mod_recta, mod_rojo, mod_gris)
```

```{r}
error_f <- function(df, mod){
  function(mod){
    preds <- predict(mod, newdata = df)
    round(sqrt(mean((preds - df$y) ^ 2)))
  }
}
error_ent <- error_f(datos_entrena)
df_mods <- df_mods %>% 
  mutate(error_entrena = map_dbl(modelo, error_ent))
df_mods
```

y vemos que el modelo  con menor error de entrenamiento es la curva roja que parece ser demasiado
sensible a variación en los datos.

Ahora simulamos una nueva muestra grande y calculemos el error:


Sin embargo, consideremos que tenemos una nueva muestra (de prueba).

```{r, fig.width=5, fig.asp=0.7}
set.seed(218052272)
x_0 <- sample(0:13, 2000, replace = T)
error <- rnorm(length(x_0), 0, 500)
y_0 <- f(x_0) + error
datos_prueba <- tibble(x = x_0, y = y_0)
datos_prueba
```

```{r}
error_p <- error_f(datos_prueba)
df_mods <- df_mods %>% 
  mutate(error_prueba = map_dbl(modelo, error_p))
df_mods
```

y vemos que el modelo con mejor desempeño está dado por la curva gris, lo cual coincide con nuestra
intuición.


## La tarea del análisis predictivo

En resumen, la tarea del análisis predictivo es la siguiente: dados 

- Una función de pérdida $L(predicción, observado)$,
- Datos de entrenamiento $\mathcal{L} = \{(\mathbf{x}_i, y_i)\}_{i=1}^n$
- Datos de prueba $\mathcal{T} = \{(\mathbf{x'}_i, y'_i)\}_{i=1}^m$

Queremos construir un $\mathcal{L} \to \hat{f} $ (sólo depende de datos de entrenamiento) tal que

$$\hat{Err} = \frac{1}{m} \sum_{i = 1}^m L(\hat{f}(\mathbf{x'}_i) , y'_i))$$

sea lo más chico posible. Buscamos buen desempeño **fuera de la muestra** con la que construimos
el modelo. Como veremos, el error de entrenamiento es útil como herramienta de diagnóstico, pero 
en general no sirve para evaluar desempeño futuro de nuestros predictores.

Esta formulación de la tarea del análisis predictivo ha sido muy productiva (@donoho50), y ha permitido
avances grandes en muchos problemas interesantes desde hace una cuantas décadas.

Otra ventaja de esta formulación es que es relativamente fácil de implementar
para agregar contribuciones de muchas personas. Sitios como [Kaggle](https://www.kaggle.com/) son implementaciones donde:

- Hay un conjunto de datos disponibles, con los que cualquiera puede construir modelos
- Existen competidores que se inscriben y producen reglas de predicción
- Hay un referee que evalúa las reglas de los concursantes usando datos a los que sólo el referee
tiene acceso.
- (En algunos casos) Al final del concurso, los competidores muestran su metodología.

**Ejemplo**: el concurso de Netflix (2009) siguió este patrón. De este concurso se desarrolló 
parcialmente el área ahora floreciente de sistemas de recomendación.



## La maldición de la dimensionalidad

Alquien podría preguntarse: ¿por qué construir estas predicciones de alguna otra
manera que $k$-vecinos más cercanos? ¿Si mis datos no son muy chicos, entonces este 
método debería funcionar razonablemente bien? El principal problema es que usualmente
trabajamos con más de 3 o 4 variables.

- Cuando la dimensión es no muy chica, todos los datos de entrenamiento tienden a ser *ralos*,
lo que quiere decir que todos los puntos están lejanos unos de otros. 

Podemos ver un ejemplo (@ESL):

#### Ejemplo {-}
 Consideremos que la salida Y es determinística $Y = e^{-8\sum_{j=1}^p x_j^2}$.
Vamos a usar 1-vecino más cercano para hacer predicciones, con 
una muestra de entrenamiento de 1000 casos.
Generamos $x^{i}‘s uniformes en $[ 1,1]$, para $p = 2$, y calculamos la respuesta 
$Y$ para cada caso:

```{r}
fun_exp <- function(x) exp(-8 * sum(x ^ 2))
x <- map(1:1000, ~ runif(2, -1, 1))
dat <- tibble(x = x) %>% 
        mutate(y = map_dbl(x, fun_exp))
ggplot(dat %>% mutate(x_1 = map_dbl(x, 1), x_2 = map_dbl(x, 2)), 
       aes(x = x_1, y = x_2, colour = y)) + geom_point()
```

La mejor predicción en $x_0 = (0,0)$ es $f((0,0)) = 1$. Eñ vecino más
cercano al origen es
```{r vmcbajadim}
dat <- dat %>% mutate(dist_origen = map_dbl(x, ~ sqrt(sum(.x^2)))) %>% 
  arrange(dist_origen)
mas_cercano <- dat[1, ]
mas_cercano; mas_cercano$x[[1]]
```
Nuestra predicción es entonces $\hat{f}(0)=$ `r mas_cercano$y`, que
es bastante cercano al valor verdadero (1).

Ahora intentamos hacer lo mismo para dimensión $p=8$.

```{r vmcalta}
x <- map(1:1000, ~ runif(8, -1, 1))
dat <- tibble(x = x) %>% 
       mutate(y = map_dbl(x, fun_exp))
dat <- dat %>% mutate(dist_origen = map_dbl(x, ~ sqrt(sum(.x^2)))) %>% 
  arrange(dist_origen)
mas_cercano <- dat[1, ]
mas_cercano; mas_cercano$x[[1]]
```

Y el resultado es catastrófico. Nuestra predicción puntual es
```{r}
mas_cercano$y
```

Necesitariamos una muestra de alrededor de  un millón de casos
para obtener resultados no tan malos.

¿Qué es lo que está pasando? La razón es que en dimensiones altas, 
los puntos de la muestra de entrenamiento están muy lejos 
unos de otros, y están cerca de la frontera, 
incluso para tamaños de muestra relativamente grandes como n = 1000.
Cuando la dimensión crece, la situación empeora exponencialmente.

En dimensiones altas, todos los conjuntos de entrenamiento factibles
se distribuyen de manera rala en el espacio de entradas.


## ¿Cuál es la solución al problema de dimensión alta?

Estas observaciones apuntan a que  métodos simples y locales tienden a funcionar mal
en cuanto tenemos más de unas cuantas variables.

La solución a este problema no tiene atajos mágicos: igual que en análisis exploratorio,
este problema se mitiga cuando descubrimos estructura apropiada para el problema y 
construimos esta estructura dentro de los modelos que queremos utilizar. Ejemplos son:

- Modelos lineales cuando la estructura lineal es apropiada (muchos ejemplos de éxito: mediante
*feature engineering* es posible construir una estructura lineal creible). Los modelos lineales
son mucho más poderosos de los que en una mirada rápida
- Redes neuronales para procesamiento de señales (imágenes, sonido): los éxitos en este campo
se deben a una combinación virtuosa entre conjuntos de datos grandes y estructuras apropiadas,
como son unidades convolucionales que toman en cuenta la estructura espacial del problema
- Reducción de dimensionalidad: distintos métodos que buscan reducir la dimensión sin perder mucha información. Existen muchos ejemplos (por ejemplo, cuando hay muchas variables correlacionadas), por
ejemplo: sistemas de recomendación
- Reducción de dimensionalidad supervisada, como en problemas de procesamiento de texto, donde
recientemente hay muchos éxitos. Los métodos de reducción de dimensionalidad inspirados en 
modelos estadísticos del lenguaje (probabilidades de ocurrencia de palabras dadas el contexto, *embeddings*) son usados de manera general, así como redes neuronales que buscan capturar y utilizar
información secuencialmente cercana.


## ¿Por qué pueden ser malas las predicciones? {-}

Recordemos que planteamos nuestro problema de predicción como uno donde queremos aproximar la 
función $f$ con una función $\hat{f}$ construida con los datos de entrenamiento. ¿Por qué puede
ser grande este error?

Consideremos un modelo particular que genera datos para nuestro problema que es útil para entender
de dónde provienen los errores de predicción:

$$Y = f(X_1, \ldots, X_p) + \epsilon$$
donde por suponemos que $E(\epsilon | X_1,\ldots X_p) = 0$. Esta variable aleatoria representa efectos
de otras variables que no tenemos disponibles, y que afectan el resultado $Y$ que observamos. Si consideramos el error esperado de predicción bajo pérdida cuadrática, podemos escribir:

$$Y-\hat{f}(X) = (f(X)- \hat{f}(X)) + \epsilon$$

Vemos en primer lugar dos razones para errores grandes:

- La $\epsilon$ puede tomar valores grandes (pues no tenemos variables importantes que influyen en $Y$)
- $f(X) - \hat{f}(X) $ puede ser grande, es decir, nuestra estimación de $f$ es mala. A su vez,
hay dos razones por las que puede ocurrir esto:

- $\hat{f}(X)$ tiene poca variabilidad y está consistemente lejos de la verdadera $f$.
- La variabilidad de $\hat{f}(X)$ es grande, de forma que potencialmente puede estar lejos del valor $f(X)$
- Una combinación de los dos incisos anteriores.


Una manera de cuantificar esto es mediante la descomposición varianza-sesgo. Con algunos cálculos simples,
podemos demostrar que:

$$E((Y-\hat{f}(x))^2|X=x) = (E(\hat{f}(x)) - f(x))^2 + Var(\hat{f}(x)) + Var(\epsilon|X=x)$$

donde el valor esperado es sobre el conjunto de entrenamiento ${\mathcal L}$ y $\epsilon$, que suponemos independientes.

- El primer término le llamamos *sesgo* del predictor (si está consistentemente lejos de $f(x)$)
- El segundo término es la *varianza* del predictor (qué tan sensible es a la muestra de entrenamiento)
- Al tercer término le llamamos *error irreducible*.


## Ejemplo {-}

```{r}
x_0 <- 7
```
En nuestro ejemplo anterior, consideremos qué pasa cuando hacemos predicciones en x = `r x_0`. Usaremos
varias muestras de entrenamiento y vemos cómo cambia la predicción:


```{r, fig.width=5, fig.asp=0.7, warning = FALSE}
x <- c(1,7,10,0,0,5,9,13,2,4,17,18,1,2)
x_g <- seq(0,15,0.5)
y_g <- f(x_g)
dat_g <- tibble(x = x_g, y = y_g)
f <- function(x){
  ifelse(x < 10, 1000*sqrt(x), 1000*sqrt(10))
}

simular <- function(rep){
    error <- rnorm(length(x), 0, 500)
    y <- round(f(x) + error)
    datos_entrena <- data.frame(x = x, y = y)
    curva_1 <- loess(y ~ x, data = datos_entrena, degree = 2, span = 1)
    curva_2 <- loess(y ~ x, data = datos_entrena, degree = 2, span = 0.5)
    curva_3 <- lm(y ~ x, data = datos_entrena)
    modelos <- list(curva_1, curva_2, curva_3)
    x_in <- tibble(x = x_0)
    preds <- map_dbl(modelos, ~ predict(.x, newdata = x_in))
    tibble(rep = rep, metodo = c("gris","rojo", "recta"), preds = preds, error = preds - f(7))
}
simular_curvas <- function(rep, x_g){
    error <- rnorm(length(x), 0, 500)
    y <- round(f(x) + error)
    datos_entrena <- data.frame(x = x, y = y)
    curva_1 <- loess(y ~ x, data = datos_entrena, degree = 2, span = 1.2)
    curva_2 <- loess(y ~ x, data = datos_entrena, degree = 2, span = 0.5)
    curva_3 <- lm(y ~ x, data = datos_entrena)
    modelos <- list(curva_1, curva_2, curva_3)
    x_in <- tibble(x = x_g)
    preds <- map(modelos, ~ tibble(pred = predict(.x, newdata = x_in))) %>% 
             map(~ bind_cols(x_in, .x))
    tibble(rep = rep, metodo = c("gris","rojo", "recta"), preds = preds)
}
sims <- map(1:500, ~ simular(.x)) %>% bind_rows
sims_curvas <- map(1:20, ~ simular_curvas(.x, x_g)) %>% bind_rows %>% unnest
```

```{r}
ggplot(sims_curvas, aes(x = x)) +
    geom_line(aes(y = pred, group = interaction(metodo, rep), colour = metodo)) +
     geom_line(data = dat_g, aes(y = y), size = 2) + facet_wrap(~metodo)
```


Ahora vemos las predicciones comparadas con el óptimo, que es f(7):

```{r}
ggplot(sims, aes(x= 7, y = preds)) + 
    geom_line(data = dat_g %>% filter(x > 4 & x < 11), aes(x = x, y = y)) +
    geom_boxplot(aes(colour = metodo)) 
    #geom_hline(yintercept = f(7), colour = "red") +
    #annotate("text", x = 5, y = f(7), colour = "red", label = "valor correcto", hjust = 1, vjust = -0.2)d
```

Y confirmamos que:

- En este punto, el error preditivo del modelo rojo es alto porque su varianza es alto (depende muy fuertemente
de los datos)
- El error predictivo de la recta es malo porque está consistemente lejos de la curva verdadera
- El modelo gris parece tener ligero sesgo, pero menos que el modelo lineal, y varía menos que
la curva roja.

## Subajuste y sobreajuste {-}

Aunque la descomposición de sesgo, varianza y error irreducible no aplica a todas las funciones
de pérdida (por ejemplo en problemas donde la respuesta es una variable categórica), nos provee una guía
para pensar en modelos predictivos y su desempeño

**Rigidez, sesgo y subajuste**

Todos estos nombres se refieren a ideas similares: los modelos que usamos no tienen la flexibilidad 
suficiente para aprender patrones que se generalizan y ayudan en la predicción, aún cuando haya mucha 
evidencia y datos que sostengan ese patrón. 

**Flexibilidad, varianza y sobreajuste**

Estos se refieren a modelos que son demasiado flexibles y dependen demasiado fuertemente de los datos, capturando características. Tienden a ser inestables (varían mucho con los datos) o variables, y 
asociadas a variación muestral o variación que no depende de variables conocidas.

Generalmente, cuando trabajamos en análisis predictivo, ajustes al modelo
intercambian una forma de error por otro. Buscamos un balance entre los dos para optimizar las predicciones:


```{r, fig.height = 3, fig.width = 10}
varianza <- function(x) {
    x^2.5 + 1
}
sesgo <- function(x) {
    1/(x^3)
}
sesgo_var <- tibble(x = seq(0.5, 2, by = 0.01)) %>% 
    mutate(sesgo = sesgo(x), varianza = varianza(x), `error de prediccion` = sesgo + varianza + 1) %>% 
    gather(fuente, error, sesgo:`error de prediccion`) %>% mutate(metodo = "1. Método genérico")

sesgo_var <-tibble(x = seq(0.5, 2, by = 0.01)) %>% 
    mutate(sesgo = 0.5*sesgo(x), varianza = varianza(x^0.7) - 1, `error de prediccion` = sesgo + varianza + 2) %>% 
    gather(fuente, error, sesgo:`error de prediccion`) %>% mutate(metodo = "2. Método estructurado") %>% 
    bind_rows(sesgo_var)
sesgo_var <-tibble(x = seq(0.5, 2, by = 0.01)) %>% 
    mutate(sesgo = 0.5*sesgo(x), varianza = varianza(x^0.3) - 1, `error de prediccion` = sesgo + varianza + 0.5) %>% 
    gather(fuente, error, sesgo:`error de prediccion`) %>% mutate(metodo = "3. Método estructurado \n Mejores variables") %>% 
    bind_rows(sesgo_var)

ggplot(sesgo_var, 
       aes(x = x, y = error, colour = fuente)) + geom_line(size = 1) +
    scale_color_colorblind() + xlab("Complejidad del modelo") + facet_wrap(~metodo)
```


