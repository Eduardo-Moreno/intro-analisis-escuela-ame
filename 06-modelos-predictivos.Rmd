# Modelación predictiva

En esta parte discutiremos las ideas básicas de los modelos predictivos. 
Hacer modelación
de datos muchas veces se requiere para extraer la mayor parte de información útil para poder
hacer predicciones acerca de: 

- Unidades sobre las que no tenemos información sobre la variable de interés
- Unidades que no hemos visto todavía

En análisis exploratorio usamos algunos modelos para ayudarnos a entender los datos,
pero se basa principalmente en hacer resúmenes relativamente simples de los datos.
En contraste, en modelación 
predictiva el modelo juega un papel más central: si queremos
extraer la información y patrones eficientemente, necesitamos buscar 
más sistemáticamente estructuras apropiadas
para generalizar la información de la muestra a nuevas unidades sobre las que queremos
hacer predicciones.

El análisis predictivo lo podemos ver también como un ejercicio más fino de comparación. 
Muchas veces con los datos a la mano es difícil de producir las comparaciones apropiadas 
que quisiéramos hacer, pues con métodos simples es difícil refinar las comparaciones controlando
por muchas variables. Con un modelo adecuado, podemos parcialmente suplir esta dificultad.



## Vecinos más cercanos

Una idea simple para hacer predicciones y comparaciones predictivas es la siguiente:

Si queremos hacer una predicción en las entradas $x^0 = (x_1^0,x_2^0, \ldots, x_p^0)$, podemos
buscar los puntos de nuestros datos que tengan entradas similares, y producir promedios,
medianas, o el resumen que nos interese hacer:

$$\hat{f}(x_1^0,x_2^0, \ldots, x_p^0) = \frac{1}{k} \sum_{i \in N_k(x_0)} y_i$$

## Ejemplo 

```{r, fig.width = 6, fig.height = 3}
library(tidyverse)
library(ranger)
library(pdp)
air_data <- airquality %>% 
    mutate(Wind_cat = cut(Wind, quantile(Wind, c(0, 0.33, 0.66, 1)), labels = FALSE, include.lowest = T)) %>% 
    filter(!is.na(Ozone) & !is.na(Solar.R))
air <- air_data
ggplot(air, aes(x = Solar.R, y = Ozone,  colour = Temp)) + geom_point() +
    facet_wrap(~Wind_cat, ncol = 3) + 
    scale_colour_gradientn(colours = terrain.colors(10))
```

Consideremos cómo haríamos comparaciones predictivas usand $k$ vecinos más cercanos.

```{r}
pred_grid <- expand.grid(Wind = c(5,10,14), Temp = seq(60, 90, 10), Solar.R = seq(20, 300, by = 10)) %>% 
    mutate(Wind_cat = cut(Wind, quantile(airquality$Wind, c(0, 0.33, 0.66, 1)), labels = FALSE, include.lowest = T))
air <- air_data

ajustar_modelo <- function(air, pred_grid, k = 25){
    air_s <- scale(air)
    air_s[, "Ozone"] <- air$Ozone
    air_medias <- attributes(air_s)$`scaled:center`
    air_scale  <- attributes(air_s)$`scaled:scale`
    mod_knn <- caret::knnreg(Ozone ~ Wind + Temp + Solar.R, data = air_s, k = k)
    vars <- c("Wind", "Temp", "Solar.R")
    new_data <- scale(pred_grid, center = c(air_medias[vars],0), scale = c(air_scale[vars], 1)) %>%     as.data.frame
    pred_grid$Ozone <- predict(mod_knn, newdata = new_data)
    pred_grid
}
pred_grid_mod <- ajustar_modelo(air, pred_grid, k = 27)
```



```{r}
ggplot(air, aes(x = Solar.R, y = Ozone,  colour = Temp)) + geom_point() +
    facet_wrap(~Wind_cat) + 
    scale_colour_gradientn(colours = terrain.colors(10)) +
    geom_line(data = pred_grid_mod, aes(y = Ozone, group = interaction(Temp, Wind_cat)), size = 1) 
```

```{r}
reps <- map(1:500, function(i){
    air <- sample_n(air_data, nrow(air_data), replace = T)
    pred_grid <- ajustar_modelo(air, pred_grid, k = 30)
    pred_grid$rep <- i
    pred_grid
}) %>% bind_rows

```


```{r}
reps_sum <- reps %>% ungroup %>% group_by(Wind, Temp, Solar.R, Wind_cat) %>% 
    summarise(Ozone_mean = mean(Ozone), ymax = quantile(Ozone, 0.9), ymin = quantile(Ozone, 0.1))
ggplot(reps_sum, aes(x = Solar.R, y = Ozone_mean,  colour = Temp)) +
    geom_point(alpha = 0.5) +
    facet_wrap(~Wind_cat) + 
    geom_linerange(aes(ymax = ymax, ymin = ymin)) +
    scale_colour_gradientn(colours = terrain.colors(10))
```

Estos son los niveles medios esperados, usando bootstrap para obtener intervalos 
para el valor esperado.

** Es posible construir intervalos de predicción ** - discutir después


## La maldición de la dimensionalidad

Alquien podría preguntarse: ¿por qué construir estas predicciones de alguna otra
manera que $k$-vecinos más cercanos? ¿Si mis datos no son muy chicos, entonces este 
método debería funcionar razonablemente bien? El principal problema es que usualmente
trabajamos con más de 3 o 4 variables.

- Cuando la dimensión es no muy chica, todos los datos de entrenamiento tienden a ser *ralos*,
lo que quiere decir que todos los puntos están lejanos unos de otros. 

Podemos ver un ejemplo (@ESL):

#### Ejemplo {-}
 Consideremos que la salida Y es determinística $Y = e^{-8\sum_{j=1}^p x_j^2}$.
Vamos a usar 1-vecino más cercano para hacer predicciones, con 
una muestra de entrenamiento de 1000 casos.
Generamos $x^{i}‘s uniformes en $[ 1,1]$, para $p = 2$, y calculamos la respuesta 
$Y$ para cada caso:

```{r}
fun_exp <- function(x) exp(-8 * sum(x ^ 2))
x <- map(1:1000, ~ runif(2, -1, 1))
dat <- tibble(x = x) %>% 
        mutate(y = map_dbl(x, fun_exp))
ggplot(dat %>% mutate(x_1 = map_dbl(x, 1), x_2 = map_dbl(x, 2)), 
       aes(x = x_1, y = x_2, colour = y)) + geom_point()
```

La mejor predicción en $x_0 = (0,0)$ es $f((0,0)) = 1$. Eñ vecino más
cercano al origen es
```{r}
dat <- dat %>% mutate(dist_origen = map_dbl(x, ~ sqrt(sum(.x^2)))) %>% 
  arrange(dist_origen)
mas_cercano <- dat[1, ]
mas_cercano; mas_cercano$x[[1]]
```
Nuestra predicción es entonces $\hat{f}(0)=$ `r mas_cercano$y`, que
es bastante cercano al valor verdadero (1).

Ahora intentamos hacer lo mismo para dimensión $p=8$.

```{r}
x <- map(1:1000, ~ runif(8, -1, 1))
dat <- tibble(x = x) %>% 
       mutate(y = map_dbl(x, fun_exp))
dat <- dat %>% mutate(dist_origen = map_dbl(x, ~ sqrt(sum(.x^2)))) %>% 
  arrange(dist_origen)
mas_cercano <- dat[1, ]
mas_cercano; mas_cercano$x[[1]]
```

Y el resultado es catastrófico. Nuestra predicción puntual es
```{r}
mas_cercano$y
```

Necesitariamos una muestra de alrededor de  un millón de casos
para obtener resultados no tan malos.

¿Qué es lo que está pasando? La razón es que en dimensiones altas, 
los puntos de la muestra de entrenamiento están muy lejos 
unos de otros, y están cerca de la frontera, 
incluso para tamaños de muestra relativamente grandes como n = 1000.
Cuando la dimensión crece, la situación empeora exponencialmente.

En dimensiones altas, todos los conjuntos de entrenamiento factibles
se distribuyen de manera rala en el espacio de entradas.


## ¿Cuál es la solución al problema de dimensión alta?

Estas observaciones apuntan a que  métodos simples y locales tienden a funcionar mal
en cuanto tenemos más de unas cuantas variables.

La solución a este problema no tiene atajos mágicos: igual que en análisis exploratorio,
este problema se mitiga cuando descubrimos estructura apropiada para el problema y 
construimos esta estructura dentro de los modelos que queremos utilizar. Ejemplos son:

- Modelos lineales cuando la estructura lineal es apropiada (muchos ejemplos de éxito: mediante
*feature engineering* es posible construir una estructura lineal creible). Los modelos lineales
son mucho más poderosos de los que en una mirada rápida
- Redes neuronales para procesamiento de señales (imágenes, sonido): los éxitos en este campo
se deben a una combinación virtuosa entre conjuntos de datos grandes y estructuras apropiadas,
como son unidades convolucionales que toman en cuenta la estructura espacial del problema
- Reducción de dimensionalidad: distintos métodos que buscan reducir la dimensión sin perder mucha información. Existen muchos ejemplos (por ejemplo, cuando hay muchas variables correlacionadas), por
ejemplo: sistemas de recomendación
- Reducción de dimensionalidad supervisada, como en problemas de procesamiento de texto, donde
recientemente hay muchos éxitos. Los métodos de reducción de dimensionalidad inspirados en 
modelos estadísticos del lenguaje (probabilidades de ocurrencia de palabras dadas el contexto, *embeddings*) son usados de manera general, así como redes neuronales que buscan capturar y utilizar
información secuencialmente cercana.


## Construcción de modelos







