# Modelación predictiva

¿Qué significa tener éxito en una tarea predictiva? 
¿Cuáles son los ingredientes básicos para tener éxitos en la tarea prediciva?

En la tarea general del análisis predictivo, hablamos de lograr **predicciones precisas**, y ese enfoque
es poderoso en parte porque es simple, y es un lugar donde tenemos que poner esfuerzo. 

Pero después tenemos que considerar que en nuestros problemas aplicados, tener éxito significa
usualmente que la inclusión de predicciones en el proceso de toma decisiones muestra
mejoras claras en métricas importantes *con respecto a lo que se hacía antes* (uso de reglas simples, completar tareas con
intervención discrecional de personas). Esto significa que generalmente estamos trabajando contra un **estándar dado**:

- Usar probabilidades de cancelación (modelo) de cuenta para ofrecer incentivos, en lugar de usar reglas simples 
(como días sin usar la tarjeta, el teléfono, etc.)
- Asignar programas sociales según predicciones de vulnerabilidades y capacidades futuras de los hogares, 
en lugar de usar reglas simples como localización geográfica o estimaciones simples de ingreso.
- Dar créditos según juicios de un gerente de banco en lugar de mediante un modelo de calificación crediticia.
- Acercarnos a un *benchmark* humano de clasificación de textos o imágenes de manera comparable a una persona, 
traducir textos a niveles comparables a un humano (según páneles de jueces), etc.

Aunque en el caso de procesos más maduros que incorporan modelos predictivos, muchas veces el éxito de un
esfuerzo es posible reducirlo a mejoras en la precisión de las predicciones.


## ¿Qué se necesita para lograr buenos resultados con el enfoque predictivo?

Más adelante hablaremos de la construcción y evaluación de modelos. Consideramos primero los ingredientes
básicos para tener buenos resultados con el enfoque predictivo:

- Tenemos la información correcta acerca de la variable que queremos predecir. Esto implica generalmente que 
nuestro esfuerzo se hace en un contexto donde la recolección y el análisis de datos
ya se hace de manera sistemática y adaptada al problema.

- El enfoque predictivo se enriquece con un ciclo de aplicación de modelo, medición de métricas, y mejora constante. Este ciclo
produce más datos, a veces diferentes, y el objetivo es dirigir esfuerzos en recolectar más datos importantes y adaptarnos
a cambios en el sistema.

- Finalmente, y quizá más centrado en la tarea fundamental del análisis predictivo, es necesario experimentar con distintos
enfoque de procesamiento de la información disponible y el uso de distintos modelos y enfoques predictivos.





## Predicciones y comparaciones



```{r setup6, message = FALSE}
library(tidyverse)
library(lubridate)
library(ggthemes)
library(glmnet)
theme_set(theme_minimal(base_size = 14))
```



Tenemos un conjunto de datos observados

 $$\mathcal{L}  = \{({x}^{(1)}, y^{1)}), ({x}^{(2)}, y^{2)}), \ldots, ({x}^{(n)}, y^{n)})\}$$

 
donde cada $x^{(i)} = (x^{(i)}_1, x^{(i)}_2, \ldots, x^{(i)}_p)$ es un conjunto de variables observadas
(que llamamos covariables). 

Cuando observamos un nuevo conjunto de covariables ${x}$,
cómo utilizar la información de $\mathcal L$ para:

- ¿Predecir su correspondiente valor $y$, o más generalmente, 
- ¿Qué podemos decir acerca de los posibles valores que podría tomar $y$ dado que conocemos sus covariables asociadas?

**Comparaciones predictivas**

En análisis exploratorio vimos varios ejemplos donde buscábamos hacer **comparaciones**
del comportamiento de alguna variable dependiendo de distintos niveles de otras variables
(de forma **multivariada**).
En ese caso usamos métodos relativamente simples para hacer esas comparaciones que nos
ayudan a descubrir y entender **estructura** en los datos. 

Podemos ver cómo el análisis predictivo es una extensión natural de este procedimiento. La
diferencia principal es que en este tipo de análisis nos concentramos principalmente
en **desempeño predictivo** y cómo evaluar correctamente este desempeño. En segundo plano
está evaluar la calidad de esas comparaciones y cómo podemos interpretarlas, aunque discutiremos
esto más adelante.

Ejemplos: ingresos de hogares, niveles de contaminantes, comportamiento de clientes (cancelaciones,
valor presente).

### Ejemplo {-}

Considéremos los siguientes dátos clásicos de Radiación Solar, Temperatura, Velocidad del Viento y 
Ozono para distintos días en Nueva York (@chambers83):

```{r}
library(tidyverse)
air_data <- airquality %>% 
    mutate(Wind_cat = cut(Wind, quantile(Wind, c(0, 0.33, 0.66, 1)), include.lowest = T)) %>% 
    filter(!is.na(Ozone) & !is.na(Solar.R))
air <- air_data
```

```{r, fig.width = 6, fig.height = 3}
ggplot(air, aes(x = Solar.R, y = Ozone,  colour = Temp)) + geom_point() +
    facet_wrap(~Wind_cat, ncol = 3) + 
    scale_colour_gradientn(colours = rainbow(2, rev = TRUE))
```

Esta una gráfica exploratoria útil,
qué nos permite entender estructura en los datos y hacer comparaciones útiles,
pero también sugiere qué podríamos intentar para predecir niveles de Ozono dependiendo de la resto
de las variables (Radiación Solar, Velocidad del viento y Temperatura).

## Predicciones puntuales {-}

Supongamos que buscamos hacer predicciones puntuales de una variable $Y$ mediante una función
$$f(X_1, \ldots, X_p) = f(X)$$

Si consideramos a $X = (X_1,X_2,\ldots, X_p, Y)$ como variables aleatorias, entonces buscamos
que $Y$ esté lo más cercana posible a $f(X)$. La definición de *cercanía* importa,
pero por el momento tomemos el punto de vista más común en este tipo de problemas, y supongamos
que queremos minimizar los errores que observamos:

$$(Y - f(X))^2$$
Esta es una variable aleatoria y depende de los valores que observamos. Tiene sentido entonces
buscar una $f$ que minimice, por ejemplo, el valor promedio de este error:

$$EPE = E((Y-f(X))^2)$$
O en términos de una muestra grande:

$$EPE \approx \hat{Err} = \frac{1}{m} \sum_{i=1}^m (\mathbf{y}^{(j)} - f(\mathbf{x}^{(j)}))^2$$

La solución teórica a este problema se puede encontrar condicionando a $X$, y es

$$f(X) = media(Y | X_1, X_2, \ldots, X_p])$$

Es decir: resolvemos el problema de predicción si encontramos esta función, que devuelve la media de la
respuesta $Y$ dados los valores de entrada $X$. 


## Media condicional y vecinos más cercanos {-}

Ahora consideremos que tenemos un conjunto de datos 

 $$\mathcal{L}  = \{({x}^{(1)}, y^{1)}), ({x}^{(2)}, y^{2)}), \ldots, ({x}^{(n)}, y^{n)})\}.$$

Bajo los supuestos que hicimos, buscamos estimar la media condicional. En general no es factible
hacer una estimación directa, pues tipicamente encontraremos que para ciertos valores
$(X_1,ldots, X_p)$ no tenemos ninguna observación.

Una idea simple para hacer predicciones y comparaciones predictivas es entonces la siguiente:

Si queremos hacer una predicción en las entradas $x^0 = (x_1^0,x_2^0, \ldots, x_p^0)$, podemos
buscar los puntos de nuestros datos que tengan entradas similares, y producir promedios,
medianas, o el resumen que nos interese hacer:

$$\hat{f}(\mathbf{x}) = \frac{1}{k} \sum_{i \in N_k(\mathbf{x})} y^{(i)}$$

Si los puntos cercanos no están lejos dee $x^0$, y la función $E(Y|X_1,\ldots, X_p)$ es continua,
este es un método que puede ser razonable.

## Ejemplo: caso simple {-}

En nuestro ejemplo del SAT, podríamos querer predecir la calificación promedio de los alumnos que toman
el examen ($Y$) en función de $X$= fracción de alumnos que toman el examen. Trabajamos con una muestra de los datos.

```{r, fig.width = 4, fig.height = 3}
set.seed(923)
sat_completos <- read_csv("datos/sat.csv") 
sat <- sat_completos %>% sample_frac(0.6)
ggplot(sat, aes(x = frac, y = sat)) + geom_point()
```

Para $k$ vecinos más cercanos, supongamos que queremos predecir en frac = 30, usando 5 vecinos más cercanos. Buscamos los
3 vecinos más cercanos en *frac* y promediamos para obtener una predicción:

```{r}
frac_x <- 30
pred <- sat %>% mutate(dist = abs(frac - frac_x)) %>% filter(rank(dist) <= 5) %>% 
    summarise(sat_pred = mean(sat)) %>% mutate(frac = frac_x)
pred
```

Ponemos en rojo nuestra predición, y luego repetimos para el rango de *frac* para  mostrar
cómo serían las predicciones de este método para distintos valores de *frac*. 

```{r, fig.width = 8, fig.height = 3}
g <-  ggplot(sat, aes(x = frac)) + geom_point(aes(y=sat))
g_una <- g + geom_point(data = pred, aes(y = sat_pred), colour = "red", size = 4)

# k vecinos más cercanos
mod_knn_sat <- caret::knnreg(sat ~ frac, data = sat, k = 5)
preds <- tibble(frac = seq(3, 80,1)) %>% mutate(sat_pred = predict(mod_knn_sat, newdata = .))
g_rango <- g_una + geom_line(data = preds, aes(y = sat_pred), colour = "red", size = 1)
gridExtra::grid.arrange(g_una, g_rango, ncol = 2)
```


- ¿Qué defectos o ventajas piensas que tiene este predictor? 
- ¿Piensas que este modelo ayuda a comparar la diferencia que existe en desempeño por estado
según el porcentaje de alumnos que toma el examen?


## Ejemplo: Ozono {-}

Regresamos a nuestro ejemplo de mediciones de Ozono. Podríamos estimar la media condicional
usando vecinos más cercanos. Conviene estandarizar las variables (convertirlas a la misma
escala) para que tenga más sentido calcular distancias:

```{r}
pred_grid <- expand.grid(Wind = c(5,10,15), Temp = seq(60, 90, 10), Solar.R = seq(20, 300, by = 10)) %>% 
    mutate(Wind_cat = cut(Wind, quantile(airquality$Wind, c(0, 0.33, 0.66, 1)), include.lowest = T))
air <- air_data

ajustar_modelo <- function(air, pred_grid, k = 25){
    vars <- c("Wind", "Temp", "Solar.R")
    air_s <- scale(air %>% select_if(is.numeric))
    air_medias <- attributes(air_s)$`scaled:center`
    air_scale  <- attributes(air_s)$`scaled:scale`
    air_s[, "Ozone"] <- air$Ozone
    mod_knn <- caret::knnreg(Ozone ~ Wind + Temp + Solar.R, data = air_s, k = k)
    new_data <- scale(pred_grid %>% select_if(is.numeric), 
                      center = air_medias[vars], scale = air_scale[vars]) %>% as.data.frame
    new_data <- bind_cols(new_data, pred_grid %>% select_if(negate(is.numeric)))
    pred_grid$Ozone <- predict(mod_knn, newdata = new_data)
    pred_grid
}
pred_grid_mod <- ajustar_modelo(air, pred_grid, k = 27)
```

Y podemos producir predicciones para varios valores de velocidad del viento, temperatura
y radiación solar:

```{r}
g_kvmc <- ggplot(air, aes(x = Solar.R, y = Ozone,  colour = Temp)) + geom_point() +
    facet_wrap(~Wind_cat) + 
    scale_colour_gradientn(colours = rainbow(2, rev = TRUE)) +
    geom_line(data = pred_grid_mod, aes(y = Ozone, group = interaction(Temp, Wind_cat)), size = 1) +
    labs(subtitle = "Curvas de k-vmc, para viento = 5, 10, 15")
g_kvmc
```


## Comparaciones predictivas {-}

Nótese que los modelos predictivos nos dan la oportunidad de hacer mejores comparaciones
con los datos, donde controlamos por varias variables para entender cómo cambia la respuesta.
En este caso particular, de cómo cambia la media condicional de Ozono con las
variables de temperatura, radiación y velocidad del viento. A este tipo de 
comparaciones les llamamos **comparaciones predictivas**. Para que estas sean efectivas, sin
embargo, necesitamos hacer más trabajo de validación que discutimos más adelante. 

- Más en la sección de inferencia de modelos predictivos (cobertura de intervalos).
- Más acerca de inclusión de variables relevantes y teoría acerca del proceso que genera los datos

El énfasis en comparaciones predictivas es importante para distinguirlo de *comparaciones causales*. Por ejemplo,
las comparaciones causales son necesarias cuando hacemos pregunta arerca de manipulaciones explícitas de
las covariables, por ejemplo:

- ¿Qué podríamos esperar si un estado particular (Alabama) introdujera un programa para incentivar 
que más alumnos tomen el SAT. ¿Qué pasaría con el promedio del SAT del estado?

Aún cuando nuestro modelo tenga buenas calidades predictivas y estadísticas, 
**esta es una pregunta mucha más difíci** que la que contesta una comparación predictiva. No se refiere a 
cómo son diferentes grupos de estados, si no a una misma unidad (estado) bajo distintas condiciones (@GelmanHill, cap 9).

Hablaremos un poco más de esto, pero por lo pronto: es importante tener cuidado en interpretar
comparaciones predictivas (que tienen utilidad) como causales (que en algunos casos son las que realmente nos interesa).


## Evaluación de modelos predictivos {-}

En nuestros contexto, la modelación predictiva se separa en dos pasos claramente separados:

- Construir con algunos datos $\mathcal{L} = \{({x}^{(1)}, y^{1)}), ({x}^{(2)}, y^{2)}), \ldots, ({x}^{(n)}, y^{n)})\}$ una 
función $\hat{f}$ para haer predicciones.
- Evaluar el desempeño de $\hat{f}$ como predictor.

En el ejemplo anterior usamos $k$-vecinos más cercanos. Una aproximación ingenua para medir
la calidad de la función $\hat{f}$ que construimos sería calcular el **error de entrenamiento**,
que está dado por

$$\overline{err} = \frac{1}{n} \sum_i (y^{(i)} - \hat{f}(x^{(i)})^2,$$

**y esto es una muy mala idea en general**. La razón es que cuando decimos **predicción** nos referimos a que
vamos a aplicar el modelo para casos nuevos, que no hemos visto en la muestra de entrenamiento.
Como veremos, que este error sea bajo no es necesariamente indicación que el modelo tienen capacidad
de **generalizar** para casos nuevos

## Ejemplo {-}

Para este ejemplo simulado de una sola covariable, los puntos muestran los datos de entrenamiento.

```{r, fig.width=5, fig.asp=0.7}
set.seed(280572)
x <- c(1,7,10,0,0,5,9,13,2,4,17,18,1,2)
f <- function(x){
  ifelse(x < 10, 1000*sqrt(x), 1000*sqrt(10))
}
error <- rnorm(length(x), 0, 500)
y <- round(f(x) + error)
datos_entrena <- data.frame(x = x, y = y)
curva_1 <- geom_smooth(data = datos_entrena,
  method = "loess", se = FALSE, color = "gray", span = 1.2, size = 1.1)
curva_2 <- geom_smooth(data = datos_entrena,
  method = "loess", se = FALSE, color = "red", span = 0.5, size = 1.1)
curva_3 <- geom_smooth(data = datos_entrena,
  method = "lm", se = FALSE, color = "blue", size = 1.1)
head(datos_entrena)
```


Usamos tres métodos distintos para construir un predictor, que dan las 3 curvas que vemos abajo.
¿Cuál preferirías usar para hacer predicciones?

```{r, fig.width=5, fig.asp=0.7, warning = FALSE}
ggplot(datos_entrena, aes(x=x, y=y)) + geom_point() + 
    curva_1 + curva_2 + curva_3
```

Calculamos primero los errores de entrenamiento de cada curva:


Calculamos los errores de entrenamiento de cada curva:

```{r, warning = FALSE}
mod_rojo <- loess(y ~ x, data = datos_entrena, span=0.3)
mod_gris <- loess(y ~ x, data = datos_entrena, span=1)
mod_recta <- lm(y ~ x, data = datos_entrena)
df_mods <- tibble(nombre = c('recta', 'rojo', 'gris'))
df_mods$modelo <- list(mod_recta, mod_rojo, mod_gris)
```

```{r}
error_f <- function(df, mod){
  function(mod){
    preds <- predict(mod, newdata = df)
    round(sqrt(mean((preds - df$y) ^ 2)))
  }
}
error_ent <- error_f(datos_entrena)
df_mods <- df_mods %>% 
  mutate(error_entrena = map_dbl(modelo, error_ent))
df_mods
```

y vemos que el modelo  con menor error de entrenamiento es la curva roja que parece ser demasiado
sensible a variación en los datos.

Ahora simulamos una nueva muestra grande y calculemos el error:


Sin embargo, consideremos que tenemos una nueva muestra (de prueba).

```{r, fig.width=5, fig.asp=0.7}
set.seed(218052272)
x_0 <- sample(0:13, 2000, replace = T)
error <- rnorm(length(x_0), 0, 500)
y_0 <- f(x_0) + error
datos_prueba <- tibble(x = x_0, y = y_0)
datos_prueba
```

```{r}
error_p <- error_f(datos_prueba)
df_mods <- df_mods %>% 
  mutate(error_prueba = map_dbl(modelo, error_p))
df_mods
```

y vemos que el modelo con mejor desempeño está dado por la curva gris, lo cual coincide con nuestra
intuición. Esto también nos podía hacer dudar del primer ejemplo donde usamos k-vecinos más cercanos: probablemente
hay patrones que dependen demasiado fuertemente de la muestra, y que no generalizan para otros datos.



## La tarea del análisis predictivo {-}

En resumen, la tarea del análisis predictivo es la siguiente: dados 

- Una función de error (o de pérdida) $Error(predicción, observado)$ para predicciones individuales
- Datos de entrenamiento $\mathcal{L}  = \{({x}^{(1)}, y^{1)}), ({x}^{(2)}, y^{2)}), \ldots, ({x}^{(n)}, y^{n)})\}$
- Datos de prueba $\mathcal{T} =  \{(\mathbf{x}^{(1)}, \mathbf{y}^{1)}), (\mathbf{x}^{(2)}, \mathbf{y}^{2)}), \ldots, (\mathbf{x}^{(m)}, \mathbf{y}^{m)})\}$

Queremos construir un $\mathcal{L} \to \hat{f} $ (sólo depende de datos de entrenamiento) tal que el promedio
de error sobre los datos de prueba:

$$\hat{Err} = \frac{1}{m} \sum_{i = 1}^m Error(\hat{f}(\mathbf{x}^{(j)}) , \mathbf{y}^{(j)})$$

sea lo más chico posible. Buscamos buen desempeño **fuera de la muestra** con la que construimos
el modelo. Como veremos, el error de entrenamiento es útil como herramienta de diagnóstico, pero 
en general no sirve para evaluar desempeño futuro de nuestros predictores.

Esta formulación de la tarea del análisis predictivo ha sido muy productiva (@donoho50), y ha permitido
avances grandes en muchos problemas interesantes desde hace una cuantas décadas.

Otra ventaja de esta formulación es que es relativamente fácil de implementar
para agregar contribuciones de muchas personas. Sitios como [Kaggle](https://www.kaggle.com/) son implementaciones donde:

- Hay un conjunto de datos disponibles, con los que cualquiera puede construir modelos
- Existen competidores que se inscriben y producen reglas de predicción
- Hay un referee que evalúa las reglas de los concursantes usando datos a los que sólo el referee
tiene acceso.
- (En algunos casos) Al final del concurso, los competidores muestran su metodología.

**Ejemplo**: el concurso de Netflix (2009) siguió este patrón. De este concurso se desarrolló 
parcialmente el área ahora floreciente de sistemas de recomendación.


## Dos métodos regresión lineal y k-vecinos más cercanos

Para entender cómo es que podemos tener éxito en la tarea fundamental del análisis predictivo, vamos a ver
dos métodos simples para hacer predicciones:

- $k$-vecinos más cercanos
- regresión lineal

El primer método lo explicamos en secciones anteriores, y se basa en la idea de intentar hacer predicciones
puntual tomando promedios de la respuesta para datos cercanos a donde queremos predecir. 


## Regresión lineal

Si tenemos entradas $X_1,\ldots, X_p$, buscamos
predictores que son promedios ponderados de las variables de entrada:

$$f_b(X_1,\ldots, X_p) = b_0 + b_1 X_1 + \cdots + b_p X_p$$

Con un conjunto de datos de entrenamiento $\mathcal{L}  = \{({x}^{(1)}, y^{1)}), ({x}^{(2)}, y^{2)}), \ldots, ({x}^{(n)}, y^{n)})\}$, intentamos encontrar los coeficientes $\beta$ minimizando
el error de entrenamiento, o sea resolviendo le problema de **mínimos cuadrados**:

$$\min_{b_0,b_1,\ldots, b_p} \frac{1}{n}\sum_{i=1}^n (y^{(i)} - f_b(x^{(i)}))^2$$

Las b's (coeficientes) que resuelven este problema (que es convexo y *típicamente* tiene un solo mínimo) se pueden calcular analíticamente
derivando e igualando a cero. Entonces el predictor que usamos es 

$$\hat{f}(X_1, X_2, \ldots, X_p ) = \hat{b}_0 +\ {\hat{b}_1} X_1 + \cdots + \hat{b} X_p$$

Es posible demostrar que las predicciones de este modelo son promedios ponderados de las $y_i$ observadas, pero 
con ponderadores que resultan de la estructura lineal:
$$\hat{y} = \hat{f} ({x}) = \frac{1}{n}\sum_{i=1} w_i y_i$$
donde cada  $w_i$ solo dependen de las entradas de entrenamiento $x^{(i)}, x^{(2)}, \cdots, x^{(n)}$

### Ejemplo {-}

```{r, message = FALSE, echo = FALSE, fig.width=4, fig.height = 3}
# De la respuesta https://stackoverflow.com/a/7549819
set.seed(821)
df <- data.frame(x = seq(1,100, 5))
df$y <- 2 + 3 * df$x + rnorm(nrow(df), sd = 50)
df$label <- ""
df$label[15] <- "list(x^(15), y^(15))"
p <- ggplot(data = df, aes(x = x, y = y)) + geom_smooth(method = "lm", se=FALSE) + 
  geom_point() + geom_text(aes(label = label), 
                           parse = TRUE, vjust = 1, hjust = -0.1)
lm_eqn <- function(df){
    m <- lm(y ~ x, df);
    eq <- substitute(y == a + b %.% x, 
         list(a = format(unname(coef(m)[1]), digits = 2),
              b = format(unname(coef(m)[2]), digits = 2)))
    as.character(as.expression(eq));
}
p + annotate("text", x = 25, y = 300, label = lm_eqn(df), parse = TRUE)
```

## Ejemplo: Ozono {-}

Podemos intentar ajustar un modelo lineal a los datos de ozono para hacer predicciones, como hicimos antes
para k vecinos más cercanos

```{r}
mod_1 <- lm(Ozone  ~  Temp + Wind + Solar.R + Temp:Wind + Solar.R:Temp, data = air)
```

Nótese algo interesante: el modelo es lineal, pero hicimos algo de *feature engineering* creando
entradas derivadas de nuestros datos: en este caso el producto de Temperatura por Viento y el de radiación solar por temperatura.
^odemos dibujar nuestro modelo sobre los datos, comparando con lo que obtuvimos con 27 vecinos más cercanos:

````{r, message = FALSE, fig.width = 7, fig.height = 6}
dat_1 <- air %>% filter(!is.na(Ozone) & !is.na(Solar.R))
pred_grid <- expand.grid(Wind = c(5,10,15), Temp = seq(60, 90, 10), Solar.R = seq(20, 300, by = 10)) %>% 
    mutate(Wind_cat = cut(Wind, quantile(airquality$Wind, c(0, 0.33, 0.66, 1)), include.lowest = T))
pred_grid$Ozone <- predict(mod_1, pred_grid)
g_lineal <- ggplot(air, aes(x = Solar.R, y = Ozone,  colour = Temp)) + geom_point() +
    facet_wrap(~Wind_cat) + 
    scale_colour_gradientn(colours = rainbow(2, rev = TRUE)) +
    geom_line(data = pred_grid, aes(y = Ozone, group = interaction(Temp, Wind_cat)), size = 1) +
    labs(subtitle = "Curvas de modelo lineal, para viento = 5, 10, 15") 
gridExtra::grid.arrange(g_kvmc, g_lineal, ncol = 1)
```

*Discutir*: ¿Qué primeras ventajas ves de un modelo sobre otro? ¿Computacionales? ¿Interpretación?
¿Calidad predictiva?

La diferencia no es tan grande en términos de las predicciones. 
Además de ventajas computacionales y quizá de interpretación (aunque hay más riesgo de sesgo en el modelo lineal), 
una pregunta inicial genuina
puede ser: ¿para qué usar métodos diferentes de vecinos más cercanos para predicción? La respuesta va
más allá de interpretabilidad o conveniencia computacional.

## Métodos de predicción y la maldición de la dimensionalidad {-}

Al menos hasta hace poco (aunque todavía es parcialmente cierto), habían una "industria" grande
de producción de distintos métodos de predicción (Ver por ejemplo @hundred). 

Ingenuamente, podríamos preguntarnos ¿por qué tantos métodos? ¿por qué construir estas predicciones de alguna otra
manera que $k$-vecinos más cercanos, que estima directamente la cantidad de interés?
Y más aún: si tengo grandes cantidades de datos, este método debe funcionar muy bien, pues puedo usar promedios
a lo largo de varias variables que incluyan muchas observaciones. 

Y en parte esto es cierto para problemas de dimensión baja (digamos, 2,3 o 4 variables). El problema
es que generalmente nuestros problemas son de dimensionalidad más alta.

- Cuando la dimensión es no muy chica, todos los datos de entrenamiento tienden a ser *ralos*,
lo que quiere decir que todos los puntos están lejanos unos de otros. 
- Si la dimensión es alta, entonces no hay manera de evitar uno de dos problemas: o tomamos muchos vecinos,
y entonces las predicciones son extrapolaciones graves, o tomamos muy pocos, y nuestro modelo es poco confiable
y tiene alta variabilidad. Peor aún, aunque tomemos un vecino más cercano seguimos extrapolando gravemente.

Esto es contraintuitivo en un principio.

#### Ejemplo {-}

Este ejemplo es tomado de @ESL. Consideremos que la salida Y es determinística $Y = e^{-8\sum_{j=1}^p x_j^2}$.
Vamos a usar 1-vecino más cercano para hacer predicciones, con 
una muestra de entrenamiento de 1000 casos.
Generamos $x^{i}‘s uniformes en $[ 1,1]$, para $p = 2$, y calculamos la respuesta 
$Y$ para cada caso:

```{r}
fun_exp <- function(x) exp(-8 * sum(x ^ 2))
x <- map(1:1000, ~ runif(2, -1, 1))
dat <- tibble(x = x) %>% 
        mutate(y = map_dbl(x, fun_exp))
ggplot(dat %>% mutate(x_1 = map_dbl(x, 1), x_2 = map_dbl(x, 2)), 
       aes(x = x_1, y = x_2, colour = y)) + geom_point()
```

La mejor predicción en $x_0 = (0,0)$ es $f((0,0)) = 1$. Eñ vecino más
cercano al origen es
```{r vmcbajadim}
dat <- dat %>% mutate(dist_origen = map_dbl(x, ~ sqrt(sum(.x^2)))) %>% 
  arrange(dist_origen)
mas_cercano <- dat[1, ]
mas_cercano; mas_cercano$x[[1]]
```
Nuestra predicción es entonces $\hat{f}(0)=$ `r mas_cercano$y`, que
es bastante cercano al valor verdadero (1).

Ahora intentamos hacer lo mismo para dimensión $p=8$.

```{r vmcalta}
x <- map(1:1000, ~ runif(8, -1, 1))
dat <- tibble(x = x) %>% 
       mutate(y = map_dbl(x, fun_exp))
dat <- dat %>% mutate(dist_origen = map_dbl(x, ~ sqrt(sum(.x^2)))) %>% 
  arrange(dist_origen)
mas_cercano <- dat[1, ]
mas_cercano; mas_cercano$x[[1]]
```

Y el resultado es catastrófico. Nuestra predicción puntual es
```{r}
mas_cercano$y
```

Necesitariamos una muestra de alrededor de  un millón de casos
para obtener resultados no tan malos.

¿Qué es lo que está pasando? La razón es que en dimensiones altas, 
los puntos de la muestra de entrenamiento están muy lejos 
unos de otros, y están cerca de la frontera, 
incluso para tamaños de muestra relativamente grandes como n = 1000.
Cuando la dimensión crece, la situación empeora exponencialmente.

En dimensiones altas, todos los conjuntos de entrenamiento factibles
se distribuyen de manera rala en el espacio de entradas.


## Estructura en modelos {-}

Estas observaciones apuntan a que  métodos locales simples tenerán a funcionar mal
en cuanto tenemos más de unas cuantas variables. ¿Qué salida puede tener este problema?

- Si el problema tiene alguna estructura explotable (que se puede descubrir mediante sugerencias
de teoría y expertos, exploración y conocimiento de los datos, y experimentación), es posible superar el
la maldición de la dimensionalidad.
- En primer lugar, porque tenemos mejores guías para escoger que variables usamos para predecir, y
cuáles probablemente es mala idea incluír
- En segundo lugar, porque podemos construir modelos más parsimoniosos que exploten la estructura del problema.

### Ejemplo {-}
Este segundo ejemplo también es de @ESL. Supongamos que estamos otra vez en un problema de dimensión alta

Ahora intentamos algo similar con una función que es razonable aproximar
con una función lineal. Esta función *solo depende de la primera entrada, y las demás
componentes de $x$ son ruido en términos del problema de predicción:

```{r}
fun_cuad <- function(x)  0.5 * (1 + x[1])^2
```

Y queremos predecir para $x=(0,0,\ldots,0)$, cuyo valor exacto es

```{r}
fun_cuad(0)
```

Los datos se generan de la siguiente forma:

```{r}
simular_datos <- function(p = 40){
    x <- map(1:1000,  ~ runif(p, -1, 1))
    dat <- tibble(x = x) %>% mutate(y = map_dbl(x, fun_cuad)) 
    dat
}
```

Por ejemplo, para dimensión $p=1$ (nótese que una aproximación
lineal no es tan mala):

```{r, fig.width= 4, fig.height = 3}
ejemplo <- simular_datos(p = 1) %>% mutate(x = unlist(x))
ggplot(ejemplo, aes(x = x, y = y)) + geom_point() +
    geom_smooth(method = "lm")
```


Ahora simulamos el proceso en dimensión $p=40$: simulamos las entradas, y aplicamos un vecino más cercano

```{r}
vmc_1 <- function(dat){
    dat <- dat %>% 
        mutate(dist_origen = map_dbl(x, ~ sqrt(sum(.x^2)))) %>% 
        arrange(dist_origen)
        mas_cercano <- dat[1, ]
        mas_cercano$y
}
set.seed(834)
dat <- simular_datos(p = 40)
vmc_1(dat)
```

Este no es un resultado muy bueno (muy lejos de 0.5). Sin embargo, regresión se
desempeña considerablemente mejor:

```{r}
regresion_pred <- function(dat){
    p <- length(dat$x[[1]])
    dat_reg <- cbind(
        y = dat$y, 
        x = matrix(unlist(dat$x), ncol = p, byrow=T)) %>% 
        as.data.frame()
    mod_lineal <- lm(y ~ ., dat = dat_reg)
    origen <- data.frame(matrix(rep(0, p), 1, p))
    names(origen) <- names(dat_reg)[2:(p+1)]
    predict(mod_lineal, newdata = origen)
}
regresion_pred(dat)
```

Donde podemos ver que típicamente la predicción de regresión
es mucho mejor que la de 1 vecino más cercano. **Ejercicio**: prueba con otras semillas


## ¿Cuál es la solución al problema de dimensión alta? {-}

La solución a este problema, entonces, no tiene atajos mágicos: igual que en análisis exploratorio,
este problema se mitiga cuando descubrimos estructura apropiada para el problema y 
construimos esta estructura dentro de los modelos que queremos utilizar. Ejemplos son:

- Modelos lineales cuando la estructura lineal es apropiada (muchos ejemplos de éxito): mediante
*feature engineering* es posible construir una estructura lineal creible. Los modelos lineales
son mucho más poderosos de los que en una mirada rápida puede sugerir.
- Redes neuronales para procesamiento de señales (imágenes, sonido): los éxitos en este campo
se deben a una combinación virtuosa entre conjuntos de datos grandes y estructuras apropiadas,
como son unidades convolucionales que toman en cuenta la estructura espacial del problema
- Reducción de dimensionalidad: distintos métodos que buscan reducir la dimensión sin perder mucha información. Existen muchos ejemplos (por ejemplo, cuando hay muchas variables correlacionadas), por
ejemplo: sistemas de recomendación
- Reducción de dimensionalidad supervisada, como en problemas de procesamiento de texto, donde
recientemente hay muchos éxitos. Los métodos de reducción de dimensionalidad inspirados en 
modelos estadísticos del lenguaje (probabilidades de ocurrencia de palabras dadas el contexto, *embeddings*) son usados de manera general, así como redes neuronales que buscan capturar y utilizar
información secuencialmente cercana.
- Métodos basados en árboles, que buscan explícitamente explotar interacciones, y promedian sobre muestras bootstrap
para controlar varianza.

## Teoría: ¿Por qué pueden ser malas las predicciones? {-}

En esta parte intentamos explicar algo de la teoría que intuitivamente hemos discutido en secciones anteriores.

Recordemos que planteamos nuestro problema de predicción como uno donde queremos aproximar la 
función $f$ con una función $\hat{f}$ construida con los datos de entrenamiento. ¿Por qué puede
ser grande este error?

Consideremos un modelo particular que genera datos para nuestro problema que es útil para entender
de dónde provienen los errores de predicción:

$$Y = f(X) + \epsilon$$
donde por suponemos que $media(\epsilon | X) = 0$ (si no, podríamos sumar ese valor a $f$). 
Esta variable aleatoria representa efectos
de otras variables que no tenemos disponibles, y que afectan el resultado $Y$ que observamos. Si consideramos el error esperado de predicción bajo pérdida cuadrática, podemos escribir:

$$Y-\hat{f}(X) = (f(X)- \hat{f}(X)) + \epsilon$$

Vemos en primer lugar dos razones para errores grandes:

- La $\epsilon$ puede tomar valores grandes (pues no tenemos variables importantes que influyen en $Y$)
- $f(X) - \hat{f}(X) $ puede ser grande, es decir, nuestra estimación de $f$ es mala. A su vez,
hay dos razones por las que puede ocurrir esto:

- $\hat{f}(X)$ tiene poca variabilidad y está consistemente lejos de la verdadera $f$, independientemente de la muestra
que usemos para entrenar al modelo
- La variabilidad de $\hat{f}(X)$ es grande, de forma que es probabable que el valor obtenido en una
muestra particular esté lejos del valor $f(X)$
- Una combinación de los dos incisos anteriores.


Una manera de cuantificar esto es mediante la descomposición varianza-sesgo. Con algunos cálculos simples,
podemos demostrar que:

$$E((Y-\hat{f}(x))^2|X=x) = (E(\hat{f}(x)) - f(x))^2 + Var(\hat{f}(x)) + Var(\epsilon|X=x)$$

donde el valor esperado es sobre el conjunto de entrenamiento ${\mathcal L}$ y $\epsilon$, que suponemos independientes.

- El primer término le llamamos *sesgo* del predictor (si está consistentemente lejos de $f(x)$)
- El segundo término es la *varianza* del predictor (qué tan sensible es a la muestra de entrenamiento)
- Al tercer término le llamamos *error irreducible*.


## Ejemplo {-}

Consideramos un ejemplo simulado (el de las curvas rojas, gris y la recta más arriba), y suponemos que
queremos predecir en:

```{r}
x_0 <- 7
```

Consideremos qué pasa cuando hacemos predicciones en x = `r x_0`, dependiendo de los datos que usemos
para entrenar el modelo. Usaremos
varias muestras de entrenamiento y vemos cómo cambia la predicción:

```{r, fig.width=5, fig.asp=0.7, warning = FALSE, echo = FALSE}
x <- c(1,7,10,0,0,5,9,13,2,4,17,18,1,2)
x_g <- seq(0,15,0.5)
y_g <- f(x_g)
dat_g <- tibble(x = x_g, y = y_g)
f <- function(x){
  ifelse(x < 10, 1000*sqrt(x), 1000*sqrt(10))
}
simular_curvas <- function(rep, x_g){
    error <- rnorm(length(x), 0, 500)
    y <- round(f(x) + error)
    datos_entrena <- data.frame(x = x, y = y)
    curva_1 <- loess(y ~ x, data = datos_entrena, degree = 2, span = 1.2)
    curva_2 <- loess(y ~ x, data = datos_entrena, degree = 2, span = 0.6)
    curva_3 <- lm(y ~ x, data = datos_entrena)
    modelos <- list(curva_1, curva_2, curva_3)
    x_in <- tibble(x = x_g)
    preds <- map(modelos, ~ tibble(pred = predict(.x, newdata = x_in))) %>% 
             map(~ bind_cols(x_in, .x))
    tibble(rep = rep, metodo = c("gris","rojo", "recta"), preds = preds)
}
set.seed(8812)
sims_curvas <- map(1:30, ~ simular_curvas(.x, x_g)) %>% bind_rows %>% unnest
```

En negro está la verdadera $f$: esta es la curva que queremos estimar. Cada línea de color
representa un modelo ajustado con una muestra de entrenamiento distinta:

```{r, fig.width = 10, fig.height=4}
ggplot(sims_curvas, aes(x = x)) +
    geom_vline(xintercept = 7, colour = "gray") +
    geom_line(aes(y = pred, group = interaction(metodo, rep), colour = metodo)) +
     geom_line(data = dat_g, aes(y = y), size = 2) + facet_wrap(~metodo) +
    scale_colour_manual(values = c("gray40", "blue", "red")) 
```

Y vemos que la recta tiene relativamente poca variabilidad, pero consistemente falla en aprender la forma de los
datos. Por el contrario, en algunos puntos las predicciones del modelo rojo son muy malas, pues el suavizador
varía mucho.

Ahora vemos las predicciones comparadas con el óptimo en el punto que nos interesa, que es f(7):

```{r, message = FALSE, warning = FALSE}
sims_curvas <- map(1:200, ~ simular_curvas(.x, x_g)) %>% bind_rows %>% unnest
sims <- filter(sims_curvas, x == 7)
ggplot(sims, aes(x= 7, y = pred)) + 
    geom_line(data = dat_g %>% filter(x > 4 & x < 11), aes(x = x, y = y)) +
    geom_boxplot(aes(colour = metodo)) 
```

Y confirmamos que:

- El error predictivo del modelo rojo es frecuentemente alto porque su varianza
de la predicción es alta (depende muy fuertemente de los datos)
- El error predictivo de la recta es malo porque está consistemente lejos de la curva verdadera, aunque varía menos.
- El modelo gris parece tener ligero sesgo, pero menos que el modelo lineal, y varía menos que
la curva roja.

## Subajuste y sobreajuste {-}

Aunque la descomposición de sesgo, varianza y error irreducible no aplica a todas las funciones
de pérdida (por ejemplo en problemas donde la respuesta es una variable categórica), nos provee una guía
para pensar en modelos predictivos y su desempeño.

En nuestros ejemplos anteriores:

1. Suavizadores con ventana más chica son más flexibles, y aunque su sesgo tiende a ser más bajo, ese
precio se paga en varianza más alta. Suavizadores con ventan más grande son menos flexibles, tienen menos
varianza, pero pueden tener más sesgo.
2. Prediciones de k-vecinos más cercanos, con $k$ chica, tienden a tener más varianza y menos sesgo. Cuando $k$
es grande, la varianza es menor pero el sesgo puede aumentar


**Rigidez, sesgo y subajuste**

Todos estos nombres se refieren a ideas similares: los modelos que usamos no tienen la flexibilidad 
suficiente para aprender patrones que se generalizan y ayudan en la predicción, aún cuando haya mucha 
evidencia y datos que sostengan ese patrón. 

La recta del ejemplo anterior es muy rígida para este problem y subajusta. O



**Flexibilidad, varianza y sobreajuste**

Estos se refieren a modelos que son demasiado flexibles y dependen demasiado fuertemente de los datos, 
capturando características asociadas a variación muestral o variación que no depende de variables conocidas.
Son características que no 
generalizan en la predicción. Tienden a ser inestables (varían mucho con los datos).

El modelo rojo del ejemplo anterior es dedamisado flexible y sobreajusta.

## Tradeoff rigidez - complejidad {-}

Generalmente, cuando trabajamos en análisis predictivo, ajustes al modelo
intercambian una forma de error por otro. Por ejemplo:

- Cuando usamos modelos más flexibles, tendemos a aumentar varianza y reducir sesgo.
- Cuando quitamos variables, aumentamos el sesgo, pero la varianza puede reducirse
- Poner más restricciones en el proceso de ajuste tiende a reducir la varianza a costa del sesgo
- Cuando utilizamos muestras más grandes, tendemos a reducir la varianza. Pero si son datos menos
relevantes para el problema, el sesgo puede aumentar.
- Reducir dimensionalidad puede reducir la varianza, pero, dependiendo de cómo la hagamos, 
puede aumentar el sesgo.

**Buscamos un balance entre los dos para optimizar las predicciones**


```{r, fig.height = 5, fig.width = 12, echo = FALSE}
varianza <- function(x) {
    x^2.5 + 1
}
sesgo <- function(x) {
    1/(x^3)
}
sesgo_var <- tibble(x = seq(0.5, 2, by = 0.01)) %>% 
    mutate(sesgo = sesgo(x), varianza = varianza(x), `error de prediccion` = sesgo + varianza + 1) %>% 
    gather(fuente, error, sesgo:`error de prediccion`) %>% mutate(metodo = "1. Método genérico")

sesgo_var <-tibble(x = seq(0.5, 2, by = 0.01)) %>% 
    mutate(sesgo = 0.5*sesgo(x), varianza = varianza(x^0.7) - 1, `error de prediccion` = sesgo + varianza + 2) %>% 
    gather(fuente, error, sesgo:`error de prediccion`) %>% mutate(metodo = "2. Método estructurado") %>% 
    bind_rows(sesgo_var)
sesgo_var <-tibble(x = seq(0.5, 2, by = 0.01)) %>% 
    mutate(sesgo = 0.5*sesgo(x), varianza = varianza(x^0.3) - 1, `error de prediccion` = sesgo + varianza + 0.5) %>% 
    gather(fuente, error, sesgo:`error de prediccion`) %>% mutate(metodo = "3. Método estructurado \n Mejores variables") %>% 
    bind_rows(sesgo_var)

ggplot(sesgo_var, 
       aes(x = x, y = error, colour = fuente)) + geom_line(size = 1) +
    scale_color_colorblind() + xlab("Complejidad del modelo") + facet_wrap(~metodo) + ylab("Error de predicción")
```


