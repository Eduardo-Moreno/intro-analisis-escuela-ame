# Refinación de modelos predictivos



## ¿Cómo mejorar un modelo predictivo? {-}

En primer lugar, hay consideraciones que producen mejoras en general (independientemente de métodos particulares
y casi de analistas particulares!)

- Pensar con detalle in la información que hay en las covariables, y cuánta información útil contienen
para hacer predicciones.¿ Existen variables importantes que no tenemos disponibles? ¿Es posible construir o conseguir variables 
relacionadas con esas característias importantes?
- Considerar cómo usar estructura em nuestros: Por ejemplo: ¿estamos usando correctamente información 
espacial o temporal? ¿Un modelo lineal es apropiado? 

Otras acciones tienen efectos sobre distintas componentes el error, y buscan corregir distintos aspectos. 
Por ejemplo, si queremos reducir varianza:

- Podemos intentar modelos más simples
- Reducir o eliminar variables
- Penalizar la complejidad de los modelos para obtener ajustes más simples
- Conseguir más datos

Si nuestro problema es sesgo, sin embargo, estas acciones pueden no ser efectivas. En este caso:

- Usar modelos más complejos
- Penalizar menos por complejidad
- Crear variables derivadas (por ejemplo, interacciones)

La pregunta es: ¿Cómo diagnosticamos si nuestro problema es sesgo o varianza (o ambos)? Algunos tips son:

**Signos de que la varianza es alta (sobreajuste)**:

- El error de prueba tiende a ser considerablemente más alto que el error de entrenamiento. El modelo se "pega" mucho
a los datos de entrenamiento, y el error parece bajo. Sin embargo, cuando lo probamos con otra muestra el error se degrada.
- El modelo cambia mucho con pequeñas perturbaciones de los datos de entrenamiento (por ejemplo, si hacemos bootstrap las 
predicciones varían mucho).

**Signos de que el sesgo es alta (subajuste)**:

- El error de prueba tiende a ser similar al de entrenamiento. 
- El modelo es muy estable, por ejemplo bajo replicaciones bootstrap de los datos. Sin embargo, hay partes en los datos
donde el modelo consistemente sobrepredice y subpredice. 
- Residuales (errores individuales) de entrenamiento tienen una distribución con colas que tienden a ser largas.

## Expansión de entradas (feature engineering) {-}

## Regularización para modelos lineales {-}

## Regularización ridge {-}

## Regularización lasso {-}

## Predicción de Grasa corporal {-}

Supongamos que queremos predecir el porcentaje de grasa en el cuerpo a partir de varias
medidas simples del cuerpo (estatura, peso, circunferencia de abdomen, cuello, rodilla, etc.). 
Este ejemplo está en @izenman.

Para
este ejemplo dividimos la muestra 40-60 en entrenamiento y prueba. Además de las entradas
originales, introducimos algunas variables para enriquecer el modelo lineal: por ejemplo el índice
de masa corporal y otras (logaritmo de cocientes de distintas mediciones corporales),
y agregamos algunas variables de ruido para hacer más difícil el problema.


```{r, message = FALSE, echo = FALSE}
set.seed(19)
library(tidyverse)
library(ggthemes)
library(glmnet)
source("R/funciones_auxiliares.R")
```

```{r, message = FALSE}
preprocesar <- function(data){
  # un ejemplo de preprocesamiento
    data_proc <- data %>% 
        mutate(
          imc = 703 * peso / estatura^2,
          muslo_rodilla = log(muslo / rodilla),
          pecho_cuello = log(pecho / cuello),
          abdomen_cadera = log(abdomen / cadera),
          biceps_antebrazo = log(biceps / antebrazo),
          muslo_rodilla = log(muslo / rodilla),
          munieca_antebrazo = log(munieca / antebrazo),
          x1 = rnorm(n(), 100, 20),
          x2 = rnorm(n(), 100, 20),
          x3 = rnorm(n(), 50, 5),
          x4 = rnorm(n(), 50, 5),
          x5 = rexp(n(), 1),  
          x6 = rexp(n(), 1),  
          x7 = rexp(n(), 1),  
          x8 = rexp(n(), 1),  
          x9 = rexp(n(), 1))
    data_proc 
}
grasa <- read_csv("./datos/bodyfat.csv") %>% rename(munieca = muñeca) 
grasa_proc <- grasa %>% preprocesar
# dividir entrenamiento y prueba
gp_ent <- grasa_proc %>% sample_frac(0.3) 
gp_pr <-  grasa_proc %>% anti_join(gp_ent) 
```

Las medidas que estamos usando tienen correlación alta, y normalmente eso significa
que nuestras predicciones pueden sufrir de varianza alta. Demos una mirada rápida a los **datos de entrenamiento**:

```{r}
gp_tbl <- gp_ent %>% 
    mutate(id = 1:n()) %>% 
    gather(variable, valor, -id) %>% 
    group_by(variable) %>%
    summarise(cuantiles = list(cuantil(valor, c(0,0.1, 0.5, 0.9, 1)))) %>% 
    unnest %>% 
    mutate(valor = round(valor, 1)) %>% 
    spread(cuantil, valor) %>% 
    # estas son útiles para variables positivas
    mutate(max_mediana = round(`1`/`0.5`,1) ) %>% 
    mutate(min_mediana = round(`0`/`0.5`,1)) %>% 
    mutate(rango_mediana = round((`0.9`- `0.1`) / `0.5`, 2))
gp_tbl %>% formatear_tabla()
```




## Grasa corporal: mínimos cuadrados {-}

El primer analista decide incluir todas las variables en un modelo lineal:

```{r}
mod_mc <- lm(grasacorp ~ ., data = gp_ent)
```

Calculamos error de entrenamiento y de prueba:

```{r}
error_abs_ent <- mean(abs(predict(mod_mc) - gp_ent$grasacorp))
error_abs_pr  <- mean(abs(predict(mod_mc, newdata = gp_pr) - gp_pr$grasacorp))
error_abs_ent %>% round(1)
error_abs_pr %>% round(1)
```

En términos de la media de grasacorporal:

```{r}
round(error_abs_ent / mean(gp_ent$grasacorp), 2)
round(error_abs_pr / mean(gp_pr$grasacorp), 2)
```

El error de entrenamiento está cerca de ser aceptable para una aplicación como esta (pensar en esto¿), si se reflejara
también como error de prueba. El error de prueba es considerablemente más alto.

Esto indica que nuestra preocupación principal ahora es sobreajuste: el modelo tiene capacidad de ajustar a los
datos que le dimos de manera razonable - pero no generaliza tan bien.

Como estamos trabajando con muestras relativamente chicas, conviene calcular intervalos para el error de prueba. Esto
lo podemos hacer de manera simple con el bootstrap. Un intervalo de 90\% para el error absoluto relativo en este caso es:

```{r, echo = FALSE}
error_relativo <- function(modelo, s = NULL){
  if(is.null(s)){
    preds <- predict(modelo, newdata = gp_pr)
    }
  else {
    preds <- predict(modelo, newx = gp_pr %>% select(-grasacorp) %>% as.matrix, s = s)
  }
  media_y <- mean(gp_pr$grasacorp)
  errores <- abs(preds - gp_pr$grasacorp)
  n <- length(errores)
  error_boot <- map_dbl(1:5000, function(i) { sample(errores / media_y, n, replace = T) %>% mean })
  limites <- quantile(error_boot, c(0.05, 0.95)) 
  errores <- c(limites[1], mean(errores) / media_y, limites[2]) %>% round(2)
  names(errores) <- c("inferior", "error", "superior")
  errores
}
error_1 <- error_relativo(mod_mc)
error_1
```


## Grasa corporal: selección de algunas variables {-}

El analista 2 reconoce que hay información común en las variables que puede producir varianza. Después de 
pensar no mucho, decide usar un método simple: simplemente va a usar la medida de peso y estatura para 
construir su predicción. 

```{r}
mod_simple_2 <- lm(grasacorp ~  peso + estatura , data = gp_ent)
```

Calculamos error de entrenamiento y de prueba:

```{r}
error_abs_ent <- mean(abs(predict(mod_simple_2) - gp_ent$grasacorp))
error_abs_pr  <- mean(abs(predict(mod_simple_2, newdata = gp_pr) - gp_pr$grasacorp))
error_abs_ent %>% round(1)
error_abs_pr %>% round(1)
```

En términos de la mediana de grasacorporal:

```{r}
round(error_abs_ent / mean(gp_ent$grasacorp), 2)
round(error_abs_pr / mean(gp_pr$grasacorp), 2)
```

El error de entrenamiento es alto y la brecha entre entrenamiento y prueba es corta. Eso quiere
decir que en este caso debemos estar más preocupados por el sesgo (en este caso, porque faltan variables)

```{r}
error_2 <- error_relativo(mod_simple_2)
error_2
```


## Grasa corporal: regularización {-}

El analista 3 decide que todas las variables podrían tener sentido para abordar este problema, pero
reconoce que puede tener un problema de varianza. Decide usar regresión regularizada (elastic net)
con tres valores: casi ninguna regularización, regularización moderada, y muy regularizado

```{r, message = FALSE}
set.seed(889)
library(glmnet)
mod_1 <- glmnet(gp_ent %>% select(-grasacorp) %>% as.matrix, gp_ent$grasacorp, alpha = 0) 
```

Los errores de entrenamiento y prueba son (para regularización con lambda = 2, veremos más adelante
cómo escoger este parámetro):

```{r, fig.width = 3, fig.height = 2, echo = FALSE}
preds_reg <- predict(mod_1, newx = gp_ent %>% select(-grasacorp) %>% as.matrix, s = 2)
errores_ent <- abs(preds_reg - gp_ent$grasacorp) 
(mean(errores_ent) / mean(gp_ent$grasacorp)) %>% round(2)
preds_regu_pr <- predict(mod_1, newx = gp_pr %>% select(-grasacorp) %>% as.matrix, s = 2)
errores_pr <- abs(preds_regu_pr - gp_pr$grasacorp) 
(mean(errores_pr) / mean(gp_pr$grasacorp)) %>% round(2)
```

Estos son los mejores resultados que hemos obtenido hasta ahora.
La estimación por intervalos con bootstrap del error de prueba da:

```{r}
error_3 <- error_relativo(mod_1, s = 2)
error_3
```


En resumen, los intervalos de error para los tres analistas son:

```{r, echo = FALSE}
error_tbl <- tibble(analista = c("min cuadrados", "seleccion", "regularización"), 
       inf = c(error_1[1], error_2[1], error_3[1]),
       error = c(error_1[2], error_2[2], error_3[2]),
       sup = c(error_1[3], error_2[3], error_3[3]))
error_tbl %>% knitr::kable(type = "html")
```

Con esta información, bastaría para escoger el modelo con regularización $\lambda = 2$. Sin embargo, hay más razones
para hacerlo. 

## Coeficientes {-}

Examinemos los coeficientes que obtuvimos de cada modelo

```{r, echo = FALSE}
coefs_mc <- tibble(coef_mc = coef(mod_mc), variable = names(coef(mod_mc)))
coefs_sel <- tibble(coef_sel = coef(mod_simple_2), variable = names(coef(mod_simple_2)))
coefs_regu <- tibble(coef_regu = coef(mod_1, s = 2) %>% as.numeric, variable = rownames(coef(mod_1, s = 2)))
coefs_tbl <- left_join(coefs_mc, coefs_regu) %>%  left_join(coefs_sel, fill = 0)
coefs_tbl <- coefs_tbl %>% select(variable, coef_mc, coef_regu, coef_sel) %>% mutate_if(is.numeric, ~round(.x, 2)) 
coefs_tbl %>% knitr::kable(type = "html")
```

**Ejercicio**:  qué diferencias ves en los tamaños de los coeficientes? ¿Qué tan difícil
crees que sea interpretar los coeficientes en cada caso?

## Diagnóstico {-}

Los errores de prueba que obtuvimos en los ejercicios anteriores fueron:

```{r}
error_tbl %>% select(analista, error) %>% knitr::kable(type = "html")
```

¿Por qué ocurren estas diferencias? Como *post-mortem*, Podemos hacer **análisis de residuales** para los errores
sobre el conjunto de prueba (nota: esto también lo puedes hacer durante la construcción de modelos con la muestra
de entrenamiento) :

```{r, fig.width = 10, fig.height = 3, echo = FALSE}
preds_mod <- gp_pr %>% select(grasacorp, imc) %>% 
  mutate(mc = predict(mod_mc, newdata = gp_pr),
         seleccion = predict(mod_simple_2, newdata = gp_pr),
         regularizacion = preds_regu_pr) %>% 
  gather(modelo, prediccion, - grasacorp, -imc)
ggplot(preds_mod, aes(x = prediccion, y = grasacorp)) + 
  geom_abline(colour = "red") +
  geom_point() +
  facet_wrap(~ modelo) 
```

Y vemos algunos problemás más graves y menos graves
con las predicciones de mínimos cuadrados: existen varias que son negativas, y en un caso,
la predicción está totalmente fuera de rango. Examinando los datos, corresponde a la observación:

```{r, echo = FALSE}
gp_pr[predict(mod_mc, newdata = gp_pr) < -10,] %>% select(edad, peso, estatura, imc)
```

El imc nos indica que la estatura (75 cm, la variable está en pulgadas y el peso en libras) 
es una medición errónea, o aunque fuera correcta, 
no quisiéramos hacer predicciones en este caso. **Nótese que este dato no apareció durante
en el entramiento, sino en la etapa de prueba**. Sin embargo, los otros dos modelos fueron resistentes
a este dato extremo: esto probablemente indica sobreajuste severo en el modelo sin regularizar. 
Quitando este caso:

```{r, fig.width = 10, fig.height = 3, echo = FALSE}
ggplot(preds_mod %>% filter(imc < 50), aes(x = prediccion, y = grasacorp)) + 
  geom_point() +
  geom_abline(colour = "red") +
  facet_wrap(~ modelo)
```

Y vemos que las predicciones con esta muestra de 
prueba del modelo regularizado todavía son ligeramente mejores que las de mínimos cuadrados. En resumen:

- El modelo regularizado es más parsimonioso que el de mínimos cuadrados.
- El modelo de mínimos cuadrados sufre de sobreajuste.
- El modelo con una selección pobre de variables tiene un desempeño malo.
- El desempeño del modelo no regularizado fue malo, aunque identificamos que se debió a un dato erróneo. El modelo
regularizado se desempeña ligeramente mejor cuando el no regularizado cuando quitamos este dato.

En otros casos, la mejoría al usar regularización puede ser más dramática (ver @ESL para varios ejemplos).




## Intervalos predictivos {-}

En algunos problemas de predicción, especialmente cuando el error promedio tiende a ser alto, es importante
producir resúmenes acerca de la incertidumbre que tenemos al hacer las predicciones. En algunos casos esto es 
indispensable. 

Una manera de expresar la incertidumbre en la predicción es mediante la construcción de intervalos predictivos:
Por ejemplo:

Nuestra predicción puntual del precio de una casa es de 250 mil dólares. Un intervalo predictivo de 90\% de probabilidad
podría ir de 210 mil dólares a 270 mil: esto quiere decir, la menos *nominalmente*, que hay un 90\% de probabilidad
de que el intervalo cubra a la observación verdadera. Esto puede ser útil para tomar decisiones o incluir nuestras
predicciones en procesos posteriores. 

Hay varias maneras de construir estos intervalos. Lo más importante es:

- Siempre checamoa que la cobertura *nominal* de los intervalos construidos sea cercana a la *real*. Usamos
la muestra de prueba para este propósito.


En el caso de modelos de regresión, una buena alternativa es utilizar **regresión bayesiana** 
(ver por ejemplo @GelmanHill para un curso interesante, y @Bishop para un enfoque más de *machine learning*). Los
métodos de regularización como ridge o lasso caben naturalmente en este contexto (en una forma más poderosa), y 
producir intervalos predictivos es muy natural en este contexto.

En otros casos (por ejemplo, redes neuronales o métodos basados en árboles), existe avance pero no hay
un enfoque establecido. En el siguiente ejemplo, mostramos un ejemplo de construcción de intervalos
de predicción para un modelo de bosques aleatorios (ver por ejemlo @rfintervals, @rfintervalpkg).


## Ejemplo: precios de casas {-}

Consideramos un modelo para predecir precios de casas basadas in un bosque aleatorio:

```{r, message = FALSE, echo = FALSE}
source("R/casas_preprocesamiento.R")
set.seed(21)
casas_completo <- casas
```

Separamos muestra de entrenamiento y prueba:

```{r}
casas_t <- casas_completo %>% sample_frac(0.9)
casas_p <- casas_completo %>% anti_join(casas_t)
```

Ajustamos un bosque aleatorio y construimos intervalos predictivos usando el paquete 
[rfinterval](https://github.com/haozhestat/rfinterval), @rfintervalpkg :

```{r}
library(rfinterval)
intervals_sc <- rfinterval(precio_miles ~ area_habitable_sup_m2 + calidad_gral +
                               nombre_zona + condicion_gral + aire_acondicionado +
                               area_lote_m2 + baños_completos + calidad_calefaccion, 
                   train_data = casas_t, test_data = casas_p,
                   params_ranger = list(mtry = 5),
                   method = "quantreg")
sc_i <- intervals_sc$quantreg_interval
preds <- intervals_sc$testPred
sc_i$precio_miles <- casas_p$precio_miles
sc_i$area <- casas_p$area_habitable_sup_m2
sc_i$pred <- preds
mean(abs(sc_i$pred - sc_i$precio_miles)) / mean(sc_i$precio_miles)
ggplot(sc_i, aes(x = pred, ymin = lower, ymax = upper, y = precio_miles)) +
    geom_point(colour = "red") +
    geom_linerange(colour = "gray") 
sc_i %>% summarise(cobertura = mean(precio_miles < upper & precio_miles > lower))
```

En este caso, los intervalos predictivos cumplen razonablemente bien su cobertura *nominal*, y desde
este punto de vista el modelo es apropiado.





