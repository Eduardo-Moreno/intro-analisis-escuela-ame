

## ¿Cómo mejorar un modelo predictivo?

Distintas acciones tienen efectos sobre distintas componentes el error. Por ejemplo, si queremos reducir varianza:

- Podemos intentar modelos más simples
- Reducir o eliminar variables
- Penalizar la complejidad de los modelos para obtener ajustes más simples
- Conseguir más datos
- Conseguir mejores variables o mediciones

Si nuestro problema es sesgo, sin embargo, estas acciones pueden no ser efectivas. En este caso:

- Usar modelos más complejos
- Penalizar menos por complejidad
- Crear variables derivadas (por ejemplo, interacciones)
- Conseguir mejores variables o mediciones

Explicaremos algo acerca de los métodos de penalización.

## Regresión lineal

Un tipo de estructura muchas veces exitosa es la lineal. Si tenemos entradas $X_1,\ldots, X_p$, buscamos
predictores que son promedios ponderados de las variables de entrada:

$$f(X_1,\ldots, X_p) = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p$$

Ajustar modelo significa en primer lugar encontrar valores apropiados para los coeficientes
$\beta_0,\beta_1, \ldots, \beta_p$. Es posible hacer esto con mínimos cuadrados. Si tenemos
la muestra de entrenamiento $\mathcal L$, intentamos encontrar coeficientes que minimicen
el error de entrenamiento:

$$\min_{\beta_0,\beta_1,\ldots, \beta_p} \sum_{i=1}^n (y_i - f(x_i))^2$$

donde $x_i = (x_i^(1), \ldots, x_i^(p)$ son las variables de entrada para cada caso de entrenamiento.

Hay dos observaciones importantes qué hacer:


- En un principio, los modelos lineales pueden parecer muy simplistas, y que su principal
problema puede ser el **sesgo**. Sin embargo, si tomamos en cuenta
de que podemos crear variables derivadas de las que tenemos y ponerlas como entradas, es posible modelar
patrones considerablemente complejos. Esto lo hacemos normalmente para reducir el sesgo del modelo

- Este principio de minimización parece ser un poco contrario a lo que discutimos arriba: intentamos minimizar el error de entrenamiento,
cuando realmente quisiéramos tener error de predicción bajo. Este intento de minimizar el error de entrenamiento,
lo cual puede producir **varianza** alta, especialmente cuando hacemos más **feature engineering**. 



## Ejemplo: feature engineering {-}

Queremos predecir las ventas futuras anuales $Y$ de una tienda que se va a construir
en un lugar dado. Las variables que describen el lugar son
$X_1 = trafico\_peatones$, $X_2=trafico\_coches$. En una aproximación simple,
podemos suponer que la tienda va a capturar una fracción de esos tráficos que
se van a convertir en ventas. Quisieramos predecir con una función de la forma
$$f_\beta (peatones, coches) = \beta_0 + \beta_1\, peatones + \beta_2\, coches.$$
Por ejemplo, después de un análisis estimamos que 

- $\hat{\beta}_0 = 1000000$ (ventas base, si observamos tráficos igual a 0: es lo que va a atraer la tienda)
- $\hat{\beta}_1 = (200)*0.02 = 4$ (capturamos 2\% del tráfico peatonal, y cada capturado gasta 200 pesos)
- $\hat{\beta}_2 = (300)*0.01 =3$ (capturamos 1\% del tráfico de autos, y cada capturado gasta 300 pesos)
Entonces haríamos predicciones con
$$\hat{f}(peatones, coches) = 1000000 +  4\,peatones + 3\, coches.$$
El modelo lineal es más flexible de lo que parece en una primera aproximación, porque
tenemos libertad para construir las variables de entrada a partir de nuestros datos.
Por ejemplo, si tenemos una tercera variable 
$estacionamiento$ que vale 1 si hay un estacionamiento cerca o 0 si no lo hay, podríamos
definir las variables
- $X_1= peatones$
- $X_2 = coches$
- $X_3 = estacionamiento$
- $X_4 = coches*estacionamiento$
Donde la idea de agregar $X_4$ es que si hay estacionamiento entonces vamos
a capturar una fracción adicional del trafico de coches, y la idea de $X_3$ es que 
la tienda atraerá más nuevas visitas si hay un estacionamiento cerca. Buscamos 
ahora modelos de la forma
$$f_\beta(X_1,X_2,X_3,X_4) = \beta_0 + \beta_1X_1 + \beta_2 X_2 + \beta_3 X_3 +\beta_4 X_4$$
y podríamos obtener después de nuestra análisis las estimaciones
- $\hat{\beta}_0 = 800000$ (ventas base)
- $\hat{\beta}_1 = 4$
- $\hat{\beta}_2 = (300)*0.005 = 1.5$
- $\hat{\beta}_3 = 400000$ (ingreso adicional si hay estacionamiento por nuevo tráfico)
- $\hat{\beta}_4 = (300)*0.02 = 6$ (ingreso adicional por tráfico de coches si hay estacionamiento)
 
 y entonces haríamos predicciones con el modelo
$$\hat{f} (X_1,X_2,X_3,X_4) = 
800000 + 4\, X_1 + 1.5 \,X_2 + 400000\, X_3 +6\, X_4$$


## Regularización

En un principio podemos agregar más variables a un modelo, con la esperanza de reducir el sesgo potencial.
Esto, sin embargo, puede incrementar la varianza y degradar el desempeño predictivo, especialmente
cuando algunas variables son ruidosas y contribuyen poco.

Como la **varianza** se produce porque las predicciones dependen fuertemente del conjunto de entrenamiento,
y las predicciones a su vez dependen de los coeficientes del modelo ajustado, podemos restringir el
proceso de ajuste para evitar la variación de los coeficientes. Dos técnicas útiles son el lasso y 
regresión ridge.

En regresión ridge, buscamos minimizar

$$D(\beta) = \sum_{i=1}^n (y_i - f(x_i))^2$$

Con una restricción $\sum_i \beta_i^2 < s$. En regresión lasso utilizamos 
$\sum_i |\beta_i| < s$


## Ejemplo: grasa corporal {-}

Supongamos que queremos predecir el porcentaje de grasa en el cuerpo a partir de varias
medidas simples del cuerpo (estatura, peso, circunferencia de abdomen, cuello, rodilla, etc.). Para
este ejemplo dividimos la muestra 50-50 en entrenamiento y prueba. Además de las entradas
originales, introducimos variables para enriquecer el modelo lineal: por ejemplo, el IMc
que es peso entre estatura, así como otros cocientes útiles como medida de abdomen / pecho
o munieca / antebrazo:

```{r, message = FALSE}
set.seed(889)
preprocesar <- function(data){
    data_proc <- data %>% 
        mutate(imc = peso/estatura, 
           abdomen_pecho = abdomen / pecho, 
           abdomen_cadera = abdomen / cadera, 
           cadera_muslo = cadera / muslo, 
           biceps_antebrazo = biceps / antebrazo, 
           munieca_antebrazo = munieca / antebrazo,
           muslo_rodilla = muslo / rodilla)
    data_proc
}
grasa_corp <- read_csv("./datos/bodyfat.csv") %>% 
    rename(munieca = muñeca) %>% 
    preprocesar
gp_ent <- grasa_corp %>% sample_frac(0.5)
gp_pr <- grasa_corp %>% anti_join(gp_ent)
mod_1 <- cv.glmnet(gp_ent %>% select(-grasacorp) %>% as.matrix, gp_ent$grasacorp, alpha = 0, 
                   lambda = exp(seq(-10, 5, 0.25)), type ="mse")
plot(mod_1)
```

Podemos calcular el error de predicción sobre la muestra de prueba. Como la muestra de prueba es
relativamente chica, podríamos usar bootstrap para calcular el error de estimación. O en este caso,
usar la fórmula $s/\sqrt{n}$

```{r, fig.width = 3, fig.height = 2}
s <- exp(c(-10, 1, 5))
errores <- map(s, function(s){
    preds_1 <- predict(mod_1, newx = gp_pr %>% select(-grasacorp) %>% as.matrix, s = s)
    error <- abs(preds_1 - gp_pr$grasacorp) %>% mean
    ee <- (abs(preds_1 - gp_pr$grasacorp) %>% sd) / sqrt(length(preds_1))
    tibble(s = s, error = error, ee = ee)
}) %>% bind_rows
ggplot(errores, aes(x = s, y = error, ymin = error-ee, ymax=error+ee)) + geom_point() +
    geom_linerange() + scale_x_log10() + ylab("Error absoluto")
```

O dividir entre la media para calcular Error relativo absoluto:

```{r, fig.width = 3, fig.height = 2}
media_prueba <- mean(gp_pr$grasacorp)
ggplot(errores, aes(x = s, y = error/media_prueba, ymin = (error-ee) / media_prueba, 
                    ymax= (error+ee)/media_prueba)) + geom_point() +
    geom_linerange() + scale_x_log10() + ylab("Error relativo absoluto")
```

Con esta información, bastaría para escoger el modelo con regularización $\lambda = 1$. Sin embargo, hay más razones
para hacerlo. Los coeficientes de los modelos:
```{r}
coef(mod_1, s = s) %>% round(2)
```


De modo que el modelo menos regularizado es considerablemente más complicado, en sentido
de que tiene considerablemente mayor variación potencial cuando cambiamos las entradas.

Por ejemplo, ¿qué pasa con las predicciones cuando incrementamos la medida de abdomen en 10\%?

```{r}
pred_fun <- function(object, newdata, s = 1) {
  newdata_2 <- newdata %>% preprocesar %>% as.matrix
  predict(object, newx = newdata_2, s = s)[, 1L, drop = TRUE]
}

calc_preds <- function(s, variable, delta, mod_1){
    preds_1 <- pred_fun(mod_1, gp_ent %>% select(-grasacorp), s)
    preds_2 <- pred_fun(mod_1, gp_ent %>% select(-grasacorp) %>% mutate({{ variable }} := {{ variable }} * (1 + delta)), s)
    tibble( s = s, preds_incr = preds_2, preds = preds_1) %>% mutate(id = row_number()) %>% 
        left_join(gp_ent %>% mutate(id = row_number()))
}
```


Vemos variaciones poco creíbles de la grasa corporal para un cambio relativamente chico. El 
modelo con la regularización correcta es más simple:

```{r}
preds_ab <- map(s[-3], ~ calc_preds(.x, estatura, 0.05, mod_1)) %>% bind_rows
ggplot(preds_ab, aes(x = grasacorp, y = preds_incr - preds, colour = estatura )) + facet_wrap(~s) +  geom_point() +
    xlab("Predicciones") + ylab("Cambio en prediccion") 
```



```{r}
preds_ab <- map(s[-3], ~ calc_preds(.x, abdomen, 0.05, mod_1)) %>% bind_rows
ggplot(preds_ab, aes(x = grasacorp, y = preds_incr - preds, colour = abdomen )) + facet_wrap(~s) +  geom_point() +
    xlab("Predicciones") + ylab("Cambio en prediccion") 
```

## Intervalos predictivos {-}

En algunos problemas de predicción, especialmente cuando el error promedio tiende a ser alto, es importante
producir resúmenes acerca de la incertidumbre que tenemos al hacer las predicciones. En algunos casos esto es 
indispensable. 

Una manera de expresar la incertidumbre en la predicción es mediante la construcción de intervalos predictivos:
Por ejemplo:

Nuestra predicción puntual del precio de una casa es de 250 mil dólares. Un intervalo predictivo de 90\% de probabilidad
podría ir de 210 mil dólares a 270 mil: esto quiere decir, la menos *nominalmente*, que hay un 90\% de probabilidad
de que el intervalo cubra a la observación verdadera. Esto puede ser útil para tomar decisiones o incluir nuestras
predicciones en procesos posteriores. 

Hay varias maneras de construir estos intervalos. Lo más importante es:

- Siempre checamoa que la cobertura *nominal* de los intervalos construidos sea cercana a la *real*. Usamos
la muestra de prueba para este propósito.


## Ejemplo: precios de casas

Consideramos un modelo para predecir precios de casas basadas in un bosque aleatorio:

```{r}
source("R/casas_preprocesamiento.R")
set.seed(21)
casas_completo <- casas
casas_t <- casas_completo %>% sample_frac(0.9)
casas_p <- casas_completo %>% anti_join(casas_t)

library(rfinterval)
intervals_sc <- rfinterval(precio_miles ~ area_habitable_sup_m2 + calidad_gral +
                               nombre_zona + condicion_gral + aire_acondicionado +
                               area_lote_m2 + baños_completos + calidad_calefaccion, 
                   train_data = casas_t, test_data = casas_p,
                   params_ranger = list(mtry = 5),
                   method = "quantreg")
sc_i <- intervals_sc$quantreg_interval
#sc_i <- intervals_sc$oob_interval
preds <- intervals_sc$testPred
sc_i$precio_miles <- casas_p$precio_miles
sc_i$area <- casas_p$area_habitable_sup_m2
sc_i$pred <- preds
mean(abs(sc_i$pred - sc_i$precio_miles)) / mean(sc_i$precio_miles)
ggplot(sc_i, aes(x = pred, ymin = lower, ymax = upper, y = precio_miles)) +
    geom_point(colour = "red") +
    geom_linerange(colour = "gray") 
sc_i %>% summarise(cobertura = mean(precio_miles < upper & precio_miles > lower))
```

En este caso, los intervalos predictivos cumplen razonablemente bien su cobertura *nominal*, y desde
este punto de vista el modelo es apropiado.





