# Inferencia y Remuestreo


```{r, message = FALSE, echo = FALSE}
library(tidyverse)
library(lubridate)
library(ggthemes)
theme_set(theme_minimal(base_size = 14))
```

Una buena parte de los problemas de análisis de datos tratan de separar señales útiles
para el problema que nos interesa de ruido producido por variación sobre la que no tenemos
información directa. Por ejemplo:

- Si usamos una muestra de una población para generalizar a la población, queremos separar de
aspectos que generalizan a la población de otros que sólo dependen de variación muestral
- Cuando queremos juzgar si un conjunto de datos observados es consistente con un modelo o
con otro conjunto de datos observados, considerando que hay variables que no controlamos que
afectan a ambos conjuntos de datos.

En todos los casos, el paso importante es intentar separar señales útiles (para explicar, predecir,
o mostrar estructura) de *fluctuaciones* que resultan de variables de las que no tenemos información.


## Pruebas de hipótesis {-}

Las primeras técnicas que veremos intentan contestar la siguiente pregunta: si observamos
cierto patrón en los datos, ¿cómo podemos cuantificar o apreciar la evidencia de que es un
patrón notable y no sólo debido a fluctuaciones en los datos particulares que tenemos?

## Comparación con poblaciones de referencia {-}

Supongamos que estamos considerando cambios rápidos en una serie de tiempo de alta frecuencia. Hemos observado la serie en su estado "normal" durante un tiempo considerable, y cuando observamos nuevos
datos quisiéramos juzgar si hay indicaciones o evidencia en contra de que el sistema sigue funcionando
de manera similar. Digamos que monitoreamos ventanas de tiempo de tamaño 20 y necesitamos tomar una decisión. Abajo
mostramos cinco ejemplos donde el sistema opera normalmente. Para comparar efectivamente:

- Eextraemos al azar 5 ventanas de los datos históricos
- Agregamos un panel con los datos nuevos

```{r, echo = FALSE}
simular_serie <- function(n = 1000, lambda = 1, ac = 0.7, x_inicial = 1){
    x <- numeric(n)
    x[1] <- x_inicial
    for(i in 2:n){
        x[i] <-  ac*(x[i - 1]) + rgamma(1, 1, 1 / lambda)
        if(i >= 10){
            x_s <- x[i - seq(9, 0, -1)]
            t_s <- seq(0, 9, 1)
        }
    }
    tibble(t = 1:n, obs = x) %>% filter(t > 50)
}
set.seed(8812)
historicos <- simular_serie(2000)
```


```{r}
tbl <- historicos
muestrear_ventanas <- function(control, obs, n_ventana = 20, long = 20){
    n_control <- nrow(control)
    inicios <- sample(1:(nrow(control) - long), n_ventana - 1)
    control_lista <- map(inicios, function(i){
        filter(control, t %in% seq(i, i + long - 1))
    })
    dat_lista <- append(control_lista, list(obs))
    orden <- sample(1:n_ventana, n_ventana)
    list(lineup = tibble(rep = orden, datos = dat_lista) %>% 
        unnest %>% group_by(rep) %>% mutate(t_0 = t - min(t)) %>% ungroup,
        pos = last(orden))
}
set.seed(1213)
obs <- tbl[500:519, ]
ventanas_tbl <- muestrear_ventanas(tbl, obs, n_ventana = 6)
ggplot(ventanas_tbl$lineup, aes(x = t_0, y = obs, colour = factor(rep==ventanas_tbl$pos))) + geom_line() + 
    geom_point() + facet_wrap(~rep, nrow = 2) +
    theme(legend.position = "none") + scale_y_log10()
```


Cuando nos señalan cuáles son los datos nuevos, 
es fácil encontrar aspectos particulares que distinguen
las nuevas observaciones de las anteriore. Pero hay variabilidad considerable en el
proceso usual, de forma que nos preguntamos si no estamos **sobreinterpretando**
variaciones que son parte normal del proceso.

Repetimos el ejercicio con nuevos datos, y otros 19 páneles escogidos con datos históricos. En un panel
están los datos recientemente observados.  ¿Hay algo en los datos que distinga al patrón nuevo?

```{r, echo = FALSE}
obs <- simular_serie(500, x_inicial = last(obs$obs))
prueba_tbl <- muestrear_ventanas(tbl, obs[1:20, ], n_ventana = 20)
ggplot(prueba_tbl$lineup, aes(x = t_0, y = obs)) + geom_line() + 
     facet_wrap(~rep, nrow = 4) + scale_y_log10()
```

**Ejercicio**: ¿cuáles son los datos nuevos (solo hay un panel con los nuevos datos)? ¿Las elecciones tomadas son consistentes con el azar, o se desvían hacia un panel donde no están los datos nuevos?


Ahora contrastemos con  un ejemplo donde ocurre un cambio considerable. En el siguiento caso,

```{r, message=FALSE, echo = FALSE}
obs_dif <- simular_serie(500, lambda = 3, ac = 0.3, x_inicial = last(obs$obs))
prueba_tbl <- muestrear_ventanas(tbl, obs_dif[1:20, ], n_ventana = 20)
ggplot(prueba_tbl$lineup, aes(x = t_0, y = obs)) + geom_line() + 
   facet_wrap(~rep, nrow = 4) + scale_y_log10()
```

Aunque es imposible estar seguros de que ha ocurrido un cambio considerable, la diferencia de una de las
series es muy considerable. Si identificamos los datos correctos,
la probabilidad de que hayamos señalado la nueva serie sobreinterpretando fluctuaciones 
en un proceso que sigue comportándose normalente es 0.05 - relativamente baja.

**Observaciones y terminología**:

1. Llamamos *hipótesis nula* a la hipótesis de que los nuevos datos son producidos bajo
las mismas condiciones que los datos control.

2. En una *prueba de hipótesis* construimos un resumen o estadística de los datos (en este caso es una
gráfica), y nos fijamos si los datos nuevos tienen un comportamiento extremo con respecto al comportamiento
usual del sistema. La prueba de arriba se llama *prueba del lineup* , o *ronda de los sospechosos*.

3. Si no escogemos la gráfica de los nuevos datos, nuestra conclusión es que la prueba no aporta evidencia
en contra de la hipótesis nula. 

4. Si escogemos la gráfica correcta, nuestra conclusión es que la prueba aporta evidencia
en contra de la hipótesis nula.

¿Qué tan fuerte es la evidencia, en caso de que descubrimos los datos no nulos? 

5. Cuando el número de paneles es más grande y detectamos los datos, la evidencia es más alta en contra de la nula. La razón es que si todos los datos son nulos, escogemos la gráfica "diferente" usando características de variación que ocurren naturalmente el sistema: es decir, escogimos al azar el panel distinto. Decimos que el *nivel de significancia de la prueba* es la probabilidad de atinarle a los
datos correctos cuando la hipótesis nula es razonable. En el caso de 20 paneles, es de 1/20 = 0.05

5. Si acertamos, y la diferencia es más notoria y fue más fácilmente detectar la gráfica diferente,
esto también sugiere más evidencia en contra de la hipótesis nula.

6. Adicionalmente, esta prueba nos puede indicar con mucho detalle qué es lo que puede estar cambiando.

7. Finalmente, esta prueba rara vez (o nunca) **nos da seguridad completa acerca de ninguna conclusión**, aún cuando
hiciéramos muchos páneles.

## Comparando distribuciones {-}


### Ejemplo {-}

En el siguiente ejemplo, tenemos mediciones para tres grupos 
a, b y c. Podemos pensar que son muestras de
tres poblaciones distintas, por ejemplo.

Nos interesa saber qué tan distintos son los datos que produce cada condición - no solamente
qué tan diferentes son las muestras. Hay muchos aspectos
que podríamos cuestionar acerca de cómo son diferentes los tres sistemas. En este caso,
nos preguntamos, por ejemplo, si las distribuciones son similares o no.



```{r, fig.width = 6, fig.height = 3, echo = FALSE}
library(tidyverse)
library(ggrepel)
library(nullabor)
library(knitr)
theme_set(theme_minimal(base_size = 14))
paleta <- scale_colour_manual(values = c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7"))
source("R/funciones_auxiliares.R")
```

```{r, echo = FALSE}
set.seed(8)
pob_tab <- tibble(id = 1:2000, x = rgamma(2000, 4, 1), 
    grupo = sample(c("a","b", "c"), 2000, prob = c(4,2,1), replace = T))
muestra_tab <- pob_tab %>% sample_n(125)
g_1 <- ggplot(muestra_tab, aes(x = grupo, y = x)) + geom_boxplot(outlier.alpha = 0) +
    geom_jitter(alpha = 0.3) + 
    coord_flip() +  labs(subtitle = "Muestra")
g_1
```

En la muestra observamos diferencias entre los grupos. Pero notamos adicionalmente que
hay mucha variación dentro de cada grupo. Nos podríamos preguntar entonces si las diferencias
que observamos se debe a que tenemos información incompleta de los grupos. 

En la muestra, la mediana del grupo c es menor que la de b, por ejemplo. 
Las dispersiones parecen ser también ditintas. Las muestras son relativamente chicas, especialmente
para el grupo b y c.

Podemos construir ahora una *hipótesis nula*, que establece que las observaciones
provienen de una población similar:

- Los tres procesos son prácticamente indistiguibles desde el punto de vista del sistema. La variación
que observamos se debe a que tenemos información incompleta: si observáramos todos los posibles
datos que cada proceso genera, veríamos que sus distribuciones son muy similares.


## Permutaciones y el lineup {-}

Para abordar este problema podemos pensar de la siguiente forma: 

- Si los grupos producen
datos bajo procesos idénticos, entonces los grupos a, b, c solo son etiquetas que no contienen información.
Podríamos entonces **permutar** las etiquetas y observar qué pasa. Una muestra con las etiquetas
permutadas es igualmente verosímil que la que obtuvimos, bajo la hipótesis nula.

Si repetimos el proceso de permutación muchas veces, podemos comparar las diferencias que observamos
en la muestras. Si las diferencias en las muestras verdaderas
no tienen ninguna característica sistemática
que las distinga de las otras muestras obtenidas por permutaciones, entonces tenemos poca
evidencia de que las diferencias que observamos en nuestra muestra sean sistemáticas.

Vamos a intentar esto, por ejemplo usando una gráfica de cuantiles. Hacemos un *lineup*, o una
*rueda de sospechosos*, donde 19 de los acusados son generados mediante permutaciones al azar,
y el culpable (los verdaderos datos) están en una posición escogida al azar. ¿Podemos identificar
los datos verdaderos? Para evitar sesgarnos, también ocultamos la etiqueta verdadera

```{r}
set.seed(88)
reps <- lineup(null_permute("grupo"), muestra_tab, n = 20)
grafica_cuantiles(reps %>%  mutate(grupo_1 = factor(digest::digest2int(grupo) %% 177)), 
                             grupo_1, x) + 
    facet_wrap(~.sample, ncol = 5) + ylab("x")
```

Y la pregunta que hacemos es **podemos distinguir nuestra muestra entre todas las 
replicaciones producidas con permutaciones**? 

En este ejemplo, es difícil indicar cuáles son los datos. Los grupos tienen distribuciones
similares y es factible que las diferencias que observamos se deban a variación muestral.  

Consideremos algunos cálculos: si los verdaderos datos son consistentes con nuestra hipótesis
nula, entonces la probabilidad de escoger al verdadero culpable es de 1/20 = 0.05. Esta se
llama *significancia de nuestra prueba de lineup*.

- Si la persona escoge los verdaderos datos, rechazamos la hipótesis nula (equivalencia entre los
tres bonches de datos), y decimos que los datos son *significativamente diferentes* al nivel 0.05. Esto es 
evidencia en contra de que los datos se producen de manera homogénea, independientemente del grupo.

- Si la persona escoge uno de los datos permutados, no rechazamos la hipótesis nula. Esto usualmente
implica que es razonable proceder en nuestro análisis de estos datos bajo la hipótesis de trabajo
de que no hay diferencia entre los grupos.

Generalmente no rechazar la hipótesis nula implica que podemos proceder tentativamente con ciertos
tratamientos de los datos que no tendrían sentido si la nula estuviéra muy equivocada.
Por ejemplo, como los datos son consistentes con la hipótesis de que todos los grupos provienen de la
mismo proceso, podemos resumir haciendo *pooling* o agregación de todos los grupos. Sobre todos los datos,
los percentiles son

```{r}
muestra_tab %>% pull(x) %>% quantile(probs = c(0.1, 0.25, 0.5, 0.75, 0.9))
```

Comparando con los datos poblacionales:

```{r}
pob_tab %>% pull(x) %>%  quantile(probs = c(0.1, 0.25, 0.5, 0.75, 0.9))
```

Como el pooling es razonable, obtenemos mejores estimaciones utilizando la muestra completa,
agregada sobre los tres procesos a, b, c.


## Cuando la estadística es numérica {-}

Ahora suupongamos que hacemos una pregunta mucho más específica, que se refiere a un número particular:
¿la media del grupo b puede ser considerablemente es diferente de la del grupo c? En lugar de usar 
gráfica, podemos calcular la estadística de interés para cada grupo, y tomar la diferencia. Comparamos
el valor observado con los resultados de las muestras obtenidas por permutación:

```{r, fig.width = 5, fig.height = 2.5, message = FALSE, warning = FALSE}
# esta es la función que calcula la estadística:
dif_medianas <- function(datos){
    datos %>% 
        summarise(mediana = median(x)) %>% 
        spread(grupo, mediana) %>% 
        mutate(diferencia = b - c)
}
# Hacemos permutaciones
set.seed(118)
reps <- lineup(null_permute("grupo"), muestra_tab, n = 10000)
# Mismo cálculo para muestra y permutaciones
dif_obs <- muestra_tab %>% group_by(grupo) %>% dif_medianas %>% pull(diferencia)
reps_mediana <- reps %>% group_by(grupo, .sample) %>% dif_medianas
```


Graficamos de dos maneras: con gráfica de cuantiles e histograma (la última es más común).
La distribución de referencia se construye con el valor de la estadística
(diferencia de medianas) que obtenemos al hacer 10000 permutaciones de los datos. Marcamos
la posición de la diferencia en nuestra muestra y su cuantil:

```{r}
dist_acumulada_perm <- ecdf(reps_mediana$diferencia)
percentil_obs <- dist_acumulada_perm(dif_obs %>% round(2))
g_1 <- ggplot(reps_mediana, aes(sample = diferencia)) + geom_qq(distribution = stats::qunif) +
    geom_hline(yintercept = dif_obs, colour = "red") +
    annotate("text", x = 0.25, y = dif_obs + 0.1, label = "diferencia observada", colour = "red")
```

```{r, echo = FALSE}
g_2 <- ggplot(reps_mediana, aes(x = diferencia)) + geom_histogram(binwidth = 0.1 ) +
    geom_vline(xintercept = dif_obs, colour = "red") +
    annotate("text", x = dif_obs, y = 300, label = percentil_obs, hjust = -0.2, colour = "red")
gridExtra::grid.arrange(g_1, g_2, ncol = 2) 
```


Y notamos que el resultado obtenido en nuestra muestra no es excepcionalmente grande. Otra vez,
no hay evidencia en contra de la hipótesis nula: el resultado obtenido no es muy extremo con 
respecto a la variabilidad que ocurre bajo la hipótesis nula.

Nótese que calculamos una cantidad adicional, que es el percentil donde nuestra observación cae
en la distribución generada por las permutación. Esta cantidad puede usarse para calcular un 
**valor p**. Podemos calcular, por ejemplo:

- Valor p de dos colas: Si la hipótesis nula es cierta, ¿cuál es la
probabilidad de observar una diferencia en valor absoluto tan o más extremo de lo que observamos? Podemos calcular
la respuesta como `r (1 - dist_acumulada_perm(dif_obs)) + dist_acumulada_perm(-dif_obs) ` 


Repitimos el ejemplo con un cambio:

```{r, echo = FALSE}
set.seed(72)
muestra_tab <- pob_tab %>% sample_n(90) %>% 
    mutate(x = ifelse(grupo == "b", 1.5 * x + 1, x))
g_1 <- ggplot(muestra_tab, aes(x = grupo, y = x)) + geom_boxplot(outlier.alpha = 0) +
    geom_jitter(alpha = 0.3) + 
    coord_flip() + ylim(c(0, 20)) + labs(subtitle = "Muestra")
g_2 <- ggplot(pob_tab, aes(x= grupo, y = x)) + geom_boxplot(outlier.alpha = 0) +
    coord_flip() + ylim(c(0, 20)) + labs(subtitle = "Población")
g_1
```



Hacemos primero la prueba del *lineup*:


```{r}
set.seed(9012)
reps <- lineup(null_permute("grupo"), muestra_tab, n = 12)
grafica_cuantiles(reps %>%  mutate(grupo_escondido = factor(digest::digest2int(grupo) %% 177)), 
                             grupo_escondido, x) + facet_wrap(~.sample) + ylab("x") +
    coord_flip() 
```


Podemos distinguir más o menos claramente que está localizada en valores
más altos y tiene mayor dispersión. Por ejemplo, podríamos considerar una
prueba para ver qué tan excepcional es la diferencia entre c y b:

```{r, fig.width = 5, fig.height = 2.5, echo = FALSE}
set.seed(118)
reps <- lineup(null_permute("grupo"), muestra_tab, n = 3000)
dif_obs <- muestra_tab %>% group_by(grupo) %>% 
    summarise(mediana  = median(x)) %>% 
    spread(grupo, mediana) %>% 
    mutate(diferencia = b - c) %>% pull(diferencia)
reps_mediana <- reps %>% 
    group_by(grupo, .sample) %>% 
    summarise(mediana  = median(x)) %>% 
    spread(grupo, mediana) %>% 
    mutate(diferencia = b - c)
g_1 <- ggplot(reps_mediana, aes(sample = diferencia)) + geom_qq(distribution = stats::qunif) +
    geom_hline(yintercept = dif_obs, colour = "red") +
    annotate("text", x = 0.4, y = dif_obs + 0.2, label = "diferencia observada", colour = "red")
g_2 <- ggplot(reps_mediana, aes(x = diferencia)) + geom_histogram(binwidth = 0.1 ) +
    geom_vline(xintercept = dif_obs, colour = "red")
gridExtra::grid.arrange(g_1, g_2, ncol = 2) 
```

La diferencia observada es extrema, y es muy poco probable que hayamos observado tal diferencia
por variación muestra. Tenemos evidencia de que la mediana del grupo "c" es más alta en la población que
"b".



## Ejemplo: tiempos de fusión

Consideremos el ejemplo de fusión de estereogramas que vimos anteriormente. Una pregunta 
que podríamos hacer es: considerando que hay mucha variación en el tiempo de fusión dentro
de cada tratamiento, necesitamos calificar la evidencia de nuestra conclusión (el tiempo de fusión se reduce al 66\% aproximadamente). En este caso, compararemos gráficas de cuantiles de los datos con los
producidos por permutaciones:

```{r, fig.width = 6, fig.height = 5, echo = FALSE}
set.seed(113)
fusion <- read_delim("./datos/fusion_time.txt", delim = " ", trim_ws = TRUE)
reps <- lineup(null_permute("nv.vv"), fusion, 20)
ggplot(reps, aes(sample = time, colour = nv.vv)) +
    geom_qq(distribution = stats::qunif, size = 0.5) +
    facet_wrap(~.sample) + scale_y_log10()
```

¿Podemos identificar los datos? 

Otro enfoque sería probar una característica específica de las muestras. En
este ejemplo, podríamos tomar, por ejemplo, el promedio de los dos cuantiles
(superior e inferior). 

```{r}
stat_fusion <- function(x){
    (quantile(x, 0.75) + quantile(x, 0.25))/2
}
reps <- lineup(null_permute("nv.vv"), fusion, 10000)
dif <- fusion %>% group_by(nv.vv) %>% 
    summarise(mediana = stat_fusion(time)) %>% 
    spread(nv.vv, mediana) %>% mutate(dif = VV / NV ) %>% pull(dif)
cocientes_perm <- reps  %>%  group_by(.sample, nv.vv) %>% 
    summarise(mediana = stat_fusion(time)) %>% 
    spread(nv.vv, mediana) %>% 
    summarise(cociente = (VV / NV))
ggplot(cocientes_perm, aes(x = cociente)) + 
    geom_histogram(binwidth = 0.05) +
    geom_vline(xintercept = dif)
2*ecdf(cocientes_perm$cociente)(dif)
```

Lo que muestra evidencia considerable, aunque no muy fuerte, de que la instrucción verbal ayuda a reducir el tiempo de fusión de los estereogramas. Regresaremos a este ejemplo para discutir la discusión de estadística usada y tratamiento de los datos.

## Ejemplo (dimensión alta) {-}

Referencia de Diane Cook

En el siguiente ejemplo, tenemos 4 grupos de avispas (50 individuos en total),
y para cada individuo se miden expresiones
de 42 genes distintos. La pregunta es: ¿Podemos separar a los grupos de avispas 
dependiendo de sus mediciones. 

En este caso podemos usar análisis discriminante, que busca proyecciones de los
datos en dimensión baja de forma que los grupos sean lo más compactos y separados posibles.

Para probar qué tan bien funciona este método, podemos hacer una prueba de permutación, aplicamos
LDA y observamos los resultados:

```{r, echo = FALSE}
data(wasps)

wasp.lda <- MASS::lda(Group~., data=wasps[,-1])
wasp.ld <- predict(wasp.lda, dimen=2)$x
true <- data.frame(wasp.ld, Group=wasps$Group)

wasp.sim <- data.frame(LD1=NULL, LD2=NULL, Group=NULL, .n=NULL)
for (i in 1:19) {
  x <- wasps
  x$Group <- sample(x$Group)
  x.lda <- MASS::lda(Group~., data=x[,-1])
  x.ld <- predict(x.lda, dimen=2)$x
  sim <- data.frame(x.ld, Group=x$Group, .n=i)
  wasp.sim <- rbind(wasp.sim, sim)
}
pos <- sample(1:20, 1)
d <- lineup(true=true, samples=wasp.sim, pos=pos)
ggplot(d, aes(x=LD1, y=LD2, colour=Group)) + 
  facet_wrap(~.sample, ncol=5) +
  geom_point() + theme(aspect.ratio=1)
```

Y vemos que incluso permutando los grupos, es generalmente posible separarlos en grupos
bien definidos: Existen combinaciones lineales que los separan. Que no podamos distinguir
los datos verdaderos de las replicaciones nulas indica que este método difícilmente puede
servir para separar los grupos claramente.


Otro enfoque sería separar los datos en una muestra de entrenamiento y una de prueba (que discutiremos
en la última sesión). Aplicamos el procedimiento a la muestra de entrenamiento y luego vemos qué pasa
con los datos de prueba:

```{r}
set.seed(8)
wasps_1 <- wasps %>% mutate(u = runif(nrow(wasps), 0, 1))
wasps_entrena <- wasps_1 %>% filter(u <= 0.8)
wasps_prueba <- wasps_1 %>% filter(u > 0.8)                            
                            
wasp.lda <- MASS::lda(Group ~ ., data=wasps_entrena[,-1])
wasp_ld_entrena <- predict(wasp.lda,  dimen=2)$x %>% 
    as_tibble(.name_repair = "universal") %>%
     mutate(tipo = "entrenamiento") %>% 
    mutate(grupo = wasps_entrena$Group)
wasp_ld_prueba <- predict(wasp.lda, newdata = wasps_prueba, dimen=2)$x  %>% 
    as_tibble(.name_repair = "universal") %>%
    mutate(tipo = "prueba")%>% 
    mutate(grupo = wasps_prueba$Group)
wasp_lda <- bind_rows(wasp_ld_entrena, wasp_ld_prueba)
ggplot(wasp_lda, aes(x = LD1, y = LD2, colour = grupo)) + geom_point(size = 3) +
    facet_wrap(~tipo) + scale_color_colorblind()
```

Aunque esta separación es menos efectiva en este ejemplo por la muestra chica, podemos ver
que la separación lograda en los datos de entrenamiento probablemente se debe a variación muestral.


## El jardín de los senderos que se bifurcan

Recientemente (aunque muchos estadísticos lo han repetido desde hace mucho tiempo), se ha reconocido
en campos como la sicología la "crisis de replicabilidad" (ver por ejemplo @falsefindings). Varios estudios que recibieron mucha publicidad inicialmente no han podido ser replicados
posteriormente por otros investigadores. Por ejemplo:

- Hacer [poses poderosas](https://www.sciencedaily.com/releases/2017/09/170911095932.htm) produce cambios fisiológicos que mejoran nuestro desempeño en ciertas tareas
- Patrones de ovulación de las mujeres influyen en los candidatos que escogen para votar
- Mostrar palabras relacionadas con "viejo" hacen que las personas caminen más lento (efectos de [priming](https://www.nature.com/news/nobel-laureate-challenges-psychologists-to-clean-up-their-act-1.11535)) 
En todos estos casos, la evidencia de estos efectos fue respaldada finalmente (pues es requisito para
publicación) por una prueba de hipótesis nula con un valor p menor a 0.05. Este estándar de publicación
es seguido por varias áreas y revistas. Este problema de replicabilidad parece ser más frecuente cuando:

1. Se trata de estudios de potencia baja: mediciones ruidosas y  tamaños de muestra chicos.
2. El plan de análisis no está claramente definido desde un principio (lo cual es difícil cuando
se están investigando "fenómenos no estudiados antes")

Aunque haya varios ejemplos de manipulaciones conscientes para obtener resultados publicables
([p-hacking](https://en.wikipedia.org/wiki/Data_dredging)), 
 como vimos en ejemplos anteriores, hay varias decisiones, todas razonables, que podemos tomar cuando 
estamos buscando las comparaciones correctas. Algunas pueden ser:

- Transformar los datos (tomar o no logaritmos, u otra transformación)
- Editar datos atípicos (razonable si los equipos pueden fallar, o hay errores de captura, por ejemplo)
- Distintas maneras de interpretar los criterios de inclusión (por ejemplo, el estudio se planeó
para personas entre 20 y 30 años, y tenemos tres casos con 33 años. ¿Los dejamos o los usamos?)

Dado un juego de datos, las justificaciones de las decisiones que se toman en cada paso se justifican
y son razonables, pero con datos distintos las decisiones pueden ser diferentes. 
Este es el jardín de los senderos que se bifurcan de [Gelman](http://www.stat.columbia.edu/~gelman/research/published/incrementalism_3.pdf), 
que invalida en parte el uso valores p como criterio de evidencia contra la hipótesis nula.
Ver por ejemplo el [comunicado de la ASA](https://www.amstat.org/asa/files/pdfs/P-ValueStatement.pdf).

## Ejemplo:
En el ejemplo de datos de fusión, decidimos probar, por ejemplo, el promedio de
los cuartiles inferior y superior, lo cual no es una decisión típica pero usamos como
ilustración.


```{r}
set.seed(8812)
media_cuartiles <- function(x){
    (quantile(x, 0.75) + quantile(x, 0.25))/2
}

valor_p_fusion <- function(fusion, stat_fusion = stat_fusion, trans = identity, comp = VV / NV){
    pos <- 1
    reps <- lineup(null_permute("nv.vv"), pos =1, fusion, n = 10000)
    dif <- fusion %>% group_by(nv.vv) %>% 
        summarise(valor_est = {{ stat_fusion }}(time)) %>% 
        spread(nv.vv, valor_est) %>% mutate(dif = {{ comp }} ) %>% pull(dif)
    cocientes_perm <- reps  %>%  group_by(.sample, nv.vv) %>% 
        summarise(valor_est = {{ stat_fusion }} (time)) %>% 
        spread(nv.vv, valor_est) %>% 
        summarise(cociente = ({{ comp }}))
    2*ecdf(cocientes_perm$cociente)(dif)
}
valor_p_fusion(fusion %>% sample_frac(0.8), stat_fusion = mean, trans = identity, comp = VV - NV)
valor_p_fusion(fusion %>% sample_frac(0.8), stat_fusion = median, trans = identity, comp = VV / NV)
valor_p_fusion(fusion %>% sample_frac(0.8), stat_fusion = media_cuartiles, trans = identity, 
               comp = VV / NV)
valor_p_fusion(fusion %>% sample_frac(0.8), stat_fusion = median, trans = log, comp = VV - NV)
valor_p_fusion(fusion %>% sample_frac(0.8), stat_fusion = mean, trans = log, comp = VV - NV)

```

Si existen grados de libertad - muchas veces necesarios para hacer un análisis exitoso-, entonces
los valores p pueden tener poco significado.


En áreas como la sicología, existen ahora movimientos fuertes en favor de la replicación. Este movimiento
sugiere dar valor a los estudios exploratorios que no reportan valor p, y posteriormente, si el estudio
es de interés, puede intentarse una replicación confirmatoria, con potencia más alta y con planes de análisis predefinidos.


