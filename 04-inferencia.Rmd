# Inferencia y Remuestreo


```{r, message = FALSE, echo = FALSE}
library(tidyverse)
library(lubridate)
library(ggthemes)
theme_set(theme_minimal(base_size = 14))
```


Una buena parte de los problemas de análisis de datos tenemos información incompleta. Por ejemplo:

- Sólo observamos una parte de la población o del sistema de interés 
(a veces por costo o tiempo, a veces porque observaciones de interés están en futuro, por ejemplo).
- Existen variables para las cuales no tenemos información que influyen en las medidas de interés

En estos casos, el problema es que **las comparaciones que hacemos están sujetas a fluctuaciones de origen desconocido**: 
variación muestral, acción de variables desconocidas, por ejemplo. Cuando hacemos comparaciones, entonces, debemos de alguna
forma integrar la **incertidumbre** producto de estas fluctuaciones. Y algo que no queremos hacer es **sobreinterpretar** esas fluctuaciones como diferencias sustantivas en las comparaciones que nos interesan.


- Podemos en primer lugar pensar qué variables que no tenemos podrían estar produciendo variación en los datos, y si esa variación tiene algún patrón que podría dificultar nuestras comparaciones de manera sistemática.
- En segundo lugar, cuando esa variación de origen incierto tiene patrones más simples, 
 podemos considerar modelos  para esas fluctuaciones que nos permitan separar señal y ruido en los datos,
o entender qué tanto ruido producen esas fluctuaciones en nuestras comparaciones.

Por ejemplo:

- Si usamos una muestra de una población para generalizar a la población, queremos separar de
comparaciones y diferencias de nuestra muestra que generalizan a la población de otras que se producen por variación muestral
- Cuando queremos juzgar si un conjunto de datos observados es consistente con un modelo (o con otro conjunto de datos observados), considerando que hay variables que no controlamos que afectan a ambos conjuntos de datos.

En todos los casos, el paso importante es intentar separar señales útiles (para explicar, predecir,
o mostrar estructura) de *fluctuaciones* que resultan de variables sobre las que no tenemos información.

Consideraremos dos herramientas fundamentales de la estadística para acercarnos 
a este fin: pruebas de hipótesis e intervalos de confianza.

## Pruebas de hipótesis {-}

Las primeras técnicas que veremos intentan contestar la siguiente pregunta: 
si observamos
cierto patrón en los datos, ¿cómo podemos cuantificar  la evidencia de que es un
patrón notable y no sólo debido a fluctuaciones en los datos particulares que tenemos?
¿Cómo sabemos que no estamos **sobreinterpretando** esas fluctuaciones?


Por ejemplo:

- Un sistema tiene cierto comportamiento "usual" para el cual tenemos datos históricos. 
El sistema presenta fluctuaciones en el tiempo.
- Observamos la última salida de nuestro sistema. Naturalmente, tiene fluctuaciones. 
¿Esas fluctuaciones son consistentes con la operación usual del sistema? ¿Existe
evidencia para pensar que algo en el sistema cambió?

## Comparación con poblaciones de referencia {-}


En algunos casos, podemos construir distribuciones de referencia para comparar resultados
que obtengamos con un "estándar" de variación, y juzgar si nuestros resultados son consistentes con la referencia
o no (@box78).

### Ejemplo {-}

Supongamos que estamos considerando cambios rápidos en una serie de tiempo de alta frecuencia. Hemos observado la serie en su estado "normal" durante un tiempo considerable, y cuando observamos nuevos
datos **quisiéramos juzgar si hay indicaciones o evidencia en contra de que el sistema sigue funcionando
de manera similar**. 

Digamos que monitoreamos ventanas de tiempo de tamaño 20 y necesitamos tomar una decisión. Abajo
mostramos cinco ejemplos donde el sistema opera normalmente, que muestra la variabilidad
en el tiempo en ventanas cortas del sistema.

Ahora suponemos que obtenemos una nueva ventana de datos. ¿Hay evidencia en contra
de que el sistema sigue funcionando de manera similar?

Nuestra primera inclinación debe ser comparar: en este caso, compararamos ventanas históricas con nuestra nueva serie:

```{r, echo = FALSE}
simular_serie <- function(n = 1000, lambda = 1, ac = 0.7, x_inicial = 1){
    x <- numeric(n)
    x[1] <- x_inicial
    for(i in 2:n){
        x[i] <-  ac*(x[i - 1]) + rgamma(1, 1, 1 / lambda)
    }
    tibble(t = 1:n, obs = x) %>% filter(t > 50) %>% 
        mutate(t = t - 50 + 1)
}
```

```{r}
# usamos datos simulados para este ejemplo
set.seed(8812)
historicos <- simular_serie(2000)
```


```{r}
muestrear_ventanas <- function(control, obs, n_ventana = 20, long = 20){
    n_control <- nrow(control)
    inicios <- sample(1:(nrow(control) - long), n_ventana - 1)
    control_lista <- map(inicios, function(i){
        muestra <- filter(control, t %in% seq(i, i + long - 1, 1)) 
        muestra
    })
    dat_lista <- append(control_lista, list(obs))
    orden <- sample(1:n_ventana, n_ventana)
    datos_lineup <- tibble(rep = orden, datos = dat_lista) %>% 
        unnest %>% group_by(rep) %>% mutate(t_0 = t - min(t)) %>% ungroup  
    list(lineup = datos_lineup,
        pos = last(orden))
}
set.seed(12)
obs <- historicos[500:519, ]
ventanas_tbl <- muestrear_ventanas(historicos, obs, n_ventana = 6)
ggplot(ventanas_tbl$lineup, aes(x = t_0, y = obs, colour = factor(rep==ventanas_tbl$pos))) + geom_line() + 
    geom_point() + facet_wrap(~rep, nrow = 2) +
    theme(legend.position = "none") + scale_y_log10()
```

¿Vemos algo diferente en los datos nuevos (el panel de color diferente)? 
Indpendientemente de la respuesta, vemos que hacer **este análisis de manera tan simple no es razonable**: 
seguramente podemos encontrar maneras en que la nueva muestra (4) es diferente
a muestras históricas. Por ejemplo, ninguna de muestras tiene un "forma de montaña" tan clara. Nos preguntamos si no estamos **sobreinterpretando** variaciones que son parte normal del proceso. 

Podemos hacer un mejor análisis si extraemos varias muestras del comportamiento
usual del sistema, graficamos junto a la nueva muestra, y **revolvemos** las gráficas
para que no sepamos cuál es cuál. 
Entonces la pregunta es:

- ¿Podemos detectar donde están los datos nuevos?

Esta se llama una **prueba de lineup**, o una prueba de ronda de sospechosos (tomado de @lineup).
En la siguiente gráfica, en uno de los páneles
están los datos recientemente observados.  ¿Hay algo en los datos que distinga al patrón nuevo?

```{r, echo = FALSE}
obs <- simular_serie(500, x_inicial = last(obs$obs))
prueba_tbl <- muestrear_ventanas(historicos, obs[1:20, ], n_ventana = 20)
ggplot(prueba_tbl$lineup, aes(x = t_0, y = obs)) + geom_line() + 
     facet_wrap(~rep, nrow = 4) + scale_y_log10()
```

**Ejercicio**: ¿cuáles son los datos nuevos (solo hay un panel con los nuevos datos)? 
¿Qué implica que la gráfica que escogamos como "más diferente" no sean los datos nuevos? 
¿Qué implica que le "atinemos" a la gráfica de los datos nuevos?

Ahora contrastemos con  un ejemplo donde ocurre un cambio considerable. En el siguiente caso,

```{r, message=FALSE, echo = FALSE}
set.seed(912)
obs_dif <- simular_serie(500, lambda = 3, ac = 0.3, x_inicial = last(historicos$obs))
prueba_tbl <- muestrear_ventanas(historicos, obs_dif[10:29, ], n_ventana = 20)
ggplot(prueba_tbl$lineup, aes(x = t_0, y = obs)) + geom_line() + 
   facet_wrap(~rep, nrow = 4) + scale_y_log10()
```

Aunque es imposible estar seguros de que ha ocurrido un cambio, la diferencia de una de las
series es muy considerable. Si identificamos los datos correctos,
la probabilidad de que hayamos señalado la nueva serie "sobreinterpretando" 
fluctuaciones en un proceso que sigue comportándose normalente es 0.05 - relativamente baja.
Detectar los datos diferentes es evidencia en contra de que el sistema sigue funcionando de la misma
manera que antes.

**Observaciones y terminología**:

1. Llamamos *hipótesis nula* a la hipótesis de que los nuevos datos son producidos bajo
las mismas condiciones que los datos de control o de referencia.

3. **Si no escogemos la gráfica de los nuevos datos, nuestra conclusión es que la prueba no aporta evidencia
en contra de la hipótesis nula.** 

4. **Si escogemos la gráfica correcta, nuestra conclusión es que la prueba aporta evidencia
en contra de la hipótesis nula.**

¿Qué tan fuerte es la evidencia, en caso de que descubrimos los datos no nulos? 

5. Cuando el número de paneles es más grande y detectamos los datos, la evidencia es más alta en contra de la nula. 
Decimos que el *nivel de significancia de la prueba* es la probabilidad de atinarle a los
datos correctos cuando la hipótesis nula es cierta (el sistema no ha cambiado). 
En el caso de 20 paneles, la significancia es de 1/20 = 0.05. Cuando detectamos los datos nuevos,
niveles de significancia más bajos implican más evidencia en contra de la nula.

5. Si acertamos, y la diferencia es más notoria y fue muy fácil detectar la gráfica diferente,
esto también sugiere más evidencia en contra de la hipótesis nula. Este nivel de dificultad de detección
es una versión cualitativa de un valor p.

6. Finalmente, esta prueba rara vez (o nunca) **nos da seguridad completa acerca de ninguna conclusión**, aún cuando
hiciéramos muchos páneles.

## Comparando distribuciones {-}

Ahora intentamos un ejemplo más común.

Supongamos tenemos mediciones para tres grupos  a, b y c. Podemos pensar que son muestras de
tres poblaciones distintas, por ejemplo.  Quizá la pregunta detrás de esta 
comparación es: el grupo de clientes b recibió una promoción especial. ¿Están gastando
más? La medición que comparamos es el gasto de los clientes.

Nos interesa saber qué tan distintos son los datos que produce cada condición - no solamente
qué tan diferentes son las muestras. Hay muchos aspectos
que podríamos cuestionar acerca de cómo son diferentes los tres sistemas. En este caso,
nos preguntamos, por ejemplo, si las distribuciones son similares o no.

```{r, fig.width = 6, fig.height = 3, echo = FALSE}
library(tidyverse)
library(ggrepel)
library(nullabor)
library(knitr)
theme_set(theme_minimal(base_size = 14))
paleta <- scale_colour_manual(values = c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7"))
source("R/funciones_auxiliares.R")
```

```{r, echo = FALSE}
set.seed(8)
pob_tab <- tibble(id = 1:2000, x = rgamma(2000, 4, 1), 
    grupo = sample(c("a","b", "c"), 2000, prob = c(4,2,1), replace = T))
muestra_tab <- pob_tab %>% sample_n(125)
g_1 <- ggplot(muestra_tab, aes(x = grupo, y = x)) + geom_boxplot(outlier.alpha = 0) +
    geom_jitter(alpha = 0.3) + 
    coord_flip() +  labs(subtitle = "Muestra")
g_1
```

En la muestra observamos diferencias entre los grupos. Pero notamos adicionalmente que
hay mucha variación dentro de cada grupo. **Nos podríamos preguntar entonces si las diferencias
que observamos se deben variación muestral, por ejemplo.** Quizá con más dinero y tiempo podríamos contrastar a todos los clientes y veríamos que sus distribuciones son prácticamente iguales.

Podemos construir ahora una *hipótesis nula*, que establece que las observaciones
provienen de una población similar:

- Los tres procesos que producen datos (a, b, c) 
son prácticamente indistiguibles desde el punto de vista del sistema. En este
caso, la variación que observamos se debería a que tenemos información incompleta.

Como en el ejemplo anterior **necesitamos construir o obtener una distribución de referencia** para comparar
qué tan extremos o diferentes son los datos que observamos. Esa distribución de referencia debería
estar basada en el supuesto de que los grupos producen datos de distribuciones similares.

Hay muchas maneras de construir distribuciones de referencia. 
Una manera versátil de producirlas es usando pruebas de permutaciones. 


## Permutaciones y el lineup {-}

Para abordar este problema podemos pensar en usar permutaciones de los grupos de
la siguiente forma (@box78, @timboot14):

- Si los grupos producen
datos bajo procesos idénticos, entonces los grupos a, b, c solo son etiquetas que no contienen información.
- Podríamos entonces **permutar al azar** las etiquetas y observar nuevamente la gráfica
de caja y brazos por grupos. 
- Bajo la hipótesis nula (grupos idénticos), esta es una muestra tan verosímil como la que obtuvimos.
- Si la hipótesis nula es cercana a ser cierta, no deberíamos de poder distinguir fácilmente los 
datos observados de los producidos con las permutaciones al azar.


Vamos a intentar esto, por ejemplo usando una gráfica de cuantiles. Hacemos un *lineup*, o una
*rueda de sospechosos* (usamos el paquete @nullabor, ver @lineup), 
donde 19 de los acusados son generados mediante permutaciones al azar 
- las etiquetas de los datos observados se permutan al azar en los datos-,
y el culpable (los verdaderos datos) están en una posición escogida al azar. ¿Podemos identificar
los datos verdaderos? Para evitar sesgarnos, también ocultamos la etiqueta verdadera

```{r}
set.seed(88)
reps <- lineup(null_permute("grupo"), muestra_tab, n = 20)
grafica_cuantiles(reps %>%  mutate(grupo_1 = factor(digest::digest2int(grupo) %% 177)), 
                             grupo_1, x) + 
    facet_wrap(~.sample, ncol = 5) + ylab("x")
```

Y la pregunta que hacemos es **podemos distinguir nuestra muestra entre todas las 
replicaciones producidas con permutaciones**? 

**Ejercicio**: ¿dónde están los datos observados? Según tu elección,  ¿qué tan diferentes son los
datos observados de los datos nulos?

En este ejemplo, es difícil indicar cuáles son los datos. Los grupos tienen distribuciones
similares y es factible que las diferencias que observamos se deban a variación muestral.  

Consideremos algunos cálculos: si los verdaderos datos son consistentes con nuestra hipótesis
nula, entonces la probabilidad de escoger al verdadero culpable es de 1/20 = 0.05. Esta se
llama *significancia de nuestra prueba de lineup*.

- Si la persona escoge los verdaderos datos, rechazamos la hipótesis nula (equivalencia entre los
tres bonches de datos), y decimos que los datos son *significativamente diferentes* al nivel 0.05. Esto es 
evidencia en contra de que los datos se producen de manera homogénea, independientemente del grupo.

- Si la persona escoge uno de los datos permutados, no rechazamos la hipótesis nula: no encontramos evidencia en contra de que los tres grupos producen datos con
distintas distribuciones.

## Prueba de permutaciones {-}

Cuando nos interesa una característica particular en los datos 
(¿las medianas  de los grupos medias? ¿Sus rangos intercuartiles son distintos?, etc.), podemos
producir alguna *estadística de prueba numérica*, en lguar de una estadística gráfica.

En nuestros ejemplos anteriores, nuestra estadística era una gráfica, y 
buscábamos identificar si la estadística para nuestra muestra era *extrema* comparado con
valores que la estadística toma bajo la hipótesis nula: es decir, comparado
con la distribución de referencia.

Ahora supongamos que hacemos una pregunta mucho más específica, que se refiere a un número particular:
¿la media del grupo b  es diferente de la del grupo c? En este caso

- Definimos una *estadística de prueba*, que mida las diferencias que nos interesan (por ejemplo, diferencia de medianas entre dos grupos)
- Bajo nuestra hipótesis nula, producimos una cantidad grande de muestras permutando las etiquetas de los grupos. Esta es nuestra *distribución de referencia* (ya no estamos limitados a 20 replicaciones, por ejemplo)
- Evaluamos nuestra estadística en cada conjunto de datos nulos.
- Observamos la distribución de estos valores: esta es nuestra distribución de referencia
- Y la pregunta clave es: ¿el valor de la estadística en nuestra muestra es *extrema* en comparación a la distribución de referencia?

Esta es una implementación:

```{r, fig.width = 5, fig.height = 2.5, message = FALSE, warning = FALSE}
# esta es la función que calcula la estadística:
dif_medianas <- function(datos){
    datos %>% 
        filter(grupo != "a") %>% 
        summarise(mediana = median(x)) %>% 
        spread(grupo, mediana) %>% 
        mutate(diferencia = b - c)
}
# Hacemos permutaciones
set.seed(118)
# produce 20000 permutaciones de las etiquetas de los grupos
reps <- lineup(null_permute("grupo"), muestra_tab, n = 20000)
# Mismo cálculo para muestra y permutaciones
reps_mediana <- reps %>% group_by(grupo, .sample) %>% dif_medianas
```

Y graficamos nuestros resultados (con un histograma y una gráfica de cuantiles, por ejemplo). la
estadística evaluada un cada una de nuestras muestras permutadas:

```{r, fig.width = 6, fig.height = 3}
g_1 <- ggplot(reps_mediana, aes(sample = diferencia)) + geom_qq(distribution = stats::qunif)  +
    xlab("f") + ylab("diferencia") + labs(subtitle = "Distribución nula o de referencia")
g_2 <- ggplot(reps_mediana, aes(x = diferencia)) + geom_histogram(binwidth = 0.3) + 
    coord_flip() + xlab("") + labs(subtitle = " ")
gridExtra::grid.arrange(g_1, g_2, ncol = 2) 
```

Este es el rango de fluctuación usual para nuestra estadística *bajo la hipótesis de que
los grupos b y c provienen de la misma distribución*.

Ahora calculamos el valor que obtuvimos en nuestros datos:

```{r}
dif_obs <- muestra_tab %>% group_by(grupo) %>% dif_medianas %>% pull(diferencia)
dif_obs
```

Y vemos que no es un valor extremo en la distribución de referencia que vimos arriba: esta
muestra no aporta evidencia en contra de que los grupos tienen distribuciones similares.

Podemos graficar otra vez marcando el valor de referencia:

```{r}
# Función de distribución acumulada (inverso de función de cuantiles)
dist_perm <- ecdf(reps_mediana$diferencia)
# Calculamos el percentil del valor observado
percentil_obs <- dist_perm(dif_obs %>% round(2))
```


```{r, fig.width = 6, fig.height = 3, echo = FALSE}
g_1 <- ggplot(reps_mediana, aes(sample = diferencia)) + geom_qq(distribution = stats::qunif)  +
    xlab("f") + ylab("diferencia") + labs(subtitle = "Distribución nula o de referencia") +
    geom_hline(yintercept = dif_obs, colour = "red") +
    annotate("text", x = 0.3, y = dif_obs + 0.2, label = "diferencia observada", colour = "red")
g_2 <- ggplot(reps_mediana, aes(x = diferencia)) + geom_histogram(binwidth = 0.3) + 
    coord_flip() + xlab("") + labs(subtitle = " ") +
    geom_vline(xintercept = dif_obs, colour = "red") +
    annotate("text", x = dif_obs, y = 2000, label = percentil_obs,vjust = -0.2, colour = "red")
gridExtra::grid.arrange(g_1, g_2, ncol = 2) 
```


Y notamos que el resultado obtenido en nuestra muestra no es excepcionalmente grande. Otra vez, no hay evidencia en contra de la hipótesis nula: el resultado obtenido no es muy extremo con respecto a la variabilidad que ocurre bajo la hipótesis nula.


### Valor p

Nótese que calculamos una cantidad adicional, que es el percentil donde nuestra observación cae
en la distribución generada por las permutación. Esta cantidad puede usarse para calcular un 
**valor p**. Podemos calcular, por ejemplo:

- **Valor p de dos colas**: Si la hipótesis nula es cierta, ¿cuál es la
probabilidad de observar una diferencia en valor absoluto tan extrema o más extrema de lo que observamos? 
Podemos calcular
la respuesta como 

```{r}
1 - abs(dist_perm(dif_obs) - dist_perm(-dif_obs))
```

Repitimos el ejemplo para otra muestra (en este ejemplo el proceso generador
de datos es diferente para el grupo b):

```{r, echo = FALSE}
set.seed(72)
muestra_tab <- pob_tab %>% sample_n(90) %>% 
    mutate(x = ifelse(grupo == "b", 1.7 * x + 1, x))
g_1 <- ggplot(muestra_tab, aes(x = grupo, y = x)) + geom_boxplot(outlier.alpha = 0) +
    geom_jitter(alpha = 0.3) + 
    coord_flip() + ylim(c(0, 20)) + labs(subtitle = "Muestra")
g_2 <- ggplot(pob_tab, aes(x= grupo, y = x)) + geom_boxplot(outlier.alpha = 0) +
    coord_flip() + ylim(c(0, 20)) + labs(subtitle = "Población")
g_1
```



Hacemos primero la prueba del *lineup*:


```{r}
set.seed(90)
reps <- lineup(null_permute("grupo"), muestra_tab, n = 12)
grafica_cuantiles(reps %>%  mutate(grupo_escondido = factor(digest::digest2int(grupo) %% 177)), 
                             grupo_escondido, x) + facet_wrap(~.sample) + ylab("x") +
    coord_flip() 
```


Podemos distinguir más o menos claramente que está localizada en valores
más altos y tiene mayor dispersión. 

O podríamos considerar una
estadística de prueba numérica para ver qué tan excepcional es la diferencia entre c y b.
En este caso, lo hacemos para la diferencia de medias, por ejemplo:

```{r, fig.width = 6, fig.height = 3, echo = FALSE, message = FALSE}
set.seed(118)
reps <- lineup(null_permute("grupo"), muestra_tab %>% filter(grupo != "a"), n = 20000)
dif_obs <- muestra_tab %>% group_by(grupo) %>% 
    summarise(mediana  = mean(x)) %>% 
    spread(grupo, mediana) %>% 
    mutate(diferencia = b - c) %>% pull(diferencia)
reps_mediana <- reps %>% 
    group_by(grupo, .sample) %>% 
    summarise(mediana  = mean(x)) %>% 
    spread(grupo, mediana) %>% 
    mutate(diferencia = b - c)
dist_perm_2 <- ecdf(reps_mediana$diferencia)
g_1 <- ggplot(reps_mediana, aes(sample = diferencia)) + geom_qq(distribution = stats::qunif) +
    geom_hline(yintercept = dif_obs, colour = "red") +
    annotate("text", x = 0.4, y = dif_obs + 0.2, label = "diferencia observada", colour = "red")
g_2 <- ggplot(reps_mediana, aes(x = diferencia)) + geom_histogram(binwidth = 0.5) +
    geom_vline(xintercept = dif_obs, colour = "red")
gridExtra::grid.arrange(g_1, g_2, ncol = 2) 
```

Que tiene un valor $p$ de dos colas de
```{r}
2*(1 - dist_perm_2(dif_obs))
```

La diferencia observada es más extrema, y es muy poco probable que hayamos observado tal diferencia
por variación muestra. Tenemos evidencia de que la mediana del grupo "b" es más alta en la población que
"c".

## Ejemplo: tiempos de fusión (opcional) {-}

Consideremos el ejemplo de fusión de estereogramas que vimos anteriormente. Una pregunta 
que podríamos hacer es: considerando que hay mucha variación en el tiempo de fusión dentro
de cada tratamiento, necesitamos calificar la evidencia de nuestra conclusión 
(el tiempo de fusión se reduce con informaión verbal). En este caso, compararemos gráficas de cuantiles de los datos con los
producidos por permutaciones:

```{r, fig.width = 6, fig.height = 5, echo = FALSE}
set.seed(113)
fusion <- read_delim("./datos/fusion_time.txt", delim = " ", trim_ws = TRUE)
reps <- lineup(null_permute("nv.vv"), fusion, 20)
ggplot(reps, aes(sample = time, colour = nv.vv)) +
    geom_qq(distribution = stats::qunif, size = 0.5) +
    facet_wrap(~.sample) + scale_y_log10()
```

**Ejercicio**: ¿Podemos identificar los datos? 

Otro enfoque sería probar una característica específica de las muestras. En
este ejemplo, podríamos tomar, por ejemplo, el promedio de los dos cuantiles
(superior e inferior). 

```{r}
stat_fusion <- function(x){
    (quantile(x, 0.75) + quantile(x, 0.25))/2
}
# permutar
reps <- lineup(null_permute("nv.vv"), fusion, 10000)
# diferencia 
dif <- fusion %>% group_by(nv.vv) %>% 
    summarise(est = stat_fusion(time)) %>% 
    spread(nv.vv, est) %>% mutate(dif = VV / NV ) %>% pull(dif)
cocientes_perm <- reps  %>%  group_by(.sample, nv.vv) %>% 
    summarise(est = stat_fusion(time)) %>% 
    spread(nv.vv, est) %>% 
    summarise(cociente = (VV / NV))
ggplot(cocientes_perm, aes(x = cociente)) + 
    geom_histogram(binwidth = 0.05) +
    geom_vline(xintercept = dif)
```

Y el valor p de dos colas es

```{r}
dist_perm_nv <- ecdf(cocientes_perm$cociente)
dist_perm_nv(dif) + (1 - dist_perm_nv(1/dif))
```

Lo que muestra evidencia considerable, aunque no muy fuerte, de que la instrucción verbal ayuda a reducir el tiempo de fusión de los estereogramas. Regresaremos a este ejemplo para discutir la discusión de estadística usada y tratamiento de los datos.

##  Separación de grupos {-}

Este ejemplo tomado de @cookwasps (tanto la idea como el código). La pregunta que se aborda en
ese estudio es:

- Existen métodos de clasificación (supervisados o no supervisados) para formar grupos en términos
de variables que describen a los individuos
- Estos métodos (análisis discriminante, o k-means, por ejemplo), pretenden formar grupos compactos,
bien separados entre ellos. Cuando aplicamos el método, obtenemos clasificadores basados en las variables
de entrada.
- La pregunta es: ¿los grupos resultantes son producto de patrones que se generalizan a la población, o
capitalizaron en variación aleatoria para formarse?
- Especialmente cuando tenemos muchas mediciones de los individuos, y una muestra relativamente chica,
Es relativamente fácil encontrar combinaciones de variables que separan los grupos, aunque estas combinaciones
y diferencias están basadas en ruido y no generalizan a la población.

Como muestran en @cookwasps, el *lineup* es útil para juzgar si tenemos evidencia en contra de que los
grupos en realidad son iguales, y usamos variación muestral para separarlos.


### Avispas {-}

En el siguiente ejemplo, tenemos 4 grupos de avispas (50 individuos en total),
y para cada individuo se miden expresiones
de 42 genes distintos. La pregunta es: ¿Podemos separar a los grupos de avispas 
dependiendo de sus mediciones?

En este caso podemos usar análisis discriminante, que busca proyecciones de los
datos en dimensión baja de forma que los grupos sean lo más compactos y separados posibles.

Para probar qué tan bien funciona este método, podemos hacer una prueba de permutación, aplicamos
LDA y observamos los resultados. 

```{r, echo = FALSE}
data(wasps)

wasp.lda <- MASS::lda(Group~., data=wasps[,-1])
wasp.ld <- predict(wasp.lda, dimen=2)$x
true <- data.frame(wasp.ld, Group=wasps$Group)

wasp.sim <- data.frame(LD1=NULL, LD2=NULL, Group=NULL, .n=NULL)
for (i in 1:19) {
  x <- wasps
  x$Group <- sample(x$Group)
  x.lda <- MASS::lda(Group~., data=x[,-1])
  x.ld <- predict(x.lda, dimen=2)$x
  sim <- data.frame(x.ld, Group=x$Group, .n=i)
  wasp.sim <- rbind(wasp.sim, sim)
}
pos <- sample(1:20, 1)
d <- lineup(true=true, samples=wasp.sim, pos=pos)
ggplot(d, aes(x=LD1, y=LD2, colour=Group)) + 
  facet_wrap(~.sample, ncol=5) +
  geom_point() + theme(aspect.ratio=1)
```

Y vemos que incluso permutando los grupos, es generalmente posible separarlos en grupos
bien definidos: la búsqueda es suficientemente agresiva para encontrar 
 combinaciones lineales que los separan. Que no podamos distinguir
los datos verdaderos de las replicaciones nulas indica que este método difícilmente puede
servir para separar los grupos claramente.

Otro enfoque sería separar los datos en una muestra de entrenamiento y una de prueba (que discutiremos
en la última sesión). Aplicamos el procedimiento a la muestra de entrenamiento y luego vemos qué pasa
con los datos de prueba:

```{r}
set.seed(8)
wasps_1 <- wasps %>% mutate(u = runif(nrow(wasps), 0, 1))
wasps_entrena <- wasps_1 %>% filter(u <= 0.8)
wasps_prueba <- wasps_1 %>% filter(u > 0.8)                            
                            
wasp.lda <- MASS::lda(Group ~ ., data=wasps_entrena[,-1])
wasp_ld_entrena <- predict(wasp.lda,  dimen=2)$x %>% 
    as_tibble(.name_repair = "universal") %>%
     mutate(tipo = "entrenamiento") %>% 
    mutate(grupo = wasps_entrena$Group)
wasp_ld_prueba <- predict(wasp.lda, newdata = wasps_prueba, dimen=2)$x  %>% 
    as_tibble(.name_repair = "universal") %>%
    mutate(tipo = "prueba")%>% 
    mutate(grupo = wasps_prueba$Group)
wasp_lda <- bind_rows(wasp_ld_entrena, wasp_ld_prueba)
ggplot(wasp_lda, aes(x = LD1, y = LD2, colour = grupo)) + geom_point(size = 3) +
    facet_wrap(~tipo) + scale_color_colorblind()
```

Aunque esta separación de datos es menos efectiva en este ejemplo por la muestra chica, podemos ver
que la separación lograda en los datos de entrenamiento probablemente se debe a variación muestral.


## La "crisis de replicabilidad" {-}

Recientemente (aunque muchos estadísticos lo han repetido desde hace mucho tiempo), se ha reconocido
en campos como la sicología la "crisis de replicabilidad" (ver por ejemplo @falsefindings). Varios estudios que recibieron mucha publicidad inicialmente no han podido ser replicados
posteriormente por otros investigadores. Por ejemplo:

- Hacer [poses poderosas](https://www.sciencedaily.com/releases/2017/09/170911095932.htm) produce cambios fisiológicos que mejoran nuestro desempeño en ciertas tareas
- Patrones de ovulación de las mujeres influyen en los candidatos que escogen para votar
- Mostrar palabras relacionadas con "viejo" hacen que las personas caminen más lento (efectos de [priming](https://www.nature.com/news/nobel-laureate-challenges-psychologists-to-clean-up-their-act-1.11535)) 

En todos estos casos, la evidencia de estos efectos fue respaldada finalmente (pues es requisito para
publicación) por una prueba de hipótesis nula con un valor p menor a 0.05. La razón es que este estándar de publicación
es seguido por varias áreas y revistas. La no-replicación va mucho más allá de la tasa de falsos positivos que
sugeriría este método (es decir, menor a 0.05).

Este problema de replicabilidad parece ser más frecuente cuando:

1. Se trata de estudios de potencia baja: mediciones ruidosas y  tamaños de muestra chicos.
2. El plan de análisis no está claramente definido desde un principio (lo cual es difícil cuando
se están investigando "fenómenos no estudiados antes")

¿A qué se atribuye esta crisis de replicabilidad?


## El jardín de los senderos que se bifurcan {-}

Aunque haya varios ejemplos de manipulaciones conscientes --e incluso, en menos casos,
malintencionadas-- para obtener resultados publicables o significativos
([p-hacking](https://en.wikipedia.org/wiki/Data_dredging)), 
 como vimos en ejemplos anteriores, hay varias decisiones, todas razonables, que podemos tomar cuando 
estamos buscando las comparaciones correctas. Algunas pueden ser:

- Transformar los datos (tomar o no logaritmos, u otra transformación)
- Editar datos atípicos (razonable si los equipos pueden fallar, o hay errores de captura, por ejemplo)
- Distintas maneras de interpretar los criterios de inclusión (por ejemplo, el estudio se planeó
para personas entre 20 y 30 años, y tenemos tres casos con 33 años. ¿Los dejamos o los usamos?)

Dado un juego de datos, las justificaciones de las decisiones que se toman 
en cada paso son razonables, pero con datos distintos las decisiones podrían ser diferentes. 
Este es el **jardín de los senderos que se bifurcan**  [Gelman](http://www.stat.columbia.edu/~gelman/research/published/incrementalism_3.pdf), 
que invalida en parte el uso valores p como criterio de evidencia contra la hipótesis nula.

Esto es exacerbado por el hecho de que existe presión fuerte sobre los investigadores
para producir resultados publicables (p < 0.05), y justamente por el hecho
de tomar un punto de corte fijo como necesario para publicar.

Ver por ejemplo el [comunicado de la ASA](https://www.amstat.org/asa/files/pdfs/P-ValueStatement.pdf).

## Ejemplo: decisiones de análisis y valores p {-}

En el ejemplo de datos de fusión, decidimos probar, por ejemplo, el promedio de
los cuartiles inferior y superior, lo cual no es una decisión típica pero usamos como
ilustración. Ahora intentamos usar distintas mediciones de la diferencia entre los grupos,
usando distintas medidas resumen y transformaciones (por ejemplo, con o sin logaritmo). Aquí hay
unas 12 combinaciones distintas para hacer el análisis (multiplicadas por criterios
de "aceptación de datos en la muestra", que simulamos tomando una submuestra al azar):


```{r}
set.seed(8812)
media_cuartiles <- function(x){
    (quantile(x, 0.75) + quantile(x, 0.25))/2
}
valor_p_fusion <- function(fusion, stat_fusion = stat_fusion, trans = identity, comp = VV / NV){
    pos <- 1
    reps <- lineup(null_permute("nv.vv"), pos =1, fusion, n = 20000)
    dif <- fusion %>% group_by(nv.vv) %>% 
        summarise(valor_est = {{ stat_fusion }}(time)) %>% 
        spread(nv.vv, valor_est) %>% mutate(dif = {{ comp }} ) %>% pull(dif)
    cocientes_perm <- reps  %>%  group_by(.sample, nv.vv) %>% 
        summarise(valor_est = {{ stat_fusion }} (time)) %>% 
        spread(nv.vv, valor_est) %>% 
        summarise(diferencia = ({{ comp }}))
    dist_perm <- ecdf(cocientes_perm$diferencia)
    dist_perm(dif) + (1 - dist_perm(-dif))
}
```



```{r}
set.seed(772)
valor_p_fusion(fusion %>% sample_frac(0.9), 
               stat_fusion = mean, trans = identity, comp = VV - NV)
valor_p_fusion(fusion %>% sample_frac(0.9), 
               stat_fusion = median, trans = identity, comp = log(VV / NV))
valor_p_fusion(fusion %>% sample_frac(0.9), 
               stat_fusion = media_cuartiles, trans = identity, 
               comp = log(VV / NV))
valor_p_fusion(fusion %>% sample_frac(0.9), 
               stat_fusion = median, trans = log, comp = VV - NV)
valor_p_fusion(fusion %>% sample_frac(0.9), stat_fusion = mean, trans = log, comp = VV - NV)

```

Si existen grados de libertad - muchas veces necesarios para hacer un análisis exitoso-, entonces
los valores p pueden tener poco significado.

## Soluciones {-}

El primer punto importante es reconocer que la mayor parte de nuestro trabajo
es **exploratorio** (recordemos el proceso complicado del análisis de datos de refinamiento de preguntas). 
En este tipo de trabajo, reportar valores p puede tener poco sentido,
y mucho menos aceptar algo "verdadero" cuando pasa un umbral de significancia dado.

Nuestro interés principal al hacer análisis es expresar correctamente y de manera útil la incertidumbre (asociada a variación muestral, por ejemplo) para que el proceso de toma de decisiones sea informado. **Un** resumen de **un número** (valor $p$, o el que sea) no puede ser tomado como criterio para tomar una decisión que generalmente es compleja.

En la siguiente sección nos concentraremos en una cara más útil de esta
valuación de evidencia, cuantificando la variabilidad que nuestros resultados pueden
tener hasta donde sea posible.

Por otra parte, los estudios confirmatorios (donde se reportan valores p ) 
también tienen un lugar. En áreas como la sicología, existen ahora movimientos fuertes en 
favor de la replicación de estudios prometedores pero donde hay sospecha
de grados de libertad del investigador. Este movimiento
sugiere dar valor a los **estudios exploratorios** que no reportan valor p, 
y posteriormente, si el estudio
es de interés, puede intentarse una **replicación confirmatoria, con potencia más alta y con planes de análisis predefinidos**.


