# Inferencia y Remuestreo


```{r, message = FALSE}
library(tidyverse)
library(lubridate)
library(ggthemes)
theme_set(theme_minimal(base_size = 14))
```

Una buena parte de los problemas de análisis de datos tratan de separar señales útiles
para el problema que nos interesa de ruido producido por variación de variables que no tenemos
control. Por ejemplo:

- Si usamos una muestra de una población para generalizar a la población, queremos separar de
aspectos que generalizan a la población de otros que sólo dependen de variación muestral
- Cuando queremos juzgar si un conjunto de datos observados es consistente con un modelo o
con otro conjunto de datos observados, considerando que hay variables que no controlamos que
afectan a ambos conjunots de datos.

En todos los casos, el paso importante es intentar separar señales útiles (para explicar, predecir,
o mostrar estructura) de *fluctuaciones* que resultan de variables de las que no tenemos información.


## Pruebas de hipótesis

La primera herramienta que veremos tiene como propósito 

### Ejemplo {-}

Por ejemplo, supongamos que estamos considerando cambios rápidos en una serie de tiempo de alta frecuencia. Hemos observado la serie en su estado "normal" durante un tiempo considerable, y cuando observamos nuevos
datos quisiéramos juzgar si hay indicaciones o evidencia en contra de que el sistema sigue funcionando
de manera similar. Digamos que monitoreamos ventanas de tamaño 20 y necesitamos tomar una decisión. Abajo
mostramos cinco ejemplos donde el sistema opera normalmente, y un ejemplo de los datos más recientes:

```{r}
simular_serie <- function(n = 1000, lambda = 1, ac = 0.7, x_inicial = 1){
    x <- numeric(n)
    pendiente <- numeric(n)
    x[1] <- x_inicial
    for(i in 2:n){
        x[i] <-  ac*(x[i - 1]) + rgamma(1, 1, 1 / lambda)
        if(i >= 10){
            x_s <- x[i - seq(9, 0, -1)]
            t_s <- seq(0, 9, 1)
            s <- sd(lm.fit(cbind(1, t_s), x_s)$residuals)
            #s <- sd(x_s)
            pendiente[i] <- s
        }
    }
    tibble(t = 1:n, obs = x, pendiente = pendiente) %>% filter(t > 50)
}
tbl <- simular_serie(2000)
muestrear_ventanas <- function(control, obs, n_ventana = 20, long = 20){
    n_control <- nrow(control)
    inicios <- sample(1:(nrow(control) - long), n_ventana - 1)
    control_lista <- map(inicios, function(i){
        filter(control, t %in% seq(i, i + long - 1))
    })
    dat_lista <- append(control_lista, list(obs))
    orden <- sample(1:n_ventana, n_ventana)
    list(lineup = tibble(rep = orden, datos = dat_lista) %>% 
        unnest %>% group_by(rep) %>% mutate(t_0 = t - min(t)) %>% ungroup,
        pos = last(orden))
}
set.seed(1213)
obs <- tbl[500:519, ]
ventanas_tbl <- muestrear_ventanas(tbl, obs, n_ventana = 6)
ggplot(ventanas_tbl$lineup, aes(x = t_0, y = obs, colour = factor(rep==ventanas_tbl$pos))) + geom_line() + 
    geom_point() + facet_wrap(~rep, nrow = 2) +
    theme(legend.position = "none") + scale_y_log10()
```


Cuando nos señalan cuáles son los datos nuevos, 
es fácil encontrar aspectos particulares que distinguen
las nuevas observaciones de las anteriores. Pero hay variabilidad considerable en el
proceso usual, de forma que nos preguntamos si no estamos sobreinterpretando 
variaciones que son parte normal del proceso.

Repetimos el ejercicio con nuevos datos: ¿cuál de los siugientes es diferente?


```{r}
obs <- simular_serie(500, x_inicial = last(obs_tbl$obs))
prueba_tbl <- muestrear_ventanas(tbl, obs[1:20, ], n_ventana = 20)
ggplot(prueba_tbl$lineup, aes(x = t_0, y = obs)) + geom_line() + 
     facet_wrap(~rep, nrow = 4) + scale_y_log10()
```

**Ejercicio**: ¿cuáles son los datos nuevos (solo hay un panel con los nuevos datos)? ¿Las elecciones tomadas son consistentes con el azar, o se desvían hacia un panel donde no están los datos nuevos?


Ahora contrastemos con  un ejemplo donde ocurre un cambio considerable:

```{r}
obs_dif <- simular_serie(500, lambda = 3, ac = 0.3, x_inicial = last(obs_tbl$obs))
prueba_tbl <- muestrear_ventanas(tbl, obs_dif[1:20, ], n_ventana = 20)
ggplot(prueba_tbl$lineup, aes(x = t_0, y = obs)) + geom_line() + 
   facet_wrap(~rep, nrow = 4) + scale_y_log10()
```

Aunque es imposible estar seguros de que ha ocurrido un cambio considerable, la diferencia de una de las
series es muy considerable. La probabilidad de haber acertado por alguna característica particular
que aparece por azar es de 0.05 - relativamente baja.

**Observaciones y terminología**:

1. Llamamos *hipótesis nula* a la hipótesis de que los nuevos datos son producidos bajo
las mismas condiciones que los datos control.

2. Si no escogemos la gráfica correcta, nuestra conclusión es que la prueba no aporta evidencia
en contra de la hipótesis nula. 

3. Si escogemos la gráfica correcta, nuestra conclusión es que la prueba aporta evidencia
en contra de la hipótesis nula.

¿Qué tan fuerte es la evidencia, en caso de que descubrimos los datos no nulos? 

4. Cuando el número de paneles es más grande y detectamos los datos, la evidencia es más alta en contra de la nula. La razón es que si todos los datos son nulos, escogemos la gráfica "diferente" usando características de variación que ocurren naturalmente el sistema: es decir, escogimos al azar el panel distinto. Decimos que el *nivel de significancia de la prueba* es la probabilidad de atinarle a los
datos correctos cuando la hipótesis nula es razonable. En el caso de 20 paneles, es de 1/20 = 0.05

5. Si acertamos, y la diferencia es más notoria y fue más fácilmente detectar la gráfica diferente,
esto también sugiere más evidencia en contra de la hipótesis nula.

6. Adicionalmente, esta prueba nos puede indicar con mucho detalle qué es lo que puede estar cambiando.

7. Finalmente, esta prueba nunca nos da seguridad completa acerca de ninguna conclusión, aún cuando
hiciéramos muchos páneles.

## Comparación con distribuciones de referencia {-}

En el ejemplo anterior, nuestra estadística de prueba es una gráfica. Cuando la estadística
de prueba es muy extrema comparado con lo que obtenemos con los datos nulos (o de referencia),
entonces terminamos con evidencia acumulada en contra de la hipótesis nula.

Nuestra estadística también puede ser numérica. Esta simplificación tiene algunas ventajas. Por ejemplo,
podemos hacer más cálculos y cuantificaciones, pero también tiene desventajas: debemos enfocar la estadística a un aspecto concreto de los datos.

Supongamos entonces que medimos la pendiente a lo largo de la ventana de tamaño 20. En este caso,
podemos construir una distribución para nuestra estadística, *bajo la hipótesis nula*. En este caso,
tomamos todas las posibles ventanas y los valores de pendiente que obtenemos, y graficamos:

```{r}
g_1 <- ggplot(tbl, aes(sample = pendiente)) + geom_qq(distribution = stats::qunif) +
    labs(subtitle = "Referencia (nula)")
g_2 <- ggplot(tbl, aes(x = pendiente)) + geom_histogram() + labs(subtitle = " ") + coord_flip() +
    xlab("")
gridExtra::grid.arrange(g_1, g_2, nrow = 1, widths = c(4, 2))
```

En el primer caso, obtuvimos una pendiente de

```{r}
sd_nula <- obs$pendiente[20]
sd_dif <- obs_dif$pendiente[20]
sd_nula
sd_dif
```

```{r}
g_1 <- ggplot(tbl, aes(sample = pendiente)) + geom_qq(distribution = stats::qunif) +
    labs(subtitle = "Referencia (nula)") + 
    geom_hline(yintercept = c(sd_nula, sd_dif), col = "red")
g_2 <- g_2 <- ggplot(tbl, aes(x = pendiente)) + geom_histogram() + labs(subtitle = " ") + coord_flip() +
    xlab("")
gridExtra::grid.arrange(g_1, g_2, nrow = 1, widths = c(4, 2))
```



Podemos cuantificar qué tan extrema es nuestra observación mediante el cálculo de un *valor p*. 
El cuantil de la observación es

```{r}
dist_acum <- ecdf(tbl$pendiente)
dist_acum(sd_nula)
dist_acum(sd_dif)
```

Si la observación fuera extraída de la población original (es decir, se cumple
la hipótesis nula),  una observación tan extrema o más de lo que observamos tendría probabilidad
de ocurrir de

```{r}
2*(1-dist_acum(sd_nula))
2*(1-dist_acum(sd_dif))
```

Para la La cual es muy baja, y aporta evidencia en contra de la hipótesis nula.


### Ejemplo {-}

En el siguiente ejemplo, tenemos mediciones para tres grupos 
a, b y c. Podemos pensar que son muestras de
tres poblaciones distintas, por ejemplo.

Nos interesa saber qué tan distintos son los datos que produce cada condición - no solamente
qué tan diferentes son las muestras. Hay muchos aspectos
que podríamos cuestionar acerca de cómo son diferentes los tres sistemas. En este caso,
nos preguntamos, por ejemplo, si las distribuciones son similares o no.


**Apophenia**: poner esfuerzos donde vale la pena poner esfuerzos, ser más eficiente.


```{r, fig.width = 6, fig.height = 3, echo = FALSE}
library(tidyverse)
library(ggrepel)
library(nullabor)
library(knitr)
theme_set(theme_minimal(base_size = 14))
paleta <- scale_colour_manual(values = c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7"))
source("R/funciones_auxiliares.R")
```

```{r, echo = FALSE}
set.seed(8)
pob_tab <- tibble(id = 1:2000, x = rgamma(2000, 4, 1), 
    grupo = sample(c("a","b", "c"), 2000, prob = c(4,2,1), replace = T))
muestra_tab <- pob_tab %>% sample_n(125)
g_1 <- ggplot(muestra_tab, aes(x = grupo, y = x)) + geom_boxplot(outlier.alpha = 0) +
    geom_jitter(alpha = 0.3) + 
    coord_flip() +  labs(subtitle = "Muestra")
#g_2 <- ggplot(pob_tab, aes(x= grupo, y = x)) + geom_boxplot(outlier.alpha = 0) +
#    coord_flip() +  labs(subtitle = "Población")
#gridExtra::grid.arrange(g_1, g_2, ncol = 2) 
g_1
```

En la muestra observamos diferencias entre los grupos. Pero notamos adicionalmente que
hay mucha variación dentro de cada grupo. Nos podríamos preguntar entonces si las diferencias
que observamos se debe a que tenemos información incompleta de los grupos

```{r}
muestra_tab %>% group_by(grupo) %>% 
    summarise(mediana = median(x) %>% round(2), n = n())
```

En la muestra, la mediana del grupo c es menor que la de b, por ejemplo. 
Las dispersiones parecen ser también ditintas. Las muestras son relativamente chicas, especialmente
para el grupo b y c.

Podemos construir ahora una *hipótesis nula*, que establece que las observaciones
provienen de una población similar.

- Los tres procesos son prácticamente indistiguibles desde el punto de vista del sistema. La variación
que observamos se debe a que tenemos información incompleta: si observáramos todos los posibles
datos que cada proceso genera, veríamos que sus distribuciones son muy similares.


## Permutaciones y el lineup

Para atacar este problema podemos pensar de la siguiente forma: si los grupos producen
datos similares, entonces los grupos a, b, c solo son etiquetas que no contienen información.
Podríamos entonces **permutar** las etiquetas y observar qué pasa. La muestra con las etiquetas
permutadas es igualmente verosímil que la que obtuvimos, bajo la hipótesis nula.

Si repetimos el proceso de permutación muchas veces, podemos comparar las diferencias que observamos
en la muestras. Si las diferencias en las muestras no tienen ninguna característica sistemática
que las distinga de las otras muestras obtenidas por permutaciones, entonces tenemos poca
evidencia de que las diferencias que observamos en nuestra muestra sean sistemáticas.

Vamos a intentar esto, por ejemplo usando una gráfica de cuantiles. Hacemos un *lineup*, o una
*rueda de sospechosos*, donde 11 de los acusados son generados mediante permutaciones al azar,
y el culpable (los verdaderos datos) están en una posición escogida al azar.

```{r}
set.seed(88)
reps <- lineup(null_permute("grupo"), muestra_tab, n = 20)
grafica_cuantiles(reps %>%  mutate(grupo_1 = factor(digest::digest2int(grupo) %% 177)), 
                             grupo_1, x) + facet_wrap(~.sample, ncol = 5) + ylab("x")
```

Y la pregunta que hacemos es **podemos distinguir nuestra muestra entre todas las 
replicaciones producidas con permutaciones**? Nota: para evitar en parte hacer trampa, reetiquetamos
las letras, y usamos otra gráfica diferente.

En este ejemplo, es difícil indicar cuáles son los datos. Los grupos tienen distribuciones
similares y es factible que las diferencias que observamos se deban a variación muestral.  

Consideremos algunos cálculos: si los verdaderos datos son consistentes con nuestra hipótesis
nula, entonces la probabilidad de escoger al verdadero culpable es de 1/12 = 0.08. Esta se
llama *significancia de nuestra prueba de lineup*.

- Si la persona escoge los verdaderos datos, rechazamos la hipótesis nula (equivalencia entre los
tres bonches de datos), y decimos que los datos son *significativamente diferentes* al nivel 0.05.

- Si la persona escoge uno de los datos permutados, no rechazamos la hipótesis nula. Esto usualmente
implica que es razonable proceder en nuestro análisis de estos datos bajo la hipótesis de trabajo
de que no hay diferencia entre los grupos.

Por ejemplo, como los datos son consistentes con la hipótesis de que todos los grupos provienen de la
mismo proceso, podemos resumir haciendo *pooling* o agregación de todos los grupos. Sobre todos los datos,
los percentiles son

```{r}
muestra_tab %>% pull(x) %>% quantile(probs = c(0.1, 0.25, 0.5, 0.75, 0.9))
```

Comparando con los datos poblacionales:

```{r}
pob_tab %>% pull(x) %>%  quantile(probs = c(0.1, 0.25, 0.5, 0.75, 0.9))
```

Como el pooling es razonable, obtenemos mejores estimaciones utilizando la muestra completa,
agregada sobre los tres procesos a, b, c.

Este tipo de análisis es especialmente útil cuando trabajamos con muestras relativamente
chicas, donde mayor varianza implica que muchas veces nos podemos distraer en sobre estimaciones
de diferencias. Si vimos los datos, el lineup no es tan útil, pero siempre podemos probar
con más personas (en estos casos es posible incluso calcular un valor p).

## Cuando la estadística es numérica {-}

Ahora suupongamos que hacemos una pregunta mucho más específica, que se refiere a un número particular:
¿la media del grupo b puede ser considerablemente es diferente de la del grupo c? En lugar de usar 
gráfica, podemos calcular la estadística de interés para cada grupo, y tomar la diferencia. Comparamos
el valor observado con los resultados de las muestras obtenidas por permutación:

```{r, fig.width = 5, fig.height = 2.5}
set.seed(118)
reps <- lineup(null_permute("grupo"), muestra_tab, n = 10000)
dif_obs <- muestra_tab %>% group_by(grupo) %>% 
    summarise(mediana  = median(x)) %>% 
    spread(grupo, mediana) %>% 
    mutate(diferencia = b - c) %>% pull(diferencia)

reps_media <- reps %>% 
    group_by(grupo, .sample) %>% 
    summarise(mediana  = median(x)) %>% 
    spread(grupo, mediana) %>% 
    mutate(diferencia = b - c)
dist_acumulada_perm <- ecdf(reps_media$diferencia)
percentil_obs <- dist_acumulada_perm(dif_obs) %>% round(2)
g_1 <- ggplot(reps_media, aes(sample = diferencia)) + geom_qq(distribution = stats::qunif) +
    geom_hline(yintercept = dif_obs, colour = "red") +
    annotate("text", x = 0.25, y = dif_obs + 0.1, label = "diferencia observada", colour = "red")
g_2 <- ggplot(reps_media, aes(x = diferencia)) + geom_histogram(binwidth = 0.1 ) +
    geom_vline(xintercept = dif_obs, colour = "red") +
    annotate("text", x = dif_obs, y = 300, label = percentil_obs, hjust = -0.2, colour = "red")
gridExtra::grid.arrange(g_1, g_2, ncol = 2) 
```


Y notamos que el resultado obtenido en nuestra muestra no es excepcionalmente grande. Otra vez, no
rechazamos la hipótesis nula.

Nótese que calculamos una cantidad adicional, que es el percentil donde nuestra observación cae
en la distribución generada por las permutación. Esta cantidad puede usarse para calcular un 
valor-p. Podemos calcular, por ejemplo:

- Valor p de una cola: Si la hipótesis nula es cierta, 
¿cuál es la probabilidad de haber observado un valor tan grande o más que el que observamos? La 
respuesta es `r 1- percentil_obs`, es decir, no muy baja. Es relativamente común observar un valor
de tal magnitud bajo la hipótesis nula.

- Valor p de dos colas: Si la hipótesis nula es cierta, ¿cuál es la
probabilidad de observar una diferencia en valor absoluto tan o más extremo de lo que observamos? Podemos calcular
la respuesta como `r (1 - dist_acumulada_perm(dif_obs)) + dist_acumulada_perm(-dif_obs) ` 


Repitimos el ejemplo con un cambio

```{r}
set.seed(72)
muestra_tab <- pob_tab %>% sample_n(90) %>% 
    mutate(x = ifelse(grupo == "b", 1.5 * x + 1, x))
g_1 <- ggplot(muestra_tab, aes(x = grupo, y = x)) + geom_boxplot(outlier.alpha = 0) +
    geom_jitter(alpha = 0.3) + 
    coord_flip() + ylim(c(0, 20)) + labs(subtitle = "Muestra")
g_2 <- ggplot(pob_tab, aes(x= grupo, y = x)) + geom_boxplot(outlier.alpha = 0) +
    coord_flip() + ylim(c(0, 20)) + labs(subtitle = "Población")
g_1
```

Por ejemplo, obtenemos para la muestra:

```{r}
muestra_tab %>% group_by(grupo) %>% summarise(mediana = median(x))
```

```{r}
set.seed(9012)
reps <- lineup(null_permute("grupo"), muestra_tab, n = 12)
grafica_cuantiles(reps %>%  mutate(grupo_escondido = factor(digest::digest2int(grupo) %% 177)), 
                             grupo_escondido, x) + facet_wrap(~.sample) + ylab("x") +
    coord_flip() 
```


Podemos distinguir más o menos claramente que está localizada en valores
más altos y tiene mayor dispersión. Por ejemplo, podríamos considerar una
prueba para ver qué tan excepcional es la diferencia entre c y b:

```{r, fig.width = 5, fig.height = 2.5}
set.seed(118)
reps <- lineup(null_permute("grupo"), muestra_tab, n = 3000)
dif_obs <- muestra_tab %>% group_by(grupo) %>% 
    summarise(mediana  = median(x)) %>% 
    spread(grupo, mediana) %>% 
    mutate(diferencia = b - c) %>% pull(diferencia)
reps_media <- reps %>% 
    group_by(grupo, .sample) %>% 
    summarise(mediana  = median(x)) %>% 
    spread(grupo, mediana) %>% 
    mutate(diferencia = b - c)
g_1 <- ggplot(reps_media, aes(sample = diferencia)) + geom_qq(distribution = stats::qunif) +
    geom_hline(yintercept = dif_obs, colour = "red") +
    annotate("text", x = 0.4, y = dif_obs + 0.2, label = "diferencia observada", colour = "red")
g_2 <- ggplot(reps_media, aes(x = diferencia)) + geom_histogram(binwidth = 0.1 ) +
    geom_vline(xintercept = dif_obs, colour = "red")
gridExtra::grid.arrange(g_1, g_2, ncol = 2) 
```

La diferencia observada es extrema, y es muy poco probable que hayamos observado tal diferencia
por variación muestra. Tenemos evidencia de que la mediana del grupo "c" es más alta en la población que
"b".



## Ejemplo: tiempos de fusión

Consideremos el ejemplo de fusión de estereogramas que vimos anteriormente. Una pregunta 
que podríamos hacer es: considerando que hay mucha variación en el tiempo de fusión dentro
de cada tratamiento, necesitamos calificar la evidencia de nuestra conclusión (el tiempo de fusión se reduce al 66\% aproximadamente).



```{r, fig.width = 6, fig.height = 5}
set.seed(113)
fusion <- read_delim("./datos/fusion_time.txt", delim = " ", trim_ws = TRUE)
reps <- lineup(null_permute("nv.vv"), fusion, 20)
ggplot(reps, aes(sample = time, colour = nv.vv)) +
    #geom_boxplot(colour = "black") +
    geom_qq(distribution = stats::qunif, size = 0.5) +
    #geom_jitter(width = 0.1, height = 0, alpha = 0.5, size = 1) + 
    facet_wrap(~.sample) + scale_y_log10()
```

Otro enfoque sería probar una característica específica de las muestras. En
este ejemplo, podríamos tomar, por ejemplo, el promedio de los dos cuantiles
(superior e inferior). 

```{r}
stat_fusion <- function(x){
    (quantile(x, 0.75) + quantile(x, 0.25))/2
}
reps <- lineup(null_permute("nv.vv"), fusion, 10000)
dif <- fusion %>% group_by(nv.vv) %>% 
    summarise(mediana = stat_fusion(time)) %>% 
    spread(nv.vv, mediana) %>% mutate(dif = VV / NV ) %>% pull(dif)
cocientes_perm <- reps  %>%  group_by(.sample, nv.vv) %>% 
    summarise(mediana = stat_fusion(time)) %>% 
    spread(nv.vv, mediana) %>% 
    summarise(cociente = (VV / NV))
ggplot(cocientes_perm, aes(x = cociente)) + 
    geom_histogram(binwidth = 0.05) +
    geom_vline(xintercept = dif)
2*ecdf(cocientes_perm$cociente)(dif)
```

Lo que muestra evidencia considerable, aunque no muy fuerte, de que la instrucción verbal ayuda a reducir el tiempo de fusión de los estereogramas.



## Ejemplo (dimensión alta)
Referencia de Diane Cook

En el siguiente ejemplo, tenemos 4 grupos de avispas (50 individuos en total),
y para cada individuo se miden expresiones
de 42 genes distintos. La pregunta es: ¿Podemos separar a los grupos de avispas 
dependiendo de sus mediciones. 

En este caso podemos usar análisis discriminante, que busca proyecciones de los
datos en dimensión baja de forma que los grupos sean lo más compactos y separados posibles.

Para probar qué tan bien funciona este método, podemos hacer una prueba de permutación, aplicamos
LDA y observamos los resultados:

```{r}
data(wasps)

wasp.lda <- MASS::lda(Group~., data=wasps[,-1])
wasp.ld <- predict(wasp.lda, dimen=2)$x
true <- data.frame(wasp.ld, Group=wasps$Group)

wasp.sim <- data.frame(LD1=NULL, LD2=NULL, Group=NULL, .n=NULL)
for (i in 1:19) {
  x <- wasps
  x$Group <- sample(x$Group)
  x.lda <- MASS::lda(Group~., data=x[,-1])
  x.ld <- predict(x.lda, dimen=2)$x
  sim <- data.frame(x.ld, Group=x$Group, .n=i)
  wasp.sim <- rbind(wasp.sim, sim)
}
pos <- sample(1:20, 1)
d <- lineup(true=true, samples=wasp.sim, pos=pos)
ggplot(d, aes(x=LD1, y=LD2, colour=Group)) + 
  facet_wrap(~.sample, ncol=5) +
  geom_point() + theme(aspect.ratio=1)
```

Y vemos que incluso permutando los grupos, es generalmente posible separarlos en grupos
bien definidos: Existen combinaciones lineales que los separan. Que no podamos distinguir
los datos verdaderos de las replicaciones nulas indica que este método difícilmente puede
servir para separar los grupos claramente.


Otro enfoque sería separar los datos:

```{r}
set.seed(8)
wasps_1 <- wasps %>% mutate(u = runif(nrow(wasps), 0, 1))
wasps_entrena <- wasps_1 %>% filter(u <= 0.8)
wasps_prueba <- wasps_1 %>% filter(u > 0.8)                            
                            
wasp.lda <- MASS::lda(Group ~ ., data=wasps_entrena[,-1])
wasp_ld_entrena <- predict(wasp.lda,  dimen=2)$x %>% 
    as_tibble(.name_repair = "universal") %>%
     mutate(tipo = "entrenamiento") %>% 
    mutate(grupo = wasps_entrena$Group)
wasp_ld_prueba <- predict(wasp.lda, newdata = wasps_prueba, dimen=2)$x  %>% 
    as_tibble(.name_repair = "universal") %>%
    mutate(tipo = "prueba")%>% 
    mutate(grupo = wasps_prueba$Group)
wasp_lda <- bind_rows(wasp_ld_entrena, wasp_ld_prueba)
ggplot(wasp_lda, aes(x = LD1, y = LD2, colour = grupo)) + geom_point(size = 3) +
    facet_wrap(~tipo) + scale_color_colorblind()
```



## El jardín de los senderos que se bifurcan

Recientemente (aunque muchos estadísticos lo han repetido desde hace mucho tiempo), se ha reconocido
en campos como la sicología la "crisis de replicabilidad" (ver por ejemplo @falsefindings). Varios estudios que recibieron mucha publicidad inicialmente no han podido ser replicados
posteriormente por otros investigadores. Por ejemplo:

- Hacer [poses poderosas](https://www.sciencedaily.com/releases/2017/09/170911095932.htm) produce cambios fisiológicos que mejoran nuestro desempeño en ciertas tareas
- Patrones de ovulación de las mujeres influyen en los candidatos que escogen para votar
- Mostrar palabras relacionadas con "viejo" hacen que las personas caminen más lento (efectos de [priming](https://www.nature.com/news/nobel-laureate-challenges-psychologists-to-clean-up-their-act-1.11535)) 
En todos estos casos, la evidencia de estos efectos fue respaldada finalmente (pues es requisito para
publicación) por una prueba de hipótesis nula con un valor p menor a 0.05. Este estándar de publicación
es seguido por varias áreas y revistas. Este problema de replicabilidad parece ser más frecuente cuando:

1. Se trata de estudios de potencia baja: mediciones ruidosas y  tamaños de muestra chicos.
2. El plan de análisis no está claramente definido desde un principio (lo cual es difícil cuando
se están investigando "fenómenos no estudiados antes")

Aunque haya varios ejemplos de manipulaciones conscientes para obtener resultados publicables
([p-hacking](https://en.wikipedia.org/wiki/Data_dredging)), 
 como vimos en ejemplos anteriores, hay varias decisiones, todas razonables, que podemos tomar cuando 
estamos buscando las comparaciones correctas. Algunas pueden ser:

- Transformar los datos (tomar o no logaritmos, u otra transformación)
- Editar datos atípicos (razonable si los equipos pueden fallar, o hay errores de captura, por ejemplo)
- Distintas maneras de interpretar los criterios de inclusión (por ejemplo, el estudio se planeó
para personas entre 20 y 30 años, y tenemos tres casos con 33 años. ¿Los dejamos o los usamos?)

Dado un juego de datos, las justificaciones de las decisiones que se toman en cada paso se justifican
y son razonables, pero con datos distintos las decisiones pueden ser diferentes. 
Este es el jardín de los senderos que se bifurcan de [Gelman](http://www.stat.columbia.edu/~gelman/research/published/incrementalism_3.pdf), 
que invalida en parte el uso valores p como criterio de evidencia contra la hipótesis nula.
Ver por ejemplo el [comunicado de la ASA](https://www.amstat.org/asa/files/pdfs/P-ValueStatement.pdf).

## Ejemplo:
En el ejemplo de datos de fusión, decidimos probar, por ejemplo, el promedio de
los cuartiles inferior y superior, lo cual no es una decisión típica pero usamos como
ilustración.


```{r}
set.seed(8812)
media_cuartiles <- function(x){
    (quantile(x, 0.75) + quantile(x, 0.25))/2
}

valor_p_fusion <- function(fusion, stat_fusion = stat_fusion, trans = identity, comp = VV / NV){
    pos <- 1
    reps <- lineup(null_permute("nv.vv"), pos =1, fusion, n = 10000)
    dif <- fusion %>% group_by(nv.vv) %>% 
        summarise(valor_est = {{ stat_fusion }}(time)) %>% 
        spread(nv.vv, valor_est) %>% mutate(dif = {{ comp }} ) %>% pull(dif)
    cocientes_perm <- reps  %>%  group_by(.sample, nv.vv) %>% 
        summarise(valor_est = {{ stat_fusion }} (time)) %>% 
        spread(nv.vv, valor_est) %>% 
        summarise(cociente = ({{ comp }}))
    2*ecdf(cocientes_perm$cociente)(dif)
}
valor_p_fusion(fusion %>% sample_frac(0.8), stat_fusion = mean, trans = identity, comp = VV - NV)
valor_p_fusion(fusion %>% sample_frac(0.8), stat_fusion = median, trans = identity, comp = VV / NV)
valor_p_fusion(fusion %>% sample_frac(0.8), stat_fusion = media_cuartiles, trans = identity, 
               comp = VV / NV)
valor_p_fusion(fusion %>% sample_frac(0.8), stat_fusion = median, trans = log, comp = VV - NV)
valor_p_fusion(fusion %>% sample_frac(0.8), stat_fusion = mean, trans = log, comp = VV - NV)

```

Si existen grados de libertad - muchas veces necesarios para hacer un análisis exitoso-, entonces
los valores p pueden tener poco significado.


En áreas como la sicología, existen ahora movimientos fuertes en favor de la replicación. Este movimiento
sugiere dar valor a los estudios exploratorios que no reportan valor p, y posteriormente, si el estudio
es de interés, puede intentarse una replicación confirmatoria, con potencia más alta y con planes de análisis predefinidos.


