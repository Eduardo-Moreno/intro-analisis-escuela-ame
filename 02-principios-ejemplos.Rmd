# Principios y ejemplos

## Algunos conceptos básicos {-}

En el análisis los datos típicamente están organizados en *bonches de números etiquetados* que corresponden
a unidades con múltiples estructuras jerárquicas, a veces más complejas y a veces menos.

Por ejemplo, los siguientes datos fueron registrados en un restaurante durante cuatro días consecutivos:

### Ejemplo {-}

```{r}
library(tidyverse)
library(ggrepel)
library(knitr)
source("./R/funciones_auxiliares.R")
propinas <- reshape2::tips %>% 
  rename(cuenta_total = total_bill, propina = tip, sexo = sex, fumador = smoker,
         dia = day, momento = time, num_personas = size) %>% 
  mutate(sexo = recode(sexo, Female = "Mujer", Male = "Hombre"), 
         fumador = recode(fumador, No = "No", Si = "Si"),
         dia = recode(dia, Sun = "Dom", Sat = "Sab", Thur = "Jue", Fri = "Vie"),
         momento = recode(momento, Dinner = "Cena", Lunch = "Comida")) %>% 
  select(-sexo) %>% 
  mutate(dia  = fct_relevel(dia, c("Jue", "Vie", "Sab", "Dom")))
sample_n(propinas, 10) %>% formatear_tabla()
```

Aquí la unidad de observación es una cuenta particular. Tenemos tres mediciones numéricas
de cada cuenta: cúanto fue la cuenta total, la propina, y el número de personas asociadas a la cuenta.
Los datos están separados según se fumó o no en la mesa, y temporalmente en dos partes: el día
(Jueves, Viernes, Sábado o Domingo), cada uno separado por Cena y Comida. 

Lo primero que nos interesa es comparar los datos dentro de cada "bonche":

- ¿Varían mucho o poco? ¿Cuáles son valores típicos o centrales? ¿Existen valores atípicos?
- ¿Cómo es la variación conjunta de varias mediciones en un bonche? 
- ¿Cómo están asociadas las distintas agrupaciones de los datos? ¿Qué agrupaciones están más asociadas?

Supongamos entonces que consideramos simplemente la variable de *cuenta_total*. Podemos comenzar
por **ordenar los datos**, y ver cuáles datos están en los extremos y cuáles están en los lugares centrales:

```{r}
propinas <- propinas %>% mutate(
    orden_cuenta = rank(cuenta_total, ties.method = "first"), 
    f = (orden_cuenta - 0.5) / n()) 
cuenta <- propinas %>% select(orden_cuenta, f, cuenta_total) %>% arrange(f)
bind_rows(head(cuenta), tail(cuenta)) %>% formatear_tabla()
```

*Observación*: acerca del valor $f$ y restar 0.5

y graficamos los datos en orden, interpolando valores consecutivos.

```{r, fig.width = 7, fig.height = 4}
g_orden <- ggplot(cuenta, aes(x = orden_cuenta, y = cuenta_total)) + geom_point(colour = "red", alpha = 0.5) + 
  labs(subtitle = "Cuenta total") 
g_cuantiles <- ggplot(cuenta, aes(x = f, y = cuenta_total)) + geom_point(colour = "red", alpha = 0.5) + geom_line() +
  labs(subtitle = "") +
  scale_x_continuous(breaks = seq(0, 1, 0.1))
gridExtra::grid.arrange(g_orden, g_cuantiles, ncol = 2)
```

A esta función le llamamos la **función de cuantiles** para la variable cuenta total. Y nos
sirve para comparar directamente los distintos valores que observamos los datos
según el orden que ocupan. 

Estas gráfica no pierde información, su forma es informativa.

**Dispersión y Valores centrales**

- El *rango* de datos va de unos 3 dólares hasta 50 dólares
- Los **valores centrales** (del cuantil 0.25 al 0.75, por ejemplo), están
entre unos 13 y 25 dólares
- Podemos usar el cuantil 0.5 (**mediana**) para dar un valor *central* de esta distribución,
que está alrededor de 18 dólares.

Y podemos dar resúmenes más refinados si es necesario

- El cuantil 0.95 es de unos 35 dólares - sólo 5\% de las cuentas son de más de 35 dólares
- El cuantil 0.05 es de unos 8 dólares - sólo 5\% de las cuentas son de 8 dólares o menos.

Finalmente, la forma de la gráfica la interpretamos usando su pendiente, haciendo comparaciones
de diferentes partes de la gráfica:

- La distribución de valores tiene asimetría: el 10\% de las cuentas más altas 
tiene considerablemente más dispersión que el 10\% de las cuentas más bajas. 

- Entre los cuantiles 0.2 y 0.5 es donde existe *mayor* densidad de datos: la pendiente
es baja, lo que significa que al avanzar en los cuantiles, los valores observados no cambian mucho.

- Cuando la pendiente es alta, quiere decir que los datos tienen más dispersión local o están más separados.

En algunos casos, es más natural hacer un *histograma*, donde dividimos el rango de la variable
en cubetas o intervalos (en este caso de igual longitud), y graficamos cuántos datos caen en cada
cubeta:

```{r, fig.width = 10, fig.height = 4}
variable <- quo(cuenta_total)
binwidth_min = 1
g_1 <- ggplot(propinas, aes(x = !!variable)) + geom_histogram(binwidth = binwidth_min) 
g_2 <- ggplot(propinas, aes(x = !!variable)) + geom_histogram(binwidth = binwidth_min * 2) 
g_3 <- ggplot(propinas, aes(x = !!variable)) + geom_histogram(binwidth = binwidth_min * 5) 
gridExtra::grid.arrange(g_1, g_2, g_3, ncol = 3)
```

Finalmente, una gráfica más compacta que resume la gráfica de cuantiles o el histograma
es el diagrama de caja y brazos. Mostramos dos versiones, la clásica de Tukey (T) y otra versión
menos común de Spear/Tufte (ST):

```{r, fig.width = 8, fig.height = 4}
library(ggthemes)
cuartiles <- quantile(cuenta$cuenta_total)
g_1 <- ggplot(cuenta, aes(x = f, y = cuenta_total)) + 
  labs(subtitle = "Gráfica de cuantiles: Cuenta total") +
  geom_hline(yintercept = cuartiles[2], colour = "gray") + 
  geom_hline(yintercept = cuartiles[3], colour = "gray") +
  geom_hline(yintercept = cuartiles[4], colour = "gray") +
  geom_point(alpha = 0.5) + geom_line() 
g_2 <- ggplot(cuenta, aes(x = factor("ST", levels =c("ST")), y = cuenta_total)) + 
  geom_tufteboxplot() +
  #geom_violin(draw_quantiles = c(0.25, 0.5, 0.75)) +
  labs(subtitle = " ") +  xlab("") + ylab("")
g_3 <- ggplot(cuenta, aes(x = factor("T"), y = cuenta_total)) + geom_boxplot() +
  labs(subtitle = " ") +  xlab("") + ylab("")
g_4 <- ggplot(cuenta, aes(x = factor("P"), y = cuenta_total)) + geom_jitter(height = 0, width =0.2, alpha = 0.5) +
  labs(subtitle = " ") +  xlab("") + ylab("")
gridExtra::grid.arrange(g_1, g_2, g_3, g_4, nrow = 1, widths = c(8, 2, 2, 2)) 
```


- **Ventajas en el análisis inicial**: en un principio del análisis, estos resúmenes
(cuantiles) pueden ser más útiles que utilizar medias y varianzas, por ejemplo. La razón es
que los cuantiles:

- Son cantidades más fácilmente interpretables
- Los cuantiles centrales son más resistentes a valores atípicos que medias o varianzas
- Sin embargo, permite identificar valores extremos




### Ejercicio
¿Cómo se ve la gráfica de cuantiles de las propinas? ¿Cómo crees que esta gráfica se
compara con distintos histogramas?

```{r, fig.width = 4, fig.height = 3}
g_1 <- ggplot(propinas, aes(sample = propina)) + 
  geom_qq(distribution = stats::qunif) + xlab("f") + ylab("propina")
g_1
```


## Casas{-}

```{r}
zonas <- nombres[table(casas$nombre_zona) > 30]
casas_x <- casas %>% mutate(nombre_zona = fct_reorder(nombre_zona, precio_miles)) %>% 
  filter(nombre_zona %in% zonas)
ggplot(casas_x, aes(x = nombre_zona, y = precio_miles)) + geom_boxplot() + coord_flip()
```

```{r}
casas_x <- casas %>% mutate(nombre_zona = fct_reorder(nombre_zona, precio_m2_miles)) %>% 
  filter(nombre_zona %in% zonas)
ggplot(casas_x, aes(x = nombre_zona, y = precio_m2_miles)) + geom_boxplot() + coord_flip()
```

Podemos cuantificar la variación que observamos de zona a zona y la variación que hay dentro de zonas,
por ejemplo, tomando

```{r}
quantile(casas_x %>% group_by(nombre_zona) %>% summarise(mediana = median(precio_m2_miles)) %>% pull(mediana)) 
```

```{r}
quantile(casas_x %>% group_by(nombre_zona) %>% mutate(residual = precio_m2_miles - median(precio_m2_miles)) %>% pull(residual))
```

Esta última cantidad tiene más sentido interpretar debido a que las dispersiones dentro
de cada grupo son similares.


```{r}
ggplot(casas_x %>% mutate(ind_calidad = cut(calidad_gral, c(0, 5, 8,10))), aes(y = precio_m2_miles, x = nombre_zona, 
        colour=factor(ind_calidad))) + 
  geom_hline(yintercept = c(1, 2), colour = "gray") +
  geom_boxplot(outlier.shape = NA) +
  coord_flip() + scale_colour_tableau() +
  facet_wrap(~ind_calidadvb gtttttf)

```

Con esta descomposición,


## Enlace {-}

Consideremos la prueba Enlace (2011) de matemáticas para primarias. Una primera pregunta 
que alguien podría hacerse es:  ¿qué escuelas son mejores, las privadas o las públicas? 

```{r, message = FALSE, echo = FALSE, include=FALSE, eval = FALSE}
col_spec <- cols_only(X2 = col_character(), X3 = col_character(),
                      X6 = col_character(), X24 = col_double(),
                      X82 = col_integer(),
                      X84 = col_character())
enlace_1 <- read_csv("./datos/enlace/escuelas_enlace_nacional_primaria_1.csv", skip=3, 
                     col_names = FALSE, guess_max = 100000, col_type = col_spec,
                     locale = locale(encoding = "ISO-8859-1"))
enlace_2 <- read_csv("./datos/enlace/escuelas_enlace_nacional_primaria_2.csv", skip=3, 
                     col_names = FALSE, guess_max = 100000, col_type = col_spec,
                     locale = locale(encoding = "ISO-8859-1"))
enlace <- bind_rows(enlace_1, enlace_2)
names(enlace) <- c("estado", "clave", "tipo", "mate_6",  "num_evaluados_total", "marginacion")
table(enlace$tipo)
enlace <- enlace %>% mutate(tipo = ifelse(tipo == "INDêGENA", "INDÍGENA", tipo))
write_csv(enlace, "datos/enlace.csv")
```

Con estos datos a la mano, podemos hacer unos primeros resúmenes de los datos. El rango
de la calificación de matemáticas para un alumno es de 0-800, y aproximadamente
la mitad de los alumnos califica en el rengo de 450 a 550. Vemos dispersión
considerable en las calificaciones de las escuelas, y diferencias considerables 
entre tipo de escuelas:

```{r, message = FALSE, echo = FALSE}
enlace <- read_csv("datos/enlace.csv")
enlace <- enlace %>%  filter(num_evaluados_total > 0, mate_6 > 0) %>% 
    mutate(tipo = fct_reorder(tipo, mate_6, mean)) %>% 
    mutate(marginacion = fct_reorder(marginacion, mate_6, median))
```

```{r, message = FALSE}
enlace_tbl <- enlace %>% group_by(tipo) %>% 
    summarise(n_escuelas = n(),
              cuantiles = list(cuantil(mate_6, c(0.05, 0.25, 0.5, 0.75, 0.95)))) %>% 
    unnest %>% mutate(valor = round(valor)) 
enlace_tbl %>% spread(cuantil, valor) %>% formatear_tabla()
```

Podemos graficar de varias maneras, por ejemplo:


```{r, fig.width = 10, fig.height = 6, echo = FALSE}
g_medianas <- ggplot(enlace_tbl %>% filter(cuantil == 0.50), aes(x = tipo, y = valor)) +
    geom_point(colour = "red") + ylim(c(150,880)) + labs(subtitle = "Gráfica 1")
g_80 <- ggplot(enlace_tbl  %>% spread(cuantil,valor), 
                                     aes(x = tipo, y = `0.5`)) +
    geom_linerange(aes(ymin= `0.05`, ymax = `0.95`), colour = "gray40") +
    geom_point(colour = "red", size = 3) + ylim(c(150,880)) + 
    labs(subtitle = "Gráfica 1") +
    ylab("Promedios Matemáticas")
    
g_80_p <- ggplot(enlace_tbl  %>% spread(cuantil,valor), aes(x = tipo, y = `0.5`)) +
    geom_linerange(aes(ymin= `0.05`, ymax = `0.95`), colour = "gray40") +
    geom_linerange(aes(ymin= `0.25`, ymax = `0.75`), size = 2, colour = "white") +
    geom_point(colour = "red", size = 3) +
     ylim(c(150,880)) + labs(subtitle = "Gráfica 2")+
    ylab("Promedios Matemáticas")

g_boxplot <- ggplot(enlace  , aes(x = tipo, y = mate_6)) + 
    geom_boxplot(outlier.size = 0.7) +
    labs(subtitle = "Gráfica 3")+ ylim(c(150,930))+
    ylab("Promedios Matemáticas")

g_cuantil <- ggplot(enlace, aes(sample = mate_6, colour = tipo)) +
  geom_qq(distribution = stats::qunif, size = 1) + ylab("Promedios Matemáticas") +
  xlab("orden")

gridExtra::grid.arrange(g_80, g_80_p, g_boxplot, g_cuantil, nrow = 2)
```

En términos de comparaciones, podemos discutir qué tan apropiada es cada gráfica. Graficar más
cuantiles es más útil para hacer comparaciones. Por ejemplo, en la Gráfica 2 podemos ver que la
mediana de las escuelas generales está cercano al cuantil 5\% de las escuelas particulares. Por otro
lado, el diagrama de caja y brazos muestra también valores "atípicos". 

La diferencia es considerable entre tipos de escuela, y el principio 1 funciona bien con 
estas gráficas. Sin embargo, sabemos que podemos mejorar en las principios 2 y 3. Podemos comenzar
por agregar, por ejemplo, el nivel del marginación del municipio donde se encuentra la escuela.

```{r}
enlace_tbl_marg <- enlace %>% 
    group_by(tipo, marginacion) %>% 
    summarise(n_alumnos = sum(num_evaluados_total),
              cuantiles = list(cuantil(mate_6, c(0.05, 0.25, 0.5, 0.75, 0.95)))) %>% 
    unnest %>% mutate(valor = round(valor)) %>% 
    filter( n_alumnos > 20)
```

```{r, fig.width = 10, fig.height = 4, echo = FALSE}
g_80_p <- ggplot(enlace_tbl_marg  %>% spread(cuantil, valor), 
                                     aes(x = marginacion, y = `0.5`)) +
    geom_linerange(aes(ymin= `0.05`, ymax = `0.95`), colour = "gray40") +
    geom_linerange(aes(ymin= `0.25`, ymax = `0.75`), size = 2, colour = "white") +
    geom_point(colour = "red", aes(size = log10(n_alumnos/1000))) +
    ylab("Promedios Matemáticas") + facet_wrap(~tipo, nrow = 1) +
    theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
    scale_size_continuous(name = "Miles de \nAlumnos",
                          breaks = c(-0.3, 0, 1, 2, 2.7),
                          labels = c(0.5, 1, 10, 100, 500)) 
g_80_p
```

```{r, echo = FALSE}
enlace_tbl <- 
  enlace %>% mutate(tipo = ifelse(tipo %in% c("GENERAL", "PARTICULAR"), as.character(tipo), "INDIGENA/CNFE")) %>% 
  mutate(tipo = fct_reorder(tipo, mate_6))
g_1 <- ggplot(enlace_tbl, 
       aes(x = marginacion, y = mate_6, colour = tipo)) +
  geom_boxplot(outlier.alpha = 0) + scale_color_colorblind() +
  theme(legend.position = "none") + ylim(c(200,800))
g_2 <- ggplot(enlace_tbl, 
       aes(x = "Total", y = mate_6, colour = tipo)) +
  geom_boxplot(outlier.alpha = 0) + scale_color_colorblind() + ylab("") + xlab("") +
   ylim(c(200,800))
gridExtra::grid.arrange(g_1, g_2, ncol =2, widths = c(9, 7.5))
```


Esta gráfica nos ayuda a mejorar nuestra pregunta inicial:

- Las escuelas en municipios de marginación más baja tienden a tener mejores resultados. 
- Escuelas particulares en municipios de marginación alta y media tienen resultados
comparables con escuelas generales en municipios de marginación baja. 
- Esto *indica* que hay factores asociados al desempeño que no tienen que ver con
el tipo de "sistema", sino del entorno de las escuelas.
- Por ejemplo los estudiantes que acuden a escuelas particulares 
probablemente provienen de familias que tienen
más recursos mejores niveles de bienestar (pueden pagar la escuela). 
¿Qué otras variables nos ayudaría a completar este análisis?
- Produjimos preguntas más difíciles, pero más relevantes para lo que queremos entender.

## Estados y calificaciones en SAT {-}

¿Cómo se relaciona el gasto por alumno, a nivel estatal, 
con sus resultados académicos? Hay trabajo
considerable en definir estos términos, pero supongamos que tenemos el
[siguiente conjunto de datos](http://jse.amstat.org/datasets/sat.txt) (@Guber2011), que son
datos oficiales agregados por estado de Estados Unidos. Tenemos las variables
*sat*, por ejemplo, que es la calificación promedio de los alumnos en cada estado
(para 1997), y la variable *expend*, que es el gasto en miles de dólares
por estudiante en (1994-1995), además de algunas otras variables. 


```{r, message = FALSE}
sat <- read_csv("datos/sat.csv")
sat_tbl <- sat %>% select(state, expend, sat) %>% 
    gather(variable, valor, expend:sat) %>% 
    group_by(variable) %>% 
    summarise(cuantiles = list(cuantil(valor))) %>% unnest %>% 
    spread(cuantil, valor)
sat_tbl %>% formatear_tabla
```


Esta variación considerable es considerable para promedios del SAT: 
el percentil 75 es alrededor de 1050 puntos, mientras que el percentil 33 corresponde a alrededor de 800, 
e igualmente hay diferencias considerables de gasto por alumno (miles de dólares) a lo largo 
de los estados.

Ahora hacemos nuestro primer ejericico de comparación: ¿Cómo se ven las
calificaciones para estados en distintos niveles de gasto? Podemos
usar una gráfica de dispersión:


```{r}
 ggplot(sat, aes(x = expend, y = sat, label = state)) + 
  geom_point(colour = "red", size = 2) + geom_text_repel(colour = "gray50") +
  xlab("Gasto por alumno (miles de dólares)") +
  ylab("Calificación promedio en SAT")
```

Estas comparaciones no son de muy alta calidad, solo estamos usando 2 variables (pocas),
y no hay mucho que podamos decir en cuanto explicación.

**Las unidades que estamos comparando pueden diferir fuertemente en otras 
dimensiones importantes, lo cual hace interpretar la gráfica muy difícil**

Sabemos que es posible que el IQ difiera en los estados, pero no como producir
diferencias de este tipo. Sin embargo, descubrimos que existe una variable adicional, 
que es el porcentaje de alumnos de cada estado
que toma el SAT. Podemos agregar como sigue:

```{r}
 ggplot(sat, aes(x = expend, y = math, label=state, colour = frac)) + 
  geom_point() + geom_text_repel() +
  xlab("Gasto por alumno (miles de dólares)") +
  ylab("Calificación en matemáticas") 
```


Y vemos entonces por qué nuestra comparación inicial es relativamente pobre:
los estados con mejores resultados promedio en el SAT son aquellos donde una
fracción relativamente baja de los estudiantes toma el examen. La diferencia
es considerable.

En este punto podemos hacer varias cosas. Una primera idea es intentar comparar
estados más similares en cuanto a la población de alumnos que asiste. Podríamos hacer
grupos como sigue:

```{r, echo = FALSE, fig.width = 6, fig.height = 3}
set.seed(991)
sat$clase <- kmeans(sat %>% select(frac), 
                    centers = 4,  nstart = 100, iter.max = 100)$cluster
sat$clase <- fct_reorder(factor(sat$clase), sat$frac, mean)
sat <- sat %>% mutate(rank_p = rank(frac, ties= "first") / length(frac))
ggplot(sat, aes(x = rank_p, y = frac, label = state, colour = factor(clase))) +
  geom_point(size = 2) + 
  #geom_text_repel(colour = "gray80", size = 3) +
  paleta
```

Estos resultados indican que es más probable que buenos alumnos decidan hacer
el SAT - y esto ocurre de manera diferente en cada estado (en algunos estados
era más común otro examen, el ACT). 

Si hacemos clusters de estados
según el % de alumnos, empezamos a ver otra historia. Ajustamos rectas
de mínimos cuadrados como referencia:

```{r, echo = FALSE}
ggplot(sat, aes(x = expend, y = math, label=state, colour = factor(clase))) + 
  geom_point(size = 3) + 
  geom_smooth(method = "lm", se = FALSE) +
  xlab("Gasto por alumno (miles)") +
  ylab("Calificación en matemáticas") + paleta +
  geom_text_repel(colour = "gray70") 
```



Sin embargo, el resultado puede variar considerablemente si categorizamos de distintas maneras.

Otra idea es más útil es *factorizar* o *controlar*
el efecto de el % de alumnos que toma el examen, y hacer una comparación de la variación
restante. Para hacer esto, podemos examinar primero la relación entre la fracción de
alumnos que toma el examen del SAT y las calificaciones:

```{r, fig.width =5, fig.height = 3, echo = FALSE}
#sat <- sat %>% mutate(cuant = qnorm(frac / 100, 500, 100))
ggplot(sat, aes(x = frac, y = sat, label = state)) + geom_point() +
    geom_smooth(method = "loess", span = 0.5, method.args = list(degree = 1)) +
    annotate("text", x = 40, y = 1050, label = "Desempeño por \n encima del esperado", colour = "gray30") +
    annotate("text", x = 15, y = 900, label = "Desempeño por \n abajo del esperado", colour  = "gray30") 

```

Y podemos podemos extraer la diferencia entre cada observación y el esperado, representado
por el suavizador azul. 

```{r}
suavizador <- loess(sat ~ frac, sat, degree = 1, span = 0.5)
sat <- sat %>% mutate(residual = residuals(suavizador))
 ggplot(sat, aes(x = expend, y = residual, label = state)) + 
  geom_point(colour = "red", size = 2) + geom_text_repel(colour = "gray50") +
  xlab("Gasto por alumno (miles de dólares)") +
  ylab("Residual SAT (vs fracción de alumnos)")
```

La indicación que obtenemos es diferente: ahora parece haber cierta 
evicencia de  que estados con más gasto
tienden a tener mejores resultados, controlando por la fracción de estudiantes que toma el SAT,
aunque hay variación considerable para cada nivel de gasto.
Nótese que esto nos muestra que nuestra pregunta original tiene una respuesta
simple pero poco útil. con este análisis podemos pensar en mejores preguntas, pero todas
son mucho más difíciles de contestar!

### Ejercicio: suavizadores

Puedes practicar y entender cómo funcionan los suavidaores con **esta app**, cuya idea
es tomada de [este libro](https://rafalab.github.io/dsbook/smoothing.html#local-weighted-regression-loess). El suavizamiento
**loess** es diferente de promedios móviles centrados usuales, y a veces ayuda a obtener mejores
suavizados (cuando hay estructura lineal en el problema):

```{r, fig.width = 10, fig.height = 6, echo = FALSE}

w_loess <- function(x){
  ifelse(abs(x) < 1, (1 - abs(x)^3)^3, 0.0)
}
lm_ponderado <- function(...) {
  geom_smooth(method = "lm", se = FALSE, ...)
}

crear_local <- function(frac_val = 10, alpha = 0.5){
  num_datos = floor(alpha * nrow(sat)) + 1
  sat_g <- sat %>% 
    mutate(activos = rank(abs(frac - frac_val)) < num_datos,
         max_dist = max(activos*abs(frac - frac_val)),
         peso = w_loess((frac - frac_val)/max_dist))
ggplot(sat_g, aes(x = frac, y = sat, label = state, size = peso)) + 
  geom_point(colour = "gray") +
  scale_size(range = c(0.5, 4)) +
    geom_smooth(method = "loess", method.args = list(degree = 1), span = alpha, se = FALSE,
              size = 1, colour = "red") +
  lm_ponderado(data = sat_g %>% filter(peso > 0), aes(weight = peso^2)) +
  geom_vline(xintercept = frac_val, colour = "gray") +
  theme(legend.position = "none") 

}
grafs <- map(c(10, 20, 30, 40, 50, 60), ~ crear_local(.x, alpha = 0.6))
gridExtra::grid.arrange(grobs = grafs,ncol = 3)
```


Promedios móviles (ponderados por distancia) dan resultados más ruidosos si queremos
capturar la estructura lineal local:

```{r, fig.width = 10, fig.height = 6, echo = FALSE}

w_loess <- function(x){
  ifelse(abs(x) < 1, (1 - abs(x)^3)^3, 0.0)
}
lm_ponderado <- function(...) {
  geom_smooth(method = "lm", formula = y ~ 1, se = FALSE, ...)
}

crear_local <- function(frac_val, alpha = 0.5){
  num_datos = floor(alpha * nrow(sat)) 
  sat_g <- sat %>% 
    mutate(activos = rank(abs(frac - frac_val)) < num_datos,
         max_dist = max(activos*abs(frac - frac_val)),
         peso = w_loess((frac - frac_val)/max_dist))
ggplot(sat_g, aes(x = frac, y = sat, label = state, size = peso)) + 
  geom_point(colour = "gray") +
  scale_size(range = c(0.5, 4)) +
    geom_smooth(method = "loess", method.args = list(degree = 0), 
                span = alpha, se = FALSE,
              size = 1, colour = "red") +
  lm_ponderado(data = sat_g %>% filter(peso > 0), aes(weight = peso)) +
  geom_vline(xintercept = frac_val, colour = "gray") +
  theme(legend.position = "none") 

}
grafs <- map(c(10, 20, 30, 40, 50, 60), ~ crear_local(.x, alpha = 0.3))
gridExtra::grid.arrange(grobs = grafs,ncol = 3)
```



## Ajuste y residuales {-}

Esta técnica de análisis en la que controlamos por variables para "escarbar en los datos",
se puede resumir como sigue. Intentamos ajustar patrones conocidos en los datos, "quitarlos de
los datos", y examinar la variación restante:

$$ datos = ajuste + residual $$

En un principio, ajuste y residuales requieren de nuestra atención. El objetivo de este proceso
es hacer más datos, no menos, con la idea de que hacer comparaciones más finas que las que los
datos sin procesar nos dan. Es una técnica que repetiremos varias veces.

## Ejemplo: tablas agregadas {-}

```{r}

```

## Comparaciones multiplicativas {-}

Hay dos maneras de hacer comparaciones:

- En escala **aditivas**: por ejemplo, el grupo A tiene 50 personas más que el grupo B
- En escala **multiplicativas**: el grupo A tiene 25\% más personas que el grupo B

¿Cuáles son más apropiadas?

Distintos aspectos del análisis aparecen dependiendo de qué tipo de comparaciones queramos
hacer, pero muy frecuentemente una tipo comparación es preferible al otro. Por ejemplo,
si queremos comparar incrementos de precios a lo largo de distintos tipos de productos, es
natural usar escalas multiplicativas. 

La elección de la comparación apropiada mejora la calidad de las comparaciones, y
simplifica el análisis y su interpretación. 

En el siguiente ejemplo, consideramos cómo varía el precio de un conjunto de casas
dependiendo de la calificación de calidad:


```{r, message = FALSE, echo = FALSE}
source("R/casas_preprocesamiento.R")
set.seed(21)
casas_completo <- casas
casas <- casas_completo %>% sample_frac(0.9)
casas_holdout <- casas_completo %>% anti_join(casas)
```

```{r, echo = FALSE}
casas_ejemplo <- casas %>% filter(calidad_gral > 3) %>% 
           mutate(nombre_zona = fct_reorder(nombre_zona, calidad_gral))
ggplot(casas_ejemplo, aes(x = factor(calidad_gral), y = precio_miles)) + 
    geom_boxplot(outlier.alpha = 0) +
    geom_jitter(width =0.1, height = 0, alpha = 0.2) 
```

En esta gráfica podemos hacer comparaciones entre precios de casas de manera
aditiva: por ejemplo, las casa más cara de Somerset es de 50 mil dólares más cara que la
siguiente más cara. O en IDOTRR las casas son 100 mil dólares más baratas que en CollgCr.

Sin embargo, una comparación multiplicativa puede ser más adecuada y simplifica la descripción
de los datos. Por ejemplo, una variación de 20 mil dólares en casas de NridgHt no es 
muy considerable (con rango de 200 a 600 mil dólares), pero sí es importante para una casa
de IDOTRR (con rango de 50 a 150 mil dólares).

¿Cómo convertimos una escala aditiva a una multiplicativa? Buscamos una función $f$ tal que

$$f(y) - f(x) = f(x+ \Delta x) - f(x) \approx \frac{\Delta x }{x}$$
para cualquier $\Delta x$. Esto implica que la derivada de $f$ tiene que ser $1/x$,
y por lo tanto la transformación que buscamos es el logaritmo. En escala logarítmica, 
las comparaciones son:

$$\log(y) - \log(x) = \log(y/x) = \log \left(1 + \frac{\Delta x}{x}\right) \approx \frac{\Delta x}{x}$$

y esta aproximación es buena cuando $|\frac{\Delta x}{x}| < 0.25$ (y muy buena si es menor a 0.1).


Veamos entonces cómo se ven los datos de casas en escala logarítmica. A la izquierda
mostramos una gráfica más apropiada para análisis, y a la derecha una que quizá prefereríamos
usar para presentación

```{r, fig.width = 9, fig.height = 3, echo = FALSE}
g_1 <- ggplot(casas_ejemplo, 
       aes(x = factor(calidad_gral), y = log(precio_miles))) +
     geom_boxplot(outlier.alpha = 0) + 
    geom_jitter(width =0.1, height = 0, alpha = 0.1)  + 
    scale_y_continuous(breaks = seq(3, 7, 0.5))
g_2 <- ggplot(casas_ejemplo, 
       aes(x = factor(calidad_gral), y = precio_miles)) +
     geom_boxplot(outlier.alpha = 0) + 
    geom_jitter(width =0.1, height = 0, alpha = 0.1) + 
    scale_y_log10(breaks = c(50, 100, 200, 400)) 
gridExtra::grid.arrange(g_1, g_2, ncol = 2)
```

La variación es más similar a lo largo de cada categoría de calidad. En la escala logarítmica,
las cajas (cuartil inferior a cuartil superior) está alrededor de tamaño 0.25, y la mediana
está aproximadamente centrada en cada caja. Así que la variación entre el cuartil superior
e inferior respecto a la mediana es de 12\% aproximadamente. *Esto es aproximadamente
cierto en cada grupo de calidad*. Podemos calcular cuantiles adicionales:

```{r}
casas_tbl <- casas_ejemplo %>% group_by(calidad_gral) %>% 
    summarise(cuantiles = list(cuantil(log(precio_miles), c(0.1, 0.25, 0.5, 0.75, 0.9)))) %>% 
    unnest
g_80_p <- ggplot(casas_tbl  %>% spread(cuantil,valor), aes(x = calidad_gral, y = `0.5`)) +
    geom_linerange(aes(ymin= `0.1`, ymax = `0.9`), colour = "gray40") +
    geom_linerange(aes(ymin= `0.25`, ymax = `0.75`), size = 2, colour = "white") +
    geom_point(colour = "red", size = 3) +
    scale_y_continuous(breaks = seq(4, 6.5, by =0.25))
g_80_p
```

```{r}
casas_tbl <- casas_ejemplo %>% group_by(calidad_gral) %>% 
    summarise(cuantiles = list(cuantil((precio_miles), c(0.1, 0.25, 0.5, 0.75, 0.9)))) %>% 
    unnest
ggplot(casas_tbl  %>% spread(cuantil,valor), aes(x = calidad_gral, y = `0.5`)) +
    geom_linerange(aes(ymin= `0.1`, ymax = `0.9`), colour = "gray40") +
    geom_linerange(aes(ymin= `0.25`, ymax = `0.75`), size = 2, colour = "white") +
    geom_point(colour = "red", size = 3) + scale_y_log10()
```


Esta es una comparación superior, pues descubrimos una regularidad importante en los
datos (principio 3). En este caso, lo logramos cambiando a una escala más apropiada para
el problema.

Podemos examinar los residuales de distintas maneras. Para describir estas gráficas
de manera cuantitativa, por ejemplo, podemos hacer lo siguiente:

$$ precio (log) = mediana\_calidad + residual $$

Primero vemos las medianas por grupo:

```{r}
medianas <- casas_ejemplo %>% group_by(calidad_gral) %>% 
    summarise(mediana = median(precio_miles)) %>% 
    arrange(desc(mediana)) %>% 
    mutate(incremento = mediana / lead(mediana) - 1)
medianas
```

Como los residuales son similares en cada grupo, los agrupamos para resumir:

```{r, echo = FALSE, message = FALSE}
residuales <- casas_ejemplo %>% left_join(medianas) %>% 
    group_by(calidad_gral) %>% 
    mutate(residual = log(precio_miles) - log(mediana)) 
quantile(residuales$residual, 
         c(0, 0.10, 0.25, 0.5, 0.75, 0.9, 1)) %>% kable
```

Así que nuestro resumen sería la tabla de arriba, junto con esta distribución de residuales. Diríamos:

- Cada punto adiconal de calidad, la mediana del precio aumenta larededor de 20\%. Dentro de cada grupo
de calidad, el 50\% de las casas está a una diferencia máxima de +/- 12% del precio mediano de la categoría,
y 80\% de las casas alrededor de +/- 27%. En algunos casos observamos diferencias de 60 - 130 \% del precio mediano.



## Ejemplo: tiempos de fusión {-}

Veamos el siguiente ejemplo, que es un experimento donde se midió el tiempo
que tardan distintas personas en fusionar un estereograma para ver una imagen 3D.
Existen dos condiciones: en una se dio indicaciones de qué figura tenían que
buscar (VV) y en otra no se dio esa indicación. ¿Las instrucciones verbales
ayudan a fusionar más rápido el estereograma?

```{r, message = FALSE, echo = FALSE}
fusion <- read_delim("./datos/fusion_time.txt", delim = " ", trim_ws = TRUE)
ggplot(fusion, aes(x = nv.vv, y = time)) + geom_boxplot() + geom_jitter()
```


Lo primero que observamos es que hay una variabilidad grande entre el tiempo que tardan
en fusionar: los rangos van de menos de 5 segundos hasta 20 - 40 segundos. Esto sugiere dos
posibilidades:

- Hay personas que son mucho más hábiles que otras para fusionar estereogramas
- Cada persona puede tardar una cantidad muy variable de tiempo para fusionar un estereograma

En cualquier caso, tiene más sentido pensar en diferencias relativas: si alguien tarda 8 segundos,
tardó el doble que alguien que tardó 4 segundos, iugal que la comparación de alguien que tarda 16 vs 8 segundos.


```{r, message = FALSE, echo = FALSE}
fusion <- read_delim("./datos/fusion_time.txt", delim = " ", trim_ws = TRUE)
g_1 <- ggplot(fusion, aes(x = nv.vv, y = log(time))) + geom_boxplot() + geom_jitter() 
g_2 <- ggplot(fusion, aes(x = nv.vv, y = time)) + geom_boxplot() + geom_jitter() +
     scale_y_log10()
gridExtra::grid.arrange(g_1, g_2, ncol = 2)
```

De la gráfica, vemos que la distribución VV está desplazada alrededor de 0.3 unidades (en logaritmo)
hacia abajo. Esto implica que el tiempo bajo VV es alrededor de 66% los tiempos bajo NV. La variación
dentro de cada grupo es considerable.

Gráficas de cuantiles puedes ser más efetivas para 

```{r}
ggplot(fusion, aes(sample = log(time), colour = nv.vv)) + 
  geom_qq(distribution = stats::qunif) 
```



## Cuándo la elección es menos importante {-}

En algunos casos, la diferencia no es muy importante. Supongamos
que tenemos pares de números positivos $(a_i, b_i)$, tales que las $b_i$ varían relativamente
poco con respecto a las diferencias $a_i - b_i$. Entonces podemos aproximar:

$$\frac{a_i - b_i}{b_i} = \frac{a_i - b_i}{A z_i + B}\approx  \frac{1}{B} (a_i - b_i),$$

de forma que las comparaciones aditivas y multipicativas son similares, y *la elección se 
reduce a una preferencia de la escala*. Esto se muestra en la primera gráfica en la siguiente
ilustración. Por otro lado, cuando hay más variación en el denominador, las comparaciones pueden dar
resultados muy diferentes:

```{r, fig.width =6, fig.height = 5}
set.seed(81)
tab_1 <- tibble(b = runif(100, 45,55),   a = b * runif(100, 0, 2), tipo = "b var chica", error = "multiplicativo")
tab_2 <- tibble(b = runif(100, 10, 100), a = b * runif(100, 0, 2), tipo = "b var grande", error = "multiplicativo")
tab_3 <- tibble(b = runif(100, 45,55),   a = b + runif(100, -1, 1), tipo = "b var chica", error = "aditivo")
tab_4 <- tibble(b = runif(100, 10, 100), a = b + runif(100, -1, 1), tipo = "b var grande", error = "aditivo")
tab <- bind_rows(tab_1, tab_2)
g_1 <- ggplot(tab, aes(x = a-b, y = (a-b)/b, colour = b)) + geom_point() +
    facet_wrap(error ~ tipo)
tab <- bind_rows(tab_3, tab_4)
g_2 <- ggplot(tab, aes(x = a-b, y = (a-b)/b, colour = b)) + geom_point() +
    facet_wrap(error ~ tipo)
gridExtra::grid.arrange(g_1, g_2, ncol = 1)
```


## Ejemplo (tablas)

```{r}
tea <- read_csv(("datos/tea.csv"))
te <- tea %>% select(how, price, sugar)
```

Nos interesa ver qué personas compran té suelto, y de qué tipo:

```{r}
precio <- te %>% group_by(price) %>% tally() %>% mutate(prop = round(100 * n / sum(n))) %>% 
  select(-n)

tipo <- te %>% group_by(how) %>% tally() %>% mutate(prop = round(100 * n / sum(n)))
tipo 
tipo <- tipo %>% select(how, prop_how = prop)
```

La mayor parte de las personas toma té en bolsas. Sin embargo, el tipo de té que compran es
muy distinto:


```{r}
te %>% group_by(how, price) %>% tally() %>% group_by(how) %>% mutate(prop = round(100 * n / sum(n))) %>% 
  select(-n) %>% spread(how, prop, fill = 0) 
```

```{r}
tabla <- te %>% group_by(how, price) %>% tally() %>% group_by(how) %>% 
  mutate(prop_price = (100 * n / sum(n))) %>% 
  group_by(price) %>% mutate(prom_prop = mean(prop_price)) %>% 
  mutate(perfil = (prop_price / prom_prop - 1) %>% round(2))  
precio_prom <- tabla %>% select(price, prom_prop) %>% unique %>% 
  mutate(promedio = round(prom_prop)) %>% select(price, promedio)
tabla_perfil <- tabla %>%   
  select(how, price, perfil) %>% spread(how, perfil, fill = -1) 
tabla_2 <- tabla_perfil %>% 
  gather(how, prop_price, -price)
if_profile <- function(x){
  any(x < 0) & any(x > 0)
}
marcar <- marcar_tabla_fun(0.25, "red", "black")
tab_out <- tabla_perfil %>% left_join(precio_prom) %>%
  arrange(desc(promedio)) %>% 
  mutate_if(if_profile, marcar) %>% 
  knitr::kable(format = "html", escape = F, digits = 2) %>% 
  kableExtra::kable_styling(bootstrap_options = c( "hover", "condensed"), full_width = FALSE)
tab_out
```

```{r, fig.width = 6, fig.height = 2}
tabla_ordenada <- tabla %>% ungroup %>% 
  left_join(tabla %>% ungroup %>% filter(how == "tea bag") %>% select(price, perfil_tea = perfil)) %>% 
  mutate(precio = fct_reorder(price, perfil_tea))
g_perfil <- ggplot(tabla_ordenada,
  aes(x = precio, xend = precio, y = perfil, yend = 0, group = how)) + 
  geom_point() + geom_segment() + facet_wrap(~how) +
  geom_hline(yintercept = 0 , colour = "gray")+ coord_flip()
g_props <- ggplot(precio, aes(y = price, x = 1, label = prop, fill = 1)) + 
    scale_fill_gradient(low = "white", high = "white") +
  geom_tile() + geom_text() + theme(axis.text.x = element_blank(), legend.position = "none") +
  xlab(" ") 
g_perfil
```



```{r}
ggplot(tabla_2 %>% ungroup %>% mutate(price = fct_reorder(price, prop_price)),
  aes(x = price, y = prop_price, group = how, colour = how)) + 
  geom_point() + coord_flip() + geom_line()
```

```{r}
ganado <- read_csv("./datos/livestock.csv")
ganado
```

```{r}
ganado_2 <- ganado %>% mutate(count_miles = count / 1e6) %>%  
  select(-count) %>% 
  spread(livestock.type, count_miles) 
ggplot(ganado_2, aes(x = Cattle, y = Poultry)) + geom_point()
```

```{r}
ganado <- ganado %>% 
  ungroup %>% 
  mutate(media = mean(log(count))) %>% 
  group_by(country) %>% 
  mutate(por_pais = mean(log(count)) - media) %>% 
  group_by(livestock.type) %>% 
  mutate(por_tipo = mean(log(count)) - media) %>% 
  mutate(residual = log(count) - (media + por_pais + por_tipo)) %>% 
  mutate(ajustado = media + por_pais + por_tipo)
ggplot(ganado, aes(x=ajustado, y=residual)) + geom_point()
```

```{r}

marcar_g <- marcar_tabla_fun_doble(-0.4, 0.4, "red", "black")
ganado_ancha <- 
  ganado %>% select(livestock.type, country, residual) %>% 
  mutate(residual = round(residual, 2)) %>% 
  spread(livestock.type, residual) 
orden <- princomp(ganado_ancha  %>% select(-country))$scores[,1] %>% round(2)
#orden <- kmeans(ganado_ancha  %>% select(-country), centers = 6)$cluster
ganado_ancha %>% 
  mutate(orden = orden) %>% 
  arrange(orden) %>% 
  mutate_if(if_profile, marcar_g) %>%
  knitr::kable(format = "html", escape = F, digits = 2) %>% 
  kableExtra::kable_styling(bootstrap_options = c( "hover", "condensed"), full_width = FALSE)
```

Y podemos resumir:

```{r}
aov_ganado <- ganado %>% select(livestock.type, country, por_pais:residual) %>% 
  gather(tipo, valor, por_pais:residual) %>% 
  mutate(etiqueta = ifelse(tipo == "residual", paste(livestock.type, country), "")) %>% ungroup %>% 
  mutate(etiqueta = ifelse(abs(valor)> 2, etiqueta, ""))
ggplot(aov_ganado, aes(sample = valor)) + geom_qq(distribution = stats::qunif) +
  facet_wrap(~tipo) 

```

